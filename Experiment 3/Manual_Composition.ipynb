{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcuNR0csGKjf"
      },
      "source": [
        "**MNIST Manual Composition**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nATW6aF4GKQB",
        "outputId": "cb6169d2-6310-4855-a12e-6893ff0717bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üî¢ Enter how many clients to sample (n): 13\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "üîÅ Total combinations to evaluate: 286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Processed 0/286 combinations\n",
            "‚úÖ Processed 100/286 combinations\n",
            "‚úÖ Processed 200/286 combinations\n",
            "‚úÖ All combinations processed and saved to CSV.\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# ========================\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ‚úÖ Weights folder (given by you)\n",
        "WEIGHTS_DIR = \"/content/drive/MyDrive/MLaaS_Weights_20_MNIST\"  # <-- CHANGE if needed\n",
        "\n",
        "# ========================\n",
        "# IMPORTS\n",
        "# ========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ========================\n",
        "# ASK USER: HOW MANY CLIENTS TO SAMPLE\n",
        "# ========================\n",
        "num_clients_to_sample = int(input(\"üî¢ Enter how many clients to sample (n): \"))\n",
        "\n",
        "CLIENTS_PER_COMBINATION = 10     # fixed as you said\n",
        "LOCAL_EPOCHS = 2               # ‚úÖ you asked \"two epochs\"\n",
        "GLOBAL_ROUNDS = 1              # ‚úÖ you asked \"one round\"\n",
        "\n",
        "# ========================\n",
        "# LOAD CLIENT METADATA\n",
        "# ========================\n",
        "df = pd.read_csv(\"/content/MNIST_Client_Profiles_For_Composability_20_20.csv\")\n",
        "\n",
        "# Optional: label columns if present (Label0..Label9)\n",
        "label_cols = [c for c in df.columns if c.lower().startswith(\"label\")]\n",
        "\n",
        "# ========================\n",
        "# MODEL DEFINITION\n",
        "# ========================\n",
        "def build_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.Dense(10, activation=\"softmax\"),\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ========================\n",
        "# LOAD MNIST (train + test)\n",
        "# ========================\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = (x_train.astype(\"float32\") / 255.0)[..., None]\n",
        "y_train = y_train.astype(\"int32\")\n",
        "\n",
        "x_test = (x_test.astype(\"float32\") / 255.0)[..., None]\n",
        "y_test = y_test.astype(\"int32\")\n",
        "\n",
        "# Evaluation subset (as you had)\n",
        "eval_data = x_test[:1000]\n",
        "eval_labels = y_test[:1000]\n",
        "\n",
        "# ========================\n",
        "# BUILD LABEL POOLS (for non-IID reconstruction if label columns exist)\n",
        "# ========================\n",
        "train_indices_by_label = {k: np.where(y_train == k)[0] for k in range(10)}\n",
        "\n",
        "# We'll keep a \"cursor\" per label so we don't always reuse the same samples\n",
        "label_cursors = {k: 0 for k in range(10)}\n",
        "\n",
        "def _take_indices_for_label(label, count):\n",
        "    \"\"\"Take `count` indices from the label pool in a rolling way.\"\"\"\n",
        "    pool = train_indices_by_label[label]\n",
        "    if count <= 0:\n",
        "        return np.array([], dtype=np.int64)\n",
        "\n",
        "    # If asked more than available, wrap around (still deterministic)\n",
        "    start = label_cursors[label]\n",
        "    end = start + count\n",
        "    if end <= len(pool):\n",
        "        out = pool[start:end]\n",
        "    else:\n",
        "        part1 = pool[start:]\n",
        "        remaining = end - len(pool)\n",
        "        part2 = pool[:remaining]\n",
        "        out = np.concatenate([part1, part2], axis=0)\n",
        "\n",
        "    label_cursors[label] = end % len(pool)\n",
        "    return out\n",
        "\n",
        "def get_client_dataset(cid, client_row, batch_size=64):\n",
        "    \"\"\"\n",
        "    Create a local dataset for the client.\n",
        "    - If Label* columns exist: build non-IID sample using those counts.\n",
        "    - Else: build a deterministic random sample using DataVolume(Samples) and Client_ID as seed.\n",
        "    \"\"\"\n",
        "    # fallback sample count\n",
        "    if \"DataVolume(Samples)\" in client_row.index and not pd.isna(client_row[\"DataVolume(Samples)\"]):\n",
        "        n_samples = int(client_row[\"DataVolume(Samples)\"])\n",
        "    else:\n",
        "        n_samples = 500  # safe fallback\n",
        "\n",
        "    n_samples = max(1, n_samples)\n",
        "\n",
        "    # Case A: label distribution exists\n",
        "    if len(label_cols) >= 10:\n",
        "        # try to map label columns to 0..9\n",
        "        # Many of your files are Label0..Label9; this handles that pattern.\n",
        "        counts = []\n",
        "        for k in range(10):\n",
        "            # find a column that ends with the digit k\n",
        "            candidates = [c for c in label_cols if c.lower().endswith(str(k))]\n",
        "            if len(candidates) == 0:\n",
        "                counts.append(0)\n",
        "            else:\n",
        "                v = client_row[candidates[0]]\n",
        "                counts.append(int(v) if not pd.isna(v) else 0)\n",
        "\n",
        "        # If counts sum is 0 or doesn't match n_samples, fix it cleanly\n",
        "        total = sum(counts)\n",
        "        if total <= 0:\n",
        "            # fallback to deterministic random\n",
        "            rng = np.random.RandomState(int(cid) + 123)\n",
        "            idx = rng.choice(len(x_train), size=n_samples, replace=False)\n",
        "        else:\n",
        "            # If total != n_samples, rescale counts to match n_samples\n",
        "            if total != n_samples:\n",
        "                scaled = np.array(counts, dtype=np.float64) / float(total)\n",
        "                counts = np.floor(scaled * n_samples).astype(int).tolist()\n",
        "                # fix rounding gap\n",
        "                gap = n_samples - sum(counts)\n",
        "                if gap > 0:\n",
        "                    # add remaining to the largest proportions\n",
        "                    order = np.argsort(-scaled)\n",
        "                    for i in range(gap):\n",
        "                        counts[int(order[i % 10])] += 1\n",
        "\n",
        "            idx_parts = []\n",
        "            for k in range(10):\n",
        "                idx_parts.append(_take_indices_for_label(k, counts[k]))\n",
        "            idx = np.concatenate(idx_parts, axis=0)\n",
        "\n",
        "    # Case B: no label columns -> deterministic random split\n",
        "    else:\n",
        "        rng = np.random.RandomState(int(cid) + 123)\n",
        "        replace = n_samples > len(x_train)\n",
        "        idx = rng.choice(len(x_train), size=n_samples, replace=replace)\n",
        "\n",
        "    x_c = x_train[idx]\n",
        "    y_c = y_train[idx]\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_c, y_c)).shuffle(\n",
        "        buffer_size=min(2000, len(y_c)), seed=int(cid) + 999, reshuffle_each_iteration=True\n",
        "    ).batch(batch_size)\n",
        "\n",
        "    return ds, len(y_c)\n",
        "\n",
        "def fedavg(weights_list, sample_counts=None):\n",
        "    \"\"\"\n",
        "    FedAvg aggregation.\n",
        "    - If sample_counts is provided: weighted average by local data size\n",
        "    - Else: simple mean\n",
        "    \"\"\"\n",
        "    if sample_counts is None:\n",
        "        avg_weights = []\n",
        "        for layer_weights in zip(*weights_list):\n",
        "            stacked = np.stack(layer_weights, axis=0)\n",
        "            avg_weights.append(np.mean(stacked, axis=0))\n",
        "        return avg_weights\n",
        "\n",
        "    sample_counts = np.asarray(sample_counts, dtype=np.float64)\n",
        "    sample_counts = sample_counts / sample_counts.sum()\n",
        "\n",
        "    avg_weights = []\n",
        "    for layer_weights in zip(*weights_list):\n",
        "        # layer_weights: tuple of arrays, one per client\n",
        "        stacked = np.stack(layer_weights, axis=0)  # (num_clients, ...)\n",
        "        # weighted sum across first axis\n",
        "        w = sample_counts.reshape((-1,) + (1,) * (stacked.ndim - 1))\n",
        "        avg = np.sum(stacked * w, axis=0)\n",
        "        avg_weights.append(avg)\n",
        "    return avg_weights\n",
        "\n",
        "# ========================\n",
        "# SAMPLE CLIENTS & GENERATE COMBINATIONS\n",
        "# ========================\n",
        "selected_df = df.sample(n=num_clients_to_sample, random_state=42)\n",
        "selected_ids = selected_df[\"Client_ID\"].tolist()\n",
        "\n",
        "combinations = list(itertools.combinations(selected_ids, CLIENTS_PER_COMBINATION))\n",
        "print(f\"üîÅ Total combinations to evaluate: {len(combinations)}\")\n",
        "\n",
        "# ========================\n",
        "# COMBINATION EVALUATION\n",
        "# - Step 1: load local weights\n",
        "# - Step 2: aggregate -> initial global weights\n",
        "# - Step 3: 1 federated round with 2 local epochs\n",
        "# - Step 4: aggregate updated weights -> final global weights\n",
        "# - Step 5: evaluate final global accuracy\n",
        "# ========================\n",
        "records = []\n",
        "\n",
        "for combo_id, client_ids in enumerate(combinations):\n",
        "    clients_data = df[df[\"Client_ID\"].isin(client_ids)].copy()\n",
        "\n",
        "    # ---- (A) Load local weights for each client (existing .npz)\n",
        "    init_weights_list = []\n",
        "    missing = False\n",
        "    for cid in client_ids:\n",
        "        full_path = os.path.join(WEIGHTS_DIR, f\"client_{cid}_local.npz\")\n",
        "        if not os.path.exists(full_path):\n",
        "            print(f\"‚ùå Missing weights for client {cid} at {full_path}\")\n",
        "            missing = True\n",
        "            break\n",
        "\n",
        "        with np.load(full_path) as data:\n",
        "            # IMPORTANT: preserve key order for consistent weight list\n",
        "            # We'll sort keys so it's stable.\n",
        "            keys = sorted(list(data.keys()))\n",
        "            weights = [data[k] for k in keys]\n",
        "            init_weights_list.append(weights)\n",
        "\n",
        "    if missing or len(init_weights_list) < CLIENTS_PER_COMBINATION:\n",
        "        continue\n",
        "\n",
        "    # ---- (B) Initial global weights = FedAvg(local weights)\n",
        "    global_weights = fedavg(init_weights_list, sample_counts=None)\n",
        "\n",
        "    # ---- (C) Run 1 federated round with 2 local epochs\n",
        "    for r in range(GLOBAL_ROUNDS):\n",
        "        updated_weights_list = []\n",
        "        sample_counts = []\n",
        "\n",
        "        for cid in client_ids:\n",
        "            # Build client local dataset (reconstructed from MNIST train)\n",
        "            row = df[df[\"Client_ID\"] == cid].iloc[0]\n",
        "            ds, n_samp = get_client_dataset(cid, row, batch_size=64)\n",
        "\n",
        "            # Local training starting from current global\n",
        "            local_model = build_model()\n",
        "            local_model.set_weights(global_weights)\n",
        "            local_model.fit(ds, epochs=LOCAL_EPOCHS, verbose=0)\n",
        "\n",
        "            updated_weights_list.append(local_model.get_weights())\n",
        "            sample_counts.append(n_samp)\n",
        "\n",
        "        # Aggregate updated weights -> new global weights (weighted FedAvg)\n",
        "        global_weights = fedavg(updated_weights_list, sample_counts=sample_counts)\n",
        "    # ---- (D) Evaluate final global model on eval_data\n",
        "    global_model = build_model()\n",
        "    global_model.set_weights(global_weights)\n",
        "    _, acc = global_model.evaluate(eval_data, eval_labels, verbose=0)\n",
        "    global_accuracy = float(acc * 100.0)\n",
        "    # ---- (E) QoS metrics (same as your existing)\n",
        "    total_volume = clients_data[\"DataVolume(Samples)\"].sum()\n",
        "    total_latency = clients_data[\"Latency(ms)\"].sum()\n",
        "    mean_quality = clients_data[\"Mean_Quality_Factor(%)\"].mean()\n",
        "    mean_reliability = clients_data[\"Reliability_Score\"].mean()\n",
        "    weight_paths = [os.path.join(WEIGHTS_DIR, f\"client_{cid}_local.npz\") for cid in client_ids]\n",
        "    records.append({\n",
        "        \"Combination_ID\": combo_id,\n",
        "        **{f\"Client_{i+1}\": cid for i, cid in enumerate(client_ids)},\n",
        "        \"Weights_Paths\": weight_paths,\n",
        "        \"Global_DataVolume\": total_volume,\n",
        "        \"Global_Latency\": total_latency,\n",
        "        \"Global_Mean_Quality_Factor\": mean_quality,\n",
        "        \"Global_Reliability_Score\": mean_reliability,\n",
        "        \"Global_Accuracy\": global_accuracy\n",
        "    })\n",
        "    if combo_id % 100 == 0:\n",
        "        print(f\"‚úÖ Processed {combo_id}/{len(combinations)} combinations\")\n",
        "# ========================\n",
        "# SAVE FINAL DATAFRAME\n",
        "# ========================\n",
        "df_combos = pd.DataFrame(records)\n",
        "df_combos.to_csv(\"client_combinations_with_qos_custom.csv\", index=False)\n",
        "print(\"‚úÖ All combinations processed and saved to CSV.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "_uqfWqSAGSFi",
        "outputId": "c8fc8ac6-da46-41d5-c6ba-4f906c7085cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Global_Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>286.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>74.067133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.156040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>62.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>71.200001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>74.249998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>76.899999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>83.899999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ],
            "text/plain": [
              "count    286.000000\n",
              "mean      74.067133\n",
              "std        4.156040\n",
              "min       62.500000\n",
              "25%       71.200001\n",
              "50%       74.249998\n",
              "75%       76.899999\n",
              "max       83.899999\n",
              "Name: Global_Accuracy, dtype: float64"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_combos['Global_Accuracy'].describe()\n",
        "df_combos.to_csv(\"Updated_combination_10_MINIST_20_20_V1.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj8xEPGris6c"
      },
      "source": [
        "**FMNIST Manual Composition**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeonM2FKisdD",
        "outputId": "25739a27-2c62-473c-ee40-bc7a16c981ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üî¢ Enter how many clients to sample (n): 12\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "üîÅ Total combinations to evaluate: 66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Processed 0/66 combinations\n",
            "‚úÖ All combinations processed and saved to CSV: client_combinations_with_qos_custom_FMNIST.csv\n"
          ]
        }
      ],
      "source": [
        "# ========================\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# ========================\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "# ========================\n",
        "# IMPORTS\n",
        "# ========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "# ========================\n",
        "# PATHS (EDIT ONLY THESE IF NEEDED)\n",
        "# ========================\n",
        "# ‚úÖ FMNIST weights folder (must contain: client_<id>_local.npz)\n",
        "WEIGHTS_DIR = \"/content/drive/MyDrive/MLaaS_Weights_30_FMNIST\"\n",
        "\n",
        "# ‚úÖ FMNIST client profile CSV (must contain Client_ID + optional Label0..Label9 etc.)\n",
        "# Example name (change to your actual file path):\n",
        "PROFILES_CSV = \"/content/FMNIST_Client_Profiles_For_Composability_30_30.csv\"\n",
        "\n",
        "# Output CSV\n",
        "OUT_CSV = \"client_combinations_with_qos_custom_FMNIST.csv\"\n",
        "\n",
        "# ========================\n",
        "# ASK USER: HOW MANY CLIENTS TO SAMPLE\n",
        "# ========================\n",
        "num_clients_to_sample = int(input(\"üî¢ Enter how many clients to sample (n): \"))\n",
        "\n",
        "# ========================\n",
        "# CONFIG (KEEP SAME AS YOUR MNIST COMBO CODE)\n",
        "# ========================\n",
        "CLIENTS_PER_COMBINATION = 10  # fixed as you said\n",
        "LOCAL_EPOCHS = 2               # ‚úÖ \"two epochs\"\n",
        "GLOBAL_ROUNDS = 1              # ‚úÖ \"one round\"\n",
        "BATCH_SIZE = 64                # consistent\n",
        "\n",
        "# ========================\n",
        "# LOAD CLIENT METADATA\n",
        "# ========================\n",
        "df = pd.read_csv(PROFILES_CSV)\n",
        "\n",
        "# Optional: label columns if present (Label0..Label9)\n",
        "label_cols = [c for c in df.columns if c.lower().startswith(\"label\")]\n",
        "\n",
        "# ========================\n",
        "# MODEL DEFINITION (SAME ARCHITECTURE AS MNIST)\n",
        "# ========================\n",
        "def build_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.Dense(10, activation=\"softmax\"),\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ========================\n",
        "# LOAD FMNIST (train + test)\n",
        "# ========================\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train = (x_train.astype(\"float32\") / 255.0)[..., None]\n",
        "y_train = y_train.astype(\"int32\")\n",
        "\n",
        "x_test = (x_test.astype(\"float32\") / 255.0)[..., None]\n",
        "y_test = y_test.astype(\"int32\")\n",
        "\n",
        "# Evaluation subset (same idea as your MNIST combo code)\n",
        "eval_data = x_test[:1000]\n",
        "eval_labels = y_test[:1000]\n",
        "\n",
        "# ========================\n",
        "# BUILD LABEL POOLS (for non-IID reconstruction if label columns exist)\n",
        "# ========================\n",
        "train_indices_by_label = {k: np.where(y_train == k)[0] for k in range(10)}\n",
        "label_cursors = {k: 0 for k in range(10)}\n",
        "\n",
        "def _take_indices_for_label(label, count):\n",
        "    \"\"\"Take `count` indices from the label pool in a rolling way.\"\"\"\n",
        "    pool = train_indices_by_label[label]\n",
        "    if count <= 0:\n",
        "        return np.array([], dtype=np.int64)\n",
        "\n",
        "    start = label_cursors[label]\n",
        "    end = start + count\n",
        "\n",
        "    if end <= len(pool):\n",
        "        out = pool[start:end]\n",
        "    else:\n",
        "        part1 = pool[start:]\n",
        "        remaining = end - len(pool)\n",
        "        part2 = pool[:remaining]\n",
        "        out = np.concatenate([part1, part2], axis=0)\n",
        "\n",
        "    label_cursors[label] = end % len(pool)\n",
        "    return out\n",
        "\n",
        "def get_client_dataset(cid, client_row, batch_size=64):\n",
        "    \"\"\"\n",
        "    Create a local dataset for the client.\n",
        "    - If Label* columns exist: build non-IID sample using those counts.\n",
        "    - Else: deterministic random sample using DataVolume(Samples) and Client_ID as seed.\n",
        "    \"\"\"\n",
        "    if \"DataVolume(Samples)\" in client_row.index and not pd.isna(client_row[\"DataVolume(Samples)\"]):\n",
        "        n_samples = int(client_row[\"DataVolume(Samples)\"])\n",
        "    else:\n",
        "        n_samples = 500\n",
        "\n",
        "    n_samples = max(1, n_samples)\n",
        "\n",
        "    # Case A: label distribution exists (Label0..Label9)\n",
        "    if len(label_cols) >= 10:\n",
        "        counts = []\n",
        "        for k in range(10):\n",
        "            candidates = [c for c in label_cols if c.lower().endswith(str(k))]\n",
        "            if len(candidates) == 0:\n",
        "                counts.append(0)\n",
        "            else:\n",
        "                v = client_row[candidates[0]]\n",
        "                counts.append(int(v) if not pd.isna(v) else 0)\n",
        "\n",
        "        total = sum(counts)\n",
        "        if total <= 0:\n",
        "            rng = np.random.RandomState(int(cid) + 123)\n",
        "            idx = rng.choice(len(x_train), size=n_samples, replace=(n_samples > len(x_train)))\n",
        "        else:\n",
        "            if total != n_samples:\n",
        "                scaled = np.array(counts, dtype=np.float64) / float(total)\n",
        "                counts = np.floor(scaled * n_samples).astype(int).tolist()\n",
        "                gap = n_samples - sum(counts)\n",
        "                if gap > 0:\n",
        "                    order = np.argsort(-scaled)\n",
        "                    for i in range(gap):\n",
        "                        counts[int(order[i % 10])] += 1\n",
        "\n",
        "            idx_parts = []\n",
        "            for k in range(10):\n",
        "                idx_parts.append(_take_indices_for_label(k, counts[k]))\n",
        "            idx = np.concatenate(idx_parts, axis=0)\n",
        "\n",
        "    # Case B: no label columns -> deterministic random split\n",
        "    else:\n",
        "        rng = np.random.RandomState(int(cid) + 123)\n",
        "        idx = rng.choice(len(x_train), size=n_samples, replace=(n_samples > len(x_train)))\n",
        "\n",
        "    x_c = x_train[idx]\n",
        "    y_c = y_train[idx]\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_c, y_c)).shuffle(\n",
        "        buffer_size=min(2000, len(y_c)),\n",
        "        seed=int(cid) + 999,\n",
        "        reshuffle_each_iteration=True\n",
        "    ).batch(batch_size)\n",
        "\n",
        "    return ds, len(y_c)\n",
        "\n",
        "def fedavg(weights_list, sample_counts=None):\n",
        "    \"\"\"\n",
        "    FedAvg aggregation.\n",
        "    - If sample_counts provided: weighted average by local data size\n",
        "    - Else: simple mean\n",
        "    \"\"\"\n",
        "    if sample_counts is None:\n",
        "        avg_weights = []\n",
        "        for layer_weights in zip(*weights_list):\n",
        "            stacked = np.stack(layer_weights, axis=0)\n",
        "            avg_weights.append(np.mean(stacked, axis=0))\n",
        "        return avg_weights\n",
        "\n",
        "    sample_counts = np.asarray(sample_counts, dtype=np.float64)\n",
        "    sample_counts = sample_counts / sample_counts.sum()\n",
        "\n",
        "    avg_weights = []\n",
        "    for layer_weights in zip(*weights_list):\n",
        "        stacked = np.stack(layer_weights, axis=0)\n",
        "        w = sample_counts.reshape((-1,) + (1,) * (stacked.ndim - 1))\n",
        "        avg = np.sum(stacked * w, axis=0)\n",
        "        avg_weights.append(avg)\n",
        "    return avg_weights\n",
        "\n",
        "# ========================\n",
        "# SAMPLE CLIENTS & GENERATE COMBINATIONS\n",
        "# ========================\n",
        "selected_df = df.sample(n=num_clients_to_sample, random_state=42)\n",
        "selected_ids = selected_df[\"Client_ID\"].tolist()\n",
        "\n",
        "combinations = list(itertools.combinations(selected_ids, CLIENTS_PER_COMBINATION))\n",
        "print(f\"üîÅ Total combinations to evaluate: {len(combinations)}\")\n",
        "\n",
        "# ========================\n",
        "# COMBINATION EVALUATION\n",
        "# ========================\n",
        "records = []\n",
        "\n",
        "for combo_id, client_ids in enumerate(combinations):\n",
        "    clients_data = df[df[\"Client_ID\"].isin(client_ids)].copy()\n",
        "\n",
        "    # ---- (A) Load local weights for each client\n",
        "    init_weights_list = []\n",
        "    missing = False\n",
        "\n",
        "    for cid in client_ids:\n",
        "        full_path = os.path.join(WEIGHTS_DIR, f\"client_{cid}_local.npz\")\n",
        "        if not os.path.exists(full_path):\n",
        "            print(f\"‚ùå Missing weights for client {cid} at {full_path}\")\n",
        "            missing = True\n",
        "            break\n",
        "\n",
        "        with np.load(full_path) as data:\n",
        "            keys = sorted(list(data.keys()))  # keep stable order\n",
        "            weights = [data[k] for k in keys]\n",
        "            init_weights_list.append(weights)\n",
        "\n",
        "    if missing or len(init_weights_list) < CLIENTS_PER_COMBINATION:\n",
        "        continue\n",
        "\n",
        "    # ---- (B) Initial global weights = FedAvg(local weights)\n",
        "    global_weights = fedavg(init_weights_list, sample_counts=None)\n",
        "\n",
        "    # ---- (C) 1 federated round with 2 local epochs\n",
        "    for r in range(GLOBAL_ROUNDS):\n",
        "        updated_weights_list = []\n",
        "        sample_counts = []\n",
        "\n",
        "        for cid in client_ids:\n",
        "            row = df[df[\"Client_ID\"] == cid].iloc[0]\n",
        "            ds, n_samp = get_client_dataset(cid, row, batch_size=BATCH_SIZE)\n",
        "\n",
        "            local_model = build_model()\n",
        "            local_model.set_weights(global_weights)\n",
        "            local_model.fit(ds, epochs=LOCAL_EPOCHS, verbose=0)\n",
        "\n",
        "            updated_weights_list.append(local_model.get_weights())\n",
        "            sample_counts.append(n_samp)\n",
        "\n",
        "        global_weights = fedavg(updated_weights_list, sample_counts=sample_counts)\n",
        "\n",
        "    # ---- (D) Evaluate final global model on eval subset\n",
        "    global_model = build_model()\n",
        "    global_model.set_weights(global_weights)\n",
        "    _, acc = global_model.evaluate(eval_data, eval_labels, verbose=0)\n",
        "    global_accuracy = float(acc * 100.0)\n",
        "\n",
        "    # ---- (E) QoS metrics (same as your MNIST combo code)\n",
        "    total_volume = clients_data[\"DataVolume(Samples)\"].sum() if \"DataVolume(Samples)\" in clients_data.columns else np.nan\n",
        "    total_latency = clients_data[\"Latency(ms)\"].sum() if \"Latency(ms)\" in clients_data.columns else np.nan\n",
        "    mean_quality = clients_data[\"Mean_Quality_Factor(%)\"].mean() if \"Mean_Quality_Factor(%)\" in clients_data.columns else np.nan\n",
        "    mean_reliability = clients_data[\"Reliability_Score\"].mean() if \"Reliability_Score\" in clients_data.columns else np.nan\n",
        "\n",
        "    weight_paths = [os.path.join(WEIGHTS_DIR, f\"client_{cid}_local.npz\") for cid in client_ids]\n",
        "\n",
        "    records.append({\n",
        "        \"Combination_ID\": combo_id,\n",
        "        **{f\"Client_{i+1}\": cid for i, cid in enumerate(client_ids)},\n",
        "        \"Weights_Paths\": weight_paths,\n",
        "        \"Global_DataVolume\": total_volume,\n",
        "        \"Global_Latency\": total_latency,\n",
        "        \"Global_Mean_Quality_Factor\": mean_quality,\n",
        "        \"Global_Reliability_Score\": mean_reliability,\n",
        "        \"Global_Accuracy\": global_accuracy,\n",
        "    })\n",
        "\n",
        "    if combo_id % 100 == 0:\n",
        "        print(f\"‚úÖ Processed {combo_id}/{len(combinations)} combinations\")\n",
        "\n",
        "# ========================\n",
        "# SAVE FINAL DATAFRAME\n",
        "# ========================\n",
        "df_combos = pd.DataFrame(records)\n",
        "df_combos.to_csv(OUT_CSV, index=False)\n",
        "print(f\"‚úÖ All combinations processed and saved to CSV: {OUT_CSV}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CIFAR10 Manual Composition**\n",
        "---"
      ],
      "metadata": {
        "id": "OAOUVk4BOHIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# ========================\n",
        "from google.colab import drive\n",
        "import os, re, json, itertools, time\n",
        "drive.mount('/content/drive')\n",
        "# ========================\n",
        "# IMPORTS\n",
        "# ========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ========================\n",
        "# PATHS (EDIT ONLY THESE IF NEEDED)\n",
        "# ========================\n",
        "# ‚úÖ Must contain: client_<id>_local.npz  (you currently have 1..30)\n",
        "WEIGHTS_DIR  = \"/content/drive/MyDrive/MLaaS_Weights_20_CIFAR\"\n",
        "\n",
        "# ‚úÖ Your CIFAR profile CSV (must have Client_ID, plus QoS columns if you want them aggregated)\n",
        "PROFILES_CSV = \"/content/CIFAR10_Client_Profiles_For_Composability_30_30.csv\"\n",
        "\n",
        "# Output CSV\n",
        "OUT_CSV      = \"client_combinations_with_qos_custom_CIFAR.csv\"\n",
        "\n",
        "# ========================\n",
        "# USER INPUT\n",
        "# ========================\n",
        "num_clients_to_sample = int(input(\"üî¢ Enter how many clients to sample (n): \"))\n",
        "\n",
        "# ========================\n",
        "# CONFIG (SAME AS YOUR MNIST/FMNISt COMBO LOGIC)\n",
        "# ========================\n",
        "CLIENTS_PER_COMBINATION = 3  # set to 10 (change if you want)\n",
        "LOCAL_EPOCHS = 2               # ‚úÖ two epochs\n",
        "GLOBAL_ROUNDS = 1              # ‚úÖ one round\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# ========================\n",
        "# LOAD CLIENT METADATA\n",
        "# ========================\n",
        "df = pd.read_csv(PROFILES_CSV)\n",
        "df[\"Client_ID\"] = df[\"Client_ID\"].astype(int)\n",
        "\n",
        "# Optional: label columns if present (Label0..Label9)\n",
        "label_cols = [c for c in df.columns if c.lower().startswith(\"label\")]\n",
        "\n",
        "# ========================\n",
        "# READ AVAILABLE WEIGHT IDS (FIXES \"MISSING WEIGHTS\")\n",
        "# ========================\n",
        "npz_files = [f for f in os.listdir(WEIGHTS_DIR) if f.endswith(\".npz\")]\n",
        "available_ids = set()\n",
        "for f in npz_files:\n",
        "    m = re.match(r\"client_(\\d+)_local\\.npz$\", f)\n",
        "    if m:\n",
        "        available_ids.add(int(m.group(1)))\n",
        "\n",
        "print(\"‚úÖ NPZ weights found:\", len(available_ids))\n",
        "if len(available_ids) == 0:\n",
        "    raise FileNotFoundError(f\"No client_<id>_local.npz files found in: {WEIGHTS_DIR}\")\n",
        "\n",
        "print(\"‚úÖ Weight ID range:\", min(available_ids), \"-\", max(available_ids))\n",
        "\n",
        "# Filter CSV to only clients that have weights\n",
        "df = df[df[\"Client_ID\"].isin(available_ids)].copy()\n",
        "print(\"‚úÖ Clients available in CSV after filtering:\", len(df))\n",
        "\n",
        "if len(df) < CLIENTS_PER_COMBINATION:\n",
        "    raise ValueError(\n",
        "        f\"Not enough clients with weights to form one combination of size {CLIENTS_PER_COMBINATION}. \"\n",
        "        f\"Available clients: {len(df)}\"\n",
        "    )\n",
        "\n",
        "# Adjust sample size safely\n",
        "if num_clients_to_sample > len(df):\n",
        "    print(f\"‚ö†Ô∏è You asked n={num_clients_to_sample}, but only {len(df)} clients have weights. Using n={len(df)}.\")\n",
        "    num_clients_to_sample = len(df)\n",
        "\n",
        "# ========================\n",
        "# CIFAR MODEL (MUST MATCH YOUR SAVED WEIGHTS ARCHITECTURE)\n",
        "# ========================\n",
        "def build_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(32, 32, 3)),\n",
        "        layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\"),\n",
        "        layers.Conv2D(32, (3,3), padding=\"same\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "\n",
        "        layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\"),\n",
        "        layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\"),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.Dense(10, activation=\"softmax\"),\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ========================\n",
        "# LOAD CIFAR-10 (train + test)\n",
        "# ========================\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "y_train = y_train.astype(\"int32\").flatten()\n",
        "y_test  = y_test.astype(\"int32\").flatten()\n",
        "\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test  = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Evaluation subset\n",
        "eval_data   = x_test[:1000]\n",
        "eval_labels = y_test[:1000]\n",
        "\n",
        "# ========================\n",
        "# LABEL POOLS (for non-IID reconstruction if Label* exists)\n",
        "# ========================\n",
        "train_indices_by_label = {k: np.where(y_train == k)[0] for k in range(10)}\n",
        "label_cursors = {k: 0 for k in range(10)}\n",
        "\n",
        "def _take_indices_for_label(label, count):\n",
        "    pool = train_indices_by_label[label]\n",
        "    if count <= 0:\n",
        "        return np.array([], dtype=np.int64)\n",
        "\n",
        "    start = label_cursors[label]\n",
        "    end = start + count\n",
        "\n",
        "    if end <= len(pool):\n",
        "        out = pool[start:end]\n",
        "    else:\n",
        "        part1 = pool[start:]\n",
        "        remaining = end - len(pool)\n",
        "        part2 = pool[:remaining]\n",
        "        out = np.concatenate([part1, part2], axis=0)\n",
        "\n",
        "    label_cursors[label] = end % len(pool)\n",
        "    return out\n",
        "\n",
        "def get_client_dataset(cid, client_row, batch_size=64):\n",
        "    # pick sample size from CSV if possible\n",
        "    if \"DataVolume(Samples)\" in client_row.index and not pd.isna(client_row[\"DataVolume(Samples)\"]):\n",
        "        n_samples = int(client_row[\"DataVolume(Samples)\"])\n",
        "    else:\n",
        "        n_samples = 500\n",
        "    n_samples = max(1, n_samples)\n",
        "\n",
        "    # If Label columns exist: reconstruct label-based sampling\n",
        "    if len(label_cols) >= 10:\n",
        "        counts = []\n",
        "        for k in range(10):\n",
        "            candidates = [c for c in label_cols if c.lower().endswith(str(k))]\n",
        "            if len(candidates) == 0:\n",
        "                counts.append(0)\n",
        "            else:\n",
        "                v = client_row[candidates[0]]\n",
        "                counts.append(int(v) if not pd.isna(v) else 0)\n",
        "\n",
        "        total = sum(counts)\n",
        "\n",
        "        if total <= 0:\n",
        "            rng = np.random.RandomState(int(cid) + 123)\n",
        "            idx = rng.choice(len(x_train), size=n_samples, replace=(n_samples > len(x_train)))\n",
        "        else:\n",
        "            # scale counts to n_samples\n",
        "            if total != n_samples:\n",
        "                scaled = np.array(counts, dtype=np.float64) / float(total)\n",
        "                counts = np.floor(scaled * n_samples).astype(int).tolist()\n",
        "                gap = n_samples - sum(counts)\n",
        "                if gap > 0:\n",
        "                    order = np.argsort(-scaled)\n",
        "                    for i in range(gap):\n",
        "                        counts[int(order[i % 10])] += 1\n",
        "\n",
        "            idx_parts = []\n",
        "            for k in range(10):\n",
        "                idx_parts.append(_take_indices_for_label(k, counts[k]))\n",
        "            idx = np.concatenate(idx_parts, axis=0)\n",
        "    else:\n",
        "        # deterministic sampling if no label columns in profile\n",
        "        rng = np.random.RandomState(int(cid) + 123)\n",
        "        idx = rng.choice(len(x_train), size=n_samples, replace=(n_samples > len(x_train)))\n",
        "\n",
        "    x_c = x_train[idx]\n",
        "    y_c = y_train[idx]\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_c, y_c)).shuffle(\n",
        "        buffer_size=min(2000, len(y_c)),\n",
        "        seed=int(cid) + 999,\n",
        "        reshuffle_each_iteration=True\n",
        "    ).batch(batch_size)\n",
        "\n",
        "    return ds, len(y_c)\n",
        "\n",
        "# ========================\n",
        "# ‚úÖ FIX 1: LOAD NPZ WEIGHTS IN CORRECT ORDER (NO STRING SORT BUG)\n",
        "# ========================\n",
        "def load_npz_weights(npz_path):\n",
        "    with np.load(npz_path) as data:\n",
        "        # data.files looks like: ['arr_0','arr_1',...]\n",
        "        keys = sorted(data.files, key=lambda k: int(k.split(\"_\")[1]))  # numeric sort\n",
        "        return [data[k] for k in keys]\n",
        "\n",
        "# ========================\n",
        "# FedAvg\n",
        "# ========================\n",
        "def fedavg(weights_list, sample_counts=None):\n",
        "    if sample_counts is None:\n",
        "        return [np.mean(np.stack(ws, axis=0), axis=0) for ws in zip(*weights_list)]\n",
        "\n",
        "    sample_counts = np.asarray(sample_counts, dtype=np.float64)\n",
        "    sample_counts = sample_counts / sample_counts.sum()\n",
        "\n",
        "    avg_weights = []\n",
        "    for layer_ws in zip(*weights_list):\n",
        "        stacked = np.stack(layer_ws, axis=0)\n",
        "        w = sample_counts.reshape((-1,) + (1,) * (stacked.ndim - 1))\n",
        "        avg_weights.append(np.sum(stacked * w, axis=0))\n",
        "    return avg_weights\n",
        "\n",
        "# ========================\n",
        "# SAMPLE CLIENTS & GENERATE COMBINATIONS\n",
        "# ========================\n",
        "selected_df = df.sample(n=num_clients_to_sample, random_state=42)\n",
        "selected_ids = selected_df[\"Client_ID\"].tolist()\n",
        "\n",
        "combinations = list(itertools.combinations(selected_ids, CLIENTS_PER_COMBINATION))\n",
        "print(f\"üîÅ Total combinations to evaluate: {len(combinations)}\")\n",
        "\n",
        "# ========================\n",
        "# COMBINATION EVALUATION\n",
        "# ========================\n",
        "records = []\n",
        "\n",
        "for combo_id, client_ids in enumerate(combinations):\n",
        "    clients_data = df[df[\"Client_ID\"].isin(client_ids)].copy()\n",
        "\n",
        "    # ---- (A) Load local weights\n",
        "    init_weights_list = []\n",
        "    missing_clients = []\n",
        "\n",
        "    for cid in client_ids:\n",
        "        full_path = os.path.join(WEIGHTS_DIR, f\"client_{cid}_local.npz\")\n",
        "        if not os.path.exists(full_path):\n",
        "            missing_clients.append(cid)\n",
        "            continue\n",
        "        init_weights_list.append(load_npz_weights(full_path))\n",
        "\n",
        "    # If any missing, skip this combo (should not happen after filtering, but safe)\n",
        "    if missing_clients or len(init_weights_list) < CLIENTS_PER_COMBINATION:\n",
        "        print(f\"‚ö†Ô∏è Combo {combo_id} skipped, missing weights for: {missing_clients}\")\n",
        "        continue\n",
        "\n",
        "    # ---- (B) Initial global weights = FedAvg(local weights)\n",
        "    global_weights = fedavg(init_weights_list)\n",
        "\n",
        "    # ---- (C) Safety check: ensure shapes match model\n",
        "    test_model = build_model()\n",
        "    expected_shapes = [w.shape for w in test_model.get_weights()]\n",
        "    got_shapes = [w.shape for w in global_weights]\n",
        "    if expected_shapes != got_shapes:\n",
        "        print(\"‚ùå Weight shape mismatch in combo:\", combo_id, \"clients:\", client_ids)\n",
        "        print(\"Expected:\", expected_shapes)\n",
        "        print(\"Got     :\", got_shapes)\n",
        "        raise ValueError(\"Loaded weights do not match model architecture/order. Fix your saved model or weight loading.\")\n",
        "\n",
        "    # ---- (D) 1 global round, 2 local epochs\n",
        "    for _ in range(GLOBAL_ROUNDS):\n",
        "        updated_weights_list = []\n",
        "        sample_counts = []\n",
        "\n",
        "        for cid in client_ids:\n",
        "            row = df[df[\"Client_ID\"] == cid].iloc[0]\n",
        "            ds, n_samp = get_client_dataset(cid, row, batch_size=BATCH_SIZE)\n",
        "\n",
        "            local_model = build_model()\n",
        "            local_model.set_weights(global_weights)\n",
        "            local_model.fit(ds, epochs=LOCAL_EPOCHS, verbose=0)\n",
        "\n",
        "            updated_weights_list.append(local_model.get_weights())\n",
        "            sample_counts.append(n_samp)\n",
        "\n",
        "        global_weights = fedavg(updated_weights_list, sample_counts=sample_counts)\n",
        "\n",
        "    # ---- (E) Evaluate final global model\n",
        "    global_model = build_model()\n",
        "    global_model.set_weights(global_weights)\n",
        "    _, acc = global_model.evaluate(eval_data, eval_labels, verbose=0)\n",
        "    global_accuracy = float(acc * 100.0)\n",
        "\n",
        "    # ---- (F) QoS metrics (if columns exist)\n",
        "    total_volume = clients_data[\"DataVolume(Samples)\"].sum() if \"DataVolume(Samples)\" in clients_data.columns else np.nan\n",
        "    total_latency = clients_data[\"Latency(ms)\"].sum() if \"Latency(ms)\" in clients_data.columns else np.nan\n",
        "    mean_quality = clients_data[\"Mean_Quality_Factor(%)\"].mean() if \"Mean_Quality_Factor(%)\" in clients_data.columns else np.nan\n",
        "    mean_reliability = clients_data[\"Reliability_Score\"].mean() if \"Reliability_Score\" in clients_data.columns else np.nan\n",
        "\n",
        "    weight_paths = [os.path.join(WEIGHTS_DIR, f\"client_{cid}_local.npz\") for cid in client_ids]\n",
        "\n",
        "    records.append({\n",
        "        \"Combination_ID\": combo_id,\n",
        "        **{f\"Client_{i+1}\": cid for i, cid in enumerate(client_ids)},\n",
        "        \"Weights_Paths\": weight_paths,\n",
        "        \"Global_DataVolume\": total_volume,\n",
        "        \"Global_Latency\": total_latency,\n",
        "        \"Global_Mean_Quality_Factor\": mean_quality,\n",
        "        \"Global_Reliability_Score\": mean_reliability,\n",
        "        \"Global_Accuracy\": global_accuracy,\n",
        "    })\n",
        "\n",
        "    if combo_id % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {combo_id}/{len(combinations)} combinations\")\n",
        "df_combos = pd.DataFrame(records)\n",
        "df_combos.to_csv(OUT_CSV, index=False)\n",
        "print(f\"‚úÖ Saved: {OUT_CSV} | Rows: {len(df_combos)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhjmAqKZWP95",
        "outputId": "c3ab6966-c4da-4944-cff0-6f891e4d9513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üî¢ Enter how many clients to sample (n): 13\n",
            "‚úÖ NPZ weights found: 30\n",
            "‚úÖ Weight ID range: 1 - 30\n",
            "‚úÖ Clients available in CSV after filtering: 30\n",
            "üîÅ Total combinations to evaluate: 286\n",
            "‚úÖ Processed 0/286 combinations\n",
            "‚úÖ Processed 50/286 combinations\n",
            "‚úÖ Processed 100/286 combinations\n",
            "‚úÖ Processed 150/286 combinations\n",
            "‚úÖ Processed 200/286 combinations\n",
            "‚úÖ Processed 250/286 combinations\n",
            "‚úÖ Saved: client_combinations_with_qos_custom_CIFAR.csv | Rows: 286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_combos.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "us7uHyKTfwLD",
        "outputId": "448fcd7b-405c-4944-b17e-5f19d20a1664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Combination_ID    Client_1    Client_2    Client_3  Global_DataVolume  \\\n",
              "count        286.0000  286.000000  286.000000  286.000000         286.000000   \n",
              "mean         142.5000   19.891608   15.486014   11.006993        7210.153846   \n",
              "std           82.7053    7.137654    8.756640    8.010516        1384.425664   \n",
              "min            0.0000    1.000000    1.000000    1.000000        3239.000000   \n",
              "25%           71.2500   16.000000    9.000000    5.000000        6198.500000   \n",
              "50%          142.5000   18.000000   13.000000    6.000000        7170.000000   \n",
              "75%          213.7500   28.000000   24.000000   17.000000        8261.250000   \n",
              "max          285.0000   29.000000   29.000000   29.000000       10325.000000   \n",
              "\n",
              "       Global_Latency  Global_Mean_Quality_Factor  Global_Reliability_Score  \\\n",
              "count      286.000000                  286.000000                286.000000   \n",
              "mean     35315.325847                   24.474573                  0.538462   \n",
              "std       3641.155727                    4.028871                  0.076197   \n",
              "min      25325.002670                   12.962591                  0.300000   \n",
              "25%      32773.464978                   21.538224                  0.500000   \n",
              "50%      35402.976155                   24.298181                  0.533333   \n",
              "75%      37908.545673                   27.632379                  0.600000   \n",
              "max      44096.368790                   33.503737                  0.700000   \n",
              "\n",
              "       Global_Accuracy  \n",
              "count       286.000000  \n",
              "mean         29.044406  \n",
              "std           5.971290  \n",
              "min          13.200000  \n",
              "25%          25.025000  \n",
              "50%          29.400000  \n",
              "75%          33.675001  \n",
              "max          41.900000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e8ee57d5-4c6d-4821-8a02-48362e9d31d4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Combination_ID</th>\n",
              "      <th>Client_1</th>\n",
              "      <th>Client_2</th>\n",
              "      <th>Client_3</th>\n",
              "      <th>Global_DataVolume</th>\n",
              "      <th>Global_Latency</th>\n",
              "      <th>Global_Mean_Quality_Factor</th>\n",
              "      <th>Global_Reliability_Score</th>\n",
              "      <th>Global_Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>286.0000</td>\n",
              "      <td>286.000000</td>\n",
              "      <td>286.000000</td>\n",
              "      <td>286.000000</td>\n",
              "      <td>286.000000</td>\n",
              "      <td>286.000000</td>\n",
              "      <td>286.000000</td>\n",
              "      <td>286.000000</td>\n",
              "      <td>286.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>142.5000</td>\n",
              "      <td>19.891608</td>\n",
              "      <td>15.486014</td>\n",
              "      <td>11.006993</td>\n",
              "      <td>7210.153846</td>\n",
              "      <td>35315.325847</td>\n",
              "      <td>24.474573</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>29.044406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>82.7053</td>\n",
              "      <td>7.137654</td>\n",
              "      <td>8.756640</td>\n",
              "      <td>8.010516</td>\n",
              "      <td>1384.425664</td>\n",
              "      <td>3641.155727</td>\n",
              "      <td>4.028871</td>\n",
              "      <td>0.076197</td>\n",
              "      <td>5.971290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3239.000000</td>\n",
              "      <td>25325.002670</td>\n",
              "      <td>12.962591</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>13.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>71.2500</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>6198.500000</td>\n",
              "      <td>32773.464978</td>\n",
              "      <td>21.538224</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>25.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>142.5000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>7170.000000</td>\n",
              "      <td>35402.976155</td>\n",
              "      <td>24.298181</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>29.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>213.7500</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>8261.250000</td>\n",
              "      <td>37908.545673</td>\n",
              "      <td>27.632379</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>33.675001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>285.0000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>10325.000000</td>\n",
              "      <td>44096.368790</td>\n",
              "      <td>33.503737</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>41.900000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8ee57d5-4c6d-4821-8a02-48362e9d31d4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e8ee57d5-4c6d-4821-8a02-48362e9d31d4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e8ee57d5-4c6d-4821-8a02-48362e9d31d4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e9ef3750-b8ba-48fb-a72f-c94302e5c3ab\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e9ef3750-b8ba-48fb-a72f-c94302e5c3ab')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e9ef3750-b8ba-48fb-a72f-c94302e5c3ab button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_combos\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Combination_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 102.85736238409962,\n        \"min\": 0.0,\n        \"max\": 286.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          286.0,\n          142.5,\n          213.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Client_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 95.57535117377765,\n        \"min\": 1.0,\n        \"max\": 286.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          19.89160839160839,\n          18.0,\n          286.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Client_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 96.46062992002223,\n        \"min\": 1.0,\n        \"max\": 286.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          15.486013986013987,\n          13.0,\n          286.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Client_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 97.61484683312553,\n        \"min\": 1.0,\n        \"max\": 286.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          11.006993006993007,\n          6.0,\n          286.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Global_DataVolume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3512.055431916824,\n        \"min\": 286.0,\n        \"max\": 10325.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          7210.153846153846,\n          7170.0,\n          286.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Global_Latency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16240.395346859892,\n        \"min\": 286.0,\n        \"max\": 44096.36878967285,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          35315.325847038854,\n          35402.97615528107,\n          286.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Global_Mean_Quality_Factor\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 94.05735586298024,\n        \"min\": 4.028871478529862,\n        \"max\": 286.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          24.47457304367652,\n          24.298180813590687,\n          286.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Global_Reliability_Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 100.95240752671288,\n        \"min\": 0.07619690224169927,\n        \"max\": 286.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5384615384615384,\n          0.5333333333333333,\n          286.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Global_Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 92.80753784399512,\n        \"min\": 5.971289760160604,\n        \"max\": 286.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          29.044405532466783,\n          29.399999976158142,\n          286.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_combos.to_csv(\"Updated_combination_3_CIFAR_20_20.csv\")"
      ],
      "metadata": {
        "id": "erOhzf-jf0ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HAR Manual Composition**\n",
        "---"
      ],
      "metadata": {
        "id": "hFrWrX5oD8aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# MOUNT GOOGLE DRIVE\n",
        "# ========================\n",
        "from google.colab import drive\n",
        "import os, re, json, itertools, time\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ========================\n",
        "# IMPORTS\n",
        "# ========================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# ========================\n",
        "# PATHS (EDIT ONLY THESE IF NEEDED)\n",
        "# ========================\n",
        "# ‚úÖ Must contain: client_<id>_local.npz   (e.g., client_1_local.npz ...)\n",
        "WEIGHTS_DIR  = \"/content/drive/MyDrive/MLaaS_Weights_20_HAR\"\n",
        "\n",
        "# ‚úÖ Your HAR client profile CSV (must contain Client_ID, plus QoS columns if you want them aggregated)\n",
        "PROFILES_CSV = \"/content/HAR_Client_Profiles_For_Composability_30_30.csv\"\n",
        "\n",
        "# ‚úÖ Your HAR dataset CSV (KEEP DATASET AS-IS)\n",
        "DATASET_CSV  = \"/content/drive/My Drive/Early Drift Detection/pamap2_final.csv\"\n",
        "\n",
        "# Output CSV\n",
        "OUT_CSV      = \"client_combinations_with_qos_custom_HAR.csv\"\n",
        "\n",
        "# ========================\n",
        "# USER INPUT\n",
        "# ========================\n",
        "num_clients_to_sample = int(input(\"üî¢ Enter how many clients to sample (n): \"))\n",
        "\n",
        "# ========================\n",
        "# CONFIG (SAME COMBO LOGIC)\n",
        "# ========================\n",
        "CLIENTS_PER_COMBINATION = 3    # ‚úÖ set 2 => if n=30 then combos=435 (same behavior you saw)\n",
        "LOCAL_EPOCHS = 2               # ‚úÖ two epochs\n",
        "GLOBAL_ROUNDS = 1              # ‚úÖ one round\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# ========================\n",
        "# LOAD CLIENT METADATA\n",
        "# ========================\n",
        "dfp = pd.read_csv(PROFILES_CSV)\n",
        "dfp[\"Client_ID\"] = dfp[\"Client_ID\"].astype(int)\n",
        "\n",
        "label_cols = [c for c in dfp.columns if c.lower().startswith(\"label\")]\n",
        "\n",
        "# ========================\n",
        "# READ AVAILABLE WEIGHT IDS (FIXES \"MISSING WEIGHTS\")\n",
        "# ========================\n",
        "npz_files = [f for f in os.listdir(WEIGHTS_DIR) if f.endswith(\".npz\")]\n",
        "available_ids = set()\n",
        "for f in npz_files:\n",
        "    m = re.match(r\"client_(\\d+)_local\\.npz$\", f)\n",
        "    if m:\n",
        "        available_ids.add(int(m.group(1)))\n",
        "\n",
        "print(\"‚úÖ NPZ weights found:\", len(available_ids))\n",
        "if len(available_ids) == 0:\n",
        "    raise FileNotFoundError(f\"No client_<id>_local.npz files found in: {WEIGHTS_DIR}\")\n",
        "\n",
        "print(\"‚úÖ Weight ID range:\", min(available_ids), \"-\", max(available_ids))\n",
        "\n",
        "# Filter profiles to only clients that have weights\n",
        "dfp = dfp[dfp[\"Client_ID\"].isin(available_ids)].copy()\n",
        "print(\"‚úÖ Clients available in profile CSV after filtering:\", len(dfp))\n",
        "\n",
        "if len(dfp) < CLIENTS_PER_COMBINATION:\n",
        "    raise ValueError(\n",
        "        f\"Not enough clients with weights to form one combination of size {CLIENTS_PER_COMBINATION}. \"\n",
        "        f\"Available clients: {len(dfp)}\"\n",
        "    )\n",
        "\n",
        "if num_clients_to_sample > len(dfp):\n",
        "    print(f\"‚ö†Ô∏è You asked n={num_clients_to_sample}, but only {len(dfp)} clients have weights. Using n={len(dfp)}.\")\n",
        "    num_clients_to_sample = len(dfp)\n",
        "\n",
        "# ========================\n",
        "# LOAD HAR DATASET (KEEP AS-IS, but make train pool for reconstruction)\n",
        "# ========================\n",
        "df = pd.read_csv(DATASET_CSV)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Encode activity\n",
        "le_activity = LabelEncoder()\n",
        "df[\"activity\"] = le_activity.fit_transform(df[\"activity\"])\n",
        "\n",
        "# Encode subject if present (keeps your earlier original-style logic)\n",
        "if \"subject\" in df.columns:\n",
        "    le_subject = LabelEncoder()\n",
        "    df[\"subject\"] = le_subject.fit_transform(df[\"subject\"])\n",
        "\n",
        "# Build X/y (drop activity and subject from features)\n",
        "drop_cols = [\"activity\"]\n",
        "if \"subject\" in df.columns:\n",
        "    drop_cols.append(\"subject\")\n",
        "\n",
        "X = df.drop(drop_cols, axis=1).values.astype(\"float32\")\n",
        "y = df[\"activity\"].values.astype(\"int32\")\n",
        "\n",
        "FEATURE_COUNT = X.shape[1]\n",
        "NUM_CLASSES   = len(np.unique(y))\n",
        "\n",
        "# Global train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# Standardize (helps MLP stability)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train).astype(\"float32\")\n",
        "X_test  = scaler.transform(X_test).astype(\"float32\")\n",
        "\n",
        "print(f\"üì¶ HAR Loaded ‚Üí Train: {len(X_train)}, Test: {len(X_test)}, Features: {FEATURE_COUNT}, Classes: {NUM_CLASSES}\")\n",
        "\n",
        "# eval subset\n",
        "eval_data   = X_test[:1000]\n",
        "eval_labels = y_test[:1000]\n",
        "\n",
        "# ========================\n",
        "# LABEL POOLS (for reconstruction if profile has Label0..LabelK)\n",
        "# ========================\n",
        "train_indices_by_label = {k: np.where(y_train == k)[0] for k in range(NUM_CLASSES)}\n",
        "label_cursors = {k: 0 for k in range(NUM_CLASSES)}\n",
        "\n",
        "def _take_indices_for_label(label, count):\n",
        "    pool = train_indices_by_label[label]\n",
        "    if count <= 0:\n",
        "        return np.array([], dtype=np.int64)\n",
        "\n",
        "    start = label_cursors[label]\n",
        "    end = start + count\n",
        "\n",
        "    if end <= len(pool):\n",
        "        out = pool[start:end]\n",
        "    else:\n",
        "        part1 = pool[start:]\n",
        "        remaining = end - len(pool)\n",
        "        part2 = pool[:remaining]\n",
        "        out = np.concatenate([part1, part2], axis=0)\n",
        "\n",
        "    label_cursors[label] = end % len(pool)\n",
        "    return out\n",
        "\n",
        "def get_client_dataset(cid, profile_row, batch_size=32):\n",
        "    # choose n_samples\n",
        "    if \"DataVolume(Samples)\" in profile_row.index and not pd.isna(profile_row[\"DataVolume(Samples)\"]):\n",
        "        n_samples = int(profile_row[\"DataVolume(Samples)\"])\n",
        "    else:\n",
        "        n_samples = 500\n",
        "    n_samples = max(1, n_samples)\n",
        "\n",
        "    # If Label columns exist, reconstruct\n",
        "    if len(label_cols) >= NUM_CLASSES:\n",
        "        counts = []\n",
        "        for k in range(NUM_CLASSES):\n",
        "            candidates = [c for c in label_cols if c.lower().endswith(str(k))]\n",
        "            if len(candidates) == 0:\n",
        "                counts.append(0)\n",
        "            else:\n",
        "                v = profile_row[candidates[0]]\n",
        "                counts.append(int(v) if not pd.isna(v) else 0)\n",
        "\n",
        "        total = sum(counts)\n",
        "\n",
        "        if total <= 0:\n",
        "            rng = np.random.RandomState(int(cid) + 123)\n",
        "            idx = rng.choice(len(X_train), size=n_samples, replace=(n_samples > len(X_train)))\n",
        "        else:\n",
        "            # scale to n_samples\n",
        "            if total != n_samples:\n",
        "                scaled = np.array(counts, dtype=np.float64) / float(total)\n",
        "                counts = np.floor(scaled * n_samples).astype(int).tolist()\n",
        "                gap = n_samples - sum(counts)\n",
        "                if gap > 0:\n",
        "                    order = np.argsort(-scaled)\n",
        "                    for i in range(gap):\n",
        "                        counts[int(order[i % NUM_CLASSES])] += 1\n",
        "\n",
        "            idx_parts = []\n",
        "            for k in range(NUM_CLASSES):\n",
        "                idx_parts.append(_take_indices_for_label(k, counts[k]))\n",
        "            idx = np.concatenate(idx_parts, axis=0)\n",
        "    else:\n",
        "        rng = np.random.RandomState(int(cid) + 123)\n",
        "        idx = rng.choice(len(X_train), size=n_samples, replace=(n_samples > len(X_train)))\n",
        "\n",
        "    x_c = X_train[idx]\n",
        "    y_c = y_train[idx]\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((x_c, y_c)).shuffle(\n",
        "        buffer_size=min(2000, len(y_c)),\n",
        "        seed=int(cid) + 999,\n",
        "        reshuffle_each_iteration=True\n",
        "    ).batch(batch_size)\n",
        "\n",
        "    return ds, len(y_c)\n",
        "\n",
        "# ========================\n",
        "# HAR MODEL (MUST MATCH THE ONE USED TO SAVE client_<id>_local.npz)\n",
        "# ========================\n",
        "def build_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(FEATURE_COUNT,)),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ========================\n",
        "# ‚úÖ FIX 1: LOAD NPZ WEIGHTS IN CORRECT ORDER\n",
        "# ========================\n",
        "def load_npz_weights(npz_path):\n",
        "    with np.load(npz_path) as data:\n",
        "        keys = sorted(data.files, key=lambda k: int(k.split(\"_\")[1]))  # arr_0..arr_n numeric\n",
        "        return [data[k] for k in keys]\n",
        "\n",
        "# ========================\n",
        "# FedAvg\n",
        "# ========================\n",
        "def fedavg(weights_list, sample_counts=None):\n",
        "    if sample_counts is None:\n",
        "        return [np.mean(np.stack(ws, axis=0), axis=0) for ws in zip(*weights_list)]\n",
        "\n",
        "    sample_counts = np.asarray(sample_counts, dtype=np.float64)\n",
        "    sample_counts = sample_counts / sample_counts.sum()\n",
        "\n",
        "    avg_weights = []\n",
        "    for layer_ws in zip(*weights_list):\n",
        "        stacked = np.stack(layer_ws, axis=0)\n",
        "        w = sample_counts.reshape((-1,) + (1,) * (stacked.ndim - 1))\n",
        "        avg_weights.append(np.sum(stacked * w, axis=0))\n",
        "    return avg_weights\n",
        "\n",
        "# ========================\n",
        "# SAMPLE CLIENTS & GENERATE COMBINATIONS\n",
        "# ========================\n",
        "selected_df = dfp.sample(n=num_clients_to_sample, random_state=42)\n",
        "selected_ids = selected_df[\"Client_ID\"].tolist()\n",
        "\n",
        "combinations = list(itertools.combinations(selected_ids, CLIENTS_PER_COMBINATION))\n",
        "print(f\"üîÅ Total combinations to evaluate: {len(combinations)}\")\n",
        "\n",
        "# ========================\n",
        "# COMBINATION EVALUATION\n",
        "# ========================\n",
        "records = []\n",
        "\n",
        "for combo_id, client_ids in enumerate(combinations):\n",
        "    clients_data = dfp[dfp[\"Client_ID\"].isin(client_ids)].copy()\n",
        "\n",
        "    # ---- (A) Load local weights\n",
        "    init_weights_list = []\n",
        "    missing_clients = []\n",
        "\n",
        "    for cid in client_ids:\n",
        "        full_path = os.path.join(WEIGHTS_DIR, f\"client_{cid}_local.npz\")\n",
        "        if not os.path.exists(full_path):\n",
        "            missing_clients.append(cid)\n",
        "            continue\n",
        "        init_weights_list.append(load_npz_weights(full_path))\n",
        "\n",
        "    if missing_clients or len(init_weights_list) < CLIENTS_PER_COMBINATION:\n",
        "        print(f\"‚ö†Ô∏è Combo {combo_id} skipped, missing weights for: {missing_clients}\")\n",
        "        continue\n",
        "\n",
        "    # ---- (B) Initial global weights\n",
        "    global_weights = fedavg(init_weights_list)\n",
        "\n",
        "    # ---- (C) Safety check: shapes match\n",
        "    test_model = build_model()\n",
        "    expected_shapes = [w.shape for w in test_model.get_weights()]\n",
        "    got_shapes = [w.shape for w in global_weights]\n",
        "    if expected_shapes != got_shapes:\n",
        "        print(\"‚ùå Weight shape mismatch in combo:\", combo_id, \"clients:\", client_ids)\n",
        "        print(\"Expected:\", expected_shapes)\n",
        "        print(\"Got     :\", got_shapes)\n",
        "        raise ValueError(\"Loaded weights do not match HAR model architecture/order.\")\n",
        "\n",
        "    # ---- (D) 1 global round, 2 local epochs\n",
        "    for _ in range(GLOBAL_ROUNDS):\n",
        "        updated_weights_list = []\n",
        "        sample_counts = []\n",
        "\n",
        "        for cid in client_ids:\n",
        "            row = dfp[dfp[\"Client_ID\"] == cid].iloc[0]\n",
        "            ds, n_samp = get_client_dataset(cid, row, batch_size=BATCH_SIZE)\n",
        "\n",
        "            local_model = build_model()\n",
        "            local_model.set_weights(global_weights)\n",
        "            local_model.fit(ds, epochs=LOCAL_EPOCHS, verbose=0)\n",
        "\n",
        "            updated_weights_list.append(local_model.get_weights())\n",
        "            sample_counts.append(n_samp)\n",
        "\n",
        "        global_weights = fedavg(updated_weights_list, sample_counts=sample_counts)\n",
        "\n",
        "    # ---- (E) Evaluate final global model\n",
        "    global_model = build_model()\n",
        "    global_model.set_weights(global_weights)\n",
        "    _, acc = global_model.evaluate(eval_data, eval_labels, verbose=0)\n",
        "    global_accuracy = float(acc * 100.0)\n",
        "\n",
        "    # ---- (F) QoS metrics (if columns exist)\n",
        "    total_volume = clients_data[\"DataVolume(Samples)\"].sum() if \"DataVolume(Samples)\" in clients_data.columns else np.nan\n",
        "    total_latency = clients_data[\"Latency(ms)\"].sum() if \"Latency(ms)\" in clients_data.columns else np.nan\n",
        "    mean_quality = clients_data[\"Mean_Quality_Factor(%)\"].mean() if \"Mean_Quality_Factor(%)\" in clients_data.columns else np.nan\n",
        "    mean_reliability = clients_data[\"Reliability_Score\"].mean() if \"Reliability_Score\" in clients_data.columns else np.nan\n",
        "\n",
        "    weight_paths = [os.path.join(WEIGHTS_DIR, f\"client_{cid}_local.npz\") for cid in client_ids]\n",
        "\n",
        "    records.append({\n",
        "        \"Combination_ID\": combo_id,\n",
        "        **{f\"Client_{i+1}\": cid for i, cid in enumerate(client_ids)},\n",
        "        \"Weights_Paths\": weight_paths,\n",
        "        \"Global_DataVolume\": total_volume,\n",
        "        \"Global_Latency\": total_latency,\n",
        "        \"Global_Mean_Quality_Factor\": mean_quality,\n",
        "        \"Global_Reliability_Score\": mean_reliability,\n",
        "        \"Global_Accuracy\": global_accuracy,\n",
        "    })\n",
        "\n",
        "    if combo_id % 50 == 0:\n",
        "        print(f\"‚úÖ Processed {combo_id}/{len(combinations)} combinations\")\n",
        "\n",
        "# ========================\n",
        "# SAVE FINAL DATAFRAME\n",
        "# ========================\n",
        "df_combos = pd.DataFrame(records)\n",
        "df_combos.to_csv(OUT_CSV, index=False)\n",
        "print(f\"‚úÖ Saved: {OUT_CSV} | Rows: {len(df_combos)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPr-HVG9fcUm",
        "outputId": "e50c0d5f-be29-47b9-b0b4-409259a0b263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üî¢ Enter how many clients to sample (n): 12\n",
            "‚úÖ NPZ weights found: 30\n",
            "‚úÖ Weight ID range: 1 - 30\n",
            "‚úÖ Clients available in profile CSV after filtering: 30\n",
            "üì¶ HAR Loaded ‚Üí Train: 447813, Test: 298542, Features: 43, Classes: 5\n",
            "üîÅ Total combinations to evaluate: 220\n",
            "‚úÖ Processed 0/220 combinations\n",
            "‚úÖ Processed 50/220 combinations\n",
            "‚úÖ Processed 100/220 combinations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_combos.describe()"
      ],
      "metadata": {
        "id": "Vl_cGrROgQc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_combos.to_csv(\"Updated_combination_3_HAR_20_20.csv\")"
      ],
      "metadata": {
        "id": "llrU40VVgLwp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}