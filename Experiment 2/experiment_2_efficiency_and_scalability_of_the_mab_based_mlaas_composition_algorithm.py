# -*- coding: utf-8 -*-
"""Experiment 2: Efficiency and Scalability of the MAB-based MLaaS Composition Algorithm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QqKFpVtDH50LJ7Uho2Vys-J3BjzIwA8H

**Service Request**
---
"""

import pandas as pd
import numpy as np
import ast

df = pd.read_csv("/content/MNIST_Client_Profiles_For_Composability_20_20.csv")
ATTR_COLUMNS = [
    'DataVolume(Samples)', 'FeatureCount', 'Local_Accuracy(%)', 'Train_Time(s)',
    'Latency(ms)', 'C_p', 'BW', 'Quality_Factor',
    'Mean_Quality_Factor(%)', 'Reliability_Score'
]
WEIGHT_RANGES = {
    'DataVolume(Samples)':      (0.80, 0.90),
    'FeatureCount':             (0.20, 0.40),
    'Local_Accuracy(%)':        (0.95, 0.99),
    'Train_Time(s)':            (0.40, 0.60),
    'Latency(ms)':              (0.90, 0.95),
    'C_p':                      (0.55, 0.75),
    'BW':                       (0.50, 0.70),
    'Quality_Factor':           (0.60, 0.80),
    'Mean_Quality_Factor(%)':   (0.70, 0.85),
    'Reliability_Score':        (0.70, 0.85)
}

rng = np.random.default_rng(seed=10)
NUM_REQUESTS = 50
service_requests = []

for rid in range(1, NUM_REQUESTS + 1):
    row = {'Request_ID': rid, 'Scenario': f"Request{rid}"}
    sample_row = df.sample(1, random_state=rid).iloc[0]

    for attr in ATTR_COLUMNS:
        val = sample_row[attr]

        # üî• FIX: convert list-like strings to numeric mean
        if isinstance(val, str) and val.startswith("[") and val.endswith("]"):
            parsed = ast.literal_eval(val)
            if isinstance(parsed, list):
                val = float(np.mean(parsed))
        row[attr] = float(val)

    for attr, (low, high) in WEIGHT_RANGES.items():
        row[f"w_{attr}"] = float(rng.uniform(low, high))

    service_requests.append(row)

Service_Request_df = pd.DataFrame(service_requests)
Service_Request_df.to_csv("Service_Request.csv", index=False)

print("Service Request DataFrame created successfully:")
Service_Request_df

Service_Request_df["Dataset"]="MNIST"
Service_Request_df

Service_Request_df['Local_Accuracy(%)'].describe()

Service_Request_df.columns

Service_Request_df[['w_DataVolume(Samples)', 'w_FeatureCount', 'w_Local_Accuracy(%)',
       'w_Train_Time(s)', 'w_Latency(ms)', 'w_C_p', 'w_BW', 'w_Quality_Factor',
       'w_Mean_Quality_Factor(%)', 'w_Reliability_Score']]

"""**New Composability Model**
---
"""

# =====================================================
# CELL 1 ‚Äî UPDATED REQUEST-AWARE COMPOSABILITY MODEL
# (NO THRESHOLD_MCS, NO FIXED WEIGHTS, SES_T = mean(Service_Request Latency))
# =====================================================
import os
import ast
import numpy as np
import pandas as pd

# -----------------------------
# CONFIG (edit here)
# -----------------------------
WEIGHTS_DIR = "/content/drive/MyDrive/MLaaS_Weights_20_MNIST"
SES_ALPHA_DEFAULT = 1.0  # SES exponent

# -----------------------------
# HELPERS
# -----------------------------
def get_client_cols(df_combos: pd.DataFrame):
    return [c for c in df_combos.columns if c.startswith("Client_")]

def make_combination_string(row: pd.Series, client_cols):
    ids = []
    for c in client_cols:
        v = row[c]
        if pd.notna(v):
            ids.append(str(int(v)))
    return "_".join(ids)

def _cosine_sim(a, b, eps=1e-12):
    a = np.asarray(a, dtype=np.float64)
    b = np.asarray(b, dtype=np.float64)
    na, nb = np.linalg.norm(a), np.linalg.norm(b)
    if na < eps or nb < eps:
        return 0.0
    return float(np.dot(a, b) / (na * nb))

def _clip01(x):
    if x is None or not np.isfinite(x):
        return np.nan
    return float(np.clip(x, 0.0, 1.0))

def _safe_float(x, default=np.nan):
    try:
        return float(x)
    except Exception:
        return float(default)

# -----------------------------
# SERVICE REQUEST: weights mapping
# -----------------------------
def build_rule_weights_from_service_request(sr_row: pd.Series) -> dict:
    """
    EXACT mapping you requested:
      DHS = w_DataVolume(Samples)
      SHS = w_Train_Time(s)
      SES = w_Latency(ms)
      HSQ = w_Quality_Factor
      SRS = w_Reliability_Score
      MUS = w_Mean_Quality_Factor(%)
    """
    return {
        "DHS": _safe_float(sr_row["w_DataVolume(Samples)"]),
        "SHS": _safe_float(sr_row["w_Train_Time(s)"]),
        "SES": _safe_float(sr_row["w_Latency(ms)"]),
        "HSQ": _safe_float(sr_row["w_Quality_Factor"]),
        "SRS": _safe_float(sr_row["w_Reliability_Score"]),
        "MUS": _safe_float(sr_row["w_Mean_Quality_Factor(%)"]),
    }

def get_shs_alpha_beta_from_service_request(sr_row: pd.Series, eps=1e-12):
    """
    SHS internal mixing:
      alpha = w_C_p
      beta  = w_BW
    Normalize to alpha + beta = 1.
    """
    a = _safe_float(sr_row["w_C_p"])
    b = _safe_float(sr_row["w_BW"])
    s = a + b
    if np.isfinite(s) and s > eps:
        return float(a / s), float(b / s)
    # fallback only if request weights are missing/invalid
    return 0.5, 0.5

# -----------------------------
# GROUP-LEVEL RULES
# -----------------------------
def compute_dhs_group(client_profiles: pd.DataFrame) -> float:
    label_cols = [c for c in client_profiles.columns if c.startswith("Label")]
    if not label_cols:
        return 0.0
    dists = client_profiles[label_cols].to_numpy(dtype=np.float64)
    terms = []
    for d in dists:
        s = d.sum()
        if s > 0:
            terms.append((d.max() - d.min()) / s)
    return float(1.0 - np.mean(terms)) if terms else 0.0

def compute_shs_group(client_profiles: pd.DataFrame, alpha: float, beta: float, eps=1e-12) -> float:
    """
    SHS using C_p and BW (request controls alpha/beta):
      1) min-max scale C_p and BW inside the group
      2) deviation from group mean
      3) SHS = 1 - mean deviation
    """
    if "C_p" not in client_profiles.columns or "BW" not in client_profiles.columns:
        return 0.0

    cp = client_profiles["C_p"].astype(float).to_numpy()
    bw = client_profiles["BW"].astype(float).to_numpy()
    if cp.size == 0 or bw.size == 0:
        return 1.0

    cp_min, cp_max = float(np.min(cp)), float(np.max(cp))
    bw_min, bw_max = float(np.min(bw)), float(np.max(bw))

    cp_scaled = np.ones_like(cp, dtype=np.float64) if abs(cp_max - cp_min) < eps else (cp - cp_min) / (cp_max - cp_min)
    bw_scaled = np.ones_like(bw, dtype=np.float64) if abs(bw_max - bw_min) < eps else (bw - bw_min) / (bw_max - bw_min)

    mu_cp = cp_scaled.mean()
    mu_bw = bw_scaled.mean()

    dev = float(alpha) * np.abs(cp_scaled - mu_cp) + float(beta) * np.abs(bw_scaled - mu_bw)
    shs = 1.0 - dev.mean()
    return float(np.clip(shs, 0.0, 1.0))

def compute_ses_group(client_profiles: pd.DataFrame, T: float, alpha: float = 1.0) -> float:
    """
    ‚úÖ SES (latency-only):
    T MUST be SES_T_MEAN = mean(Service_Request_df['Latency(ms)'])
    """
    if "Latency(ms)" not in client_profiles.columns:
        return 0.0

    latencies = client_profiles["Latency(ms)"].astype(float).to_numpy()
    if latencies.size == 0:
        return 1.0

    mean_latency = float(latencies.mean())
    if mean_latency <= float(T):
        return 1.0
    return float((float(T) / mean_latency) ** float(alpha))

def _to_quality_vector(x):
    try:
        return np.asarray(ast.literal_eval(x), dtype=np.float64)
    except Exception:
        return None

def compute_hsq_group(client_profiles: pd.DataFrame) -> float:
    if "Quality_Factor" not in client_profiles.columns:
        return 0.0

    vecs = []
    for q in client_profiles["Quality_Factor"].values:
        v = _to_quality_vector(q)
        if v is not None and v.size > 0 and np.all(np.isfinite(v)):
            vecs.append(v)

    if not vecs:
        return 0.0

    L = min(len(v) for v in vecs)
    vecs = [v[:L] for v in vecs]
    mu = np.mean(np.vstack(vecs), axis=0)
    return float(np.mean([_cosine_sim(v, mu) for v in vecs]))

def compute_srs_group(client_profiles: pd.DataFrame) -> float:
    if "Reliability_Score" not in client_profiles.columns:
        return 0.0
    r = client_profiles["Reliability_Score"].astype(float).to_numpy()
    if r.size == 0:
        return 0.0
    mu = float(r.mean())
    return float(1.0 - np.mean(np.abs(r - mu)))

# -----------------------------
# MUS (weights-based from .npz)
# -----------------------------
_VEC_CACHE = {}

def _client_weight_path(cid) -> str:
    return os.path.join(WEIGHTS_DIR, f"client_{int(cid)}_local.npz")

def _npz_to_vec(path: str) -> np.ndarray:
    if path in _VEC_CACHE:
        return _VEC_CACHE[path]
    if not os.path.exists(path):
        _VEC_CACHE[path] = np.asarray([], dtype=np.float64)
        return _VEC_CACHE[path]
    data = np.load(path, allow_pickle=True)
    vec = np.concatenate([np.asarray(data[k]).ravel() for k in data.files], axis=0) if data.files else np.asarray([], dtype=np.float64)
    _VEC_CACHE[path] = vec
    return vec

def compute_mus_for_client_ids(client_ids) -> float:
    vecs = []
    for cid in client_ids:
        v = _npz_to_vec(_client_weight_path(cid))
        if v.size > 0 and np.all(np.isfinite(v)):
            vecs.append(v)

    if not vecs:
        return np.nan

    L = min(len(v) for v in vecs)
    vecs = [v[:L] for v in vecs]
    mu = np.mean(np.vstack(vecs), axis=0)
    sims = [_cosine_sim(v, mu) for v in vecs]
    return float((np.mean(sims) + 1.0) / 2.0)

# -----------------------------
# MAIN: compute rules + request-weighted MCS (NO threshold output)
# IMPORTANT: signature matches CELL 2 call (positional / keyword-safe)
# -----------------------------
def compute_rules_and_mcs_for_client_ids(
    client_ids,
    df_profiles: pd.DataFrame,
    sr_row: pd.Series,                 # <-- service request row
    SES_T_MEAN: float,                 # <-- mean latency from Service_Request_df
    ses_alpha: float = SES_ALPHA_DEFAULT,
    eps: float = 1e-12
):
    """
    Returns dict: DHS/SHS/SES/HSQ/SRS/MUS/MCS
    - No THRESHOLD_MCS
    - Rule weights from sr_row
    - SHS alpha/beta from sr_row
    - SES threshold uses global SES_T_MEAN (mean of service requests latency)
    """
    prof = df_profiles[df_profiles["Client_ID"].isin(client_ids)]
    if prof.empty:
        return None

    # weights (rule importance)
    w_rule = build_rule_weights_from_service_request(sr_row)

    # SHS internal mixing (alpha/beta)
    shs_alpha, shs_beta = get_shs_alpha_beta_from_service_request(sr_row, eps=eps)

    # rule scores
    DHS = _clip01(compute_dhs_group(prof))
    SHS = _clip01(compute_shs_group(prof, alpha=shs_alpha, beta=shs_beta))
    SES = _clip01(compute_ses_group(prof, T=SES_T_MEAN, alpha=ses_alpha))
    HSQ = _clip01(compute_hsq_group(prof))
    SRS = _clip01(compute_srs_group(prof))
    MUS = _clip01(compute_mus_for_client_ids(client_ids))

    # weighted MCS (skip NaNs; renormalize)
    num, den = 0.0, 0.0
    for key, val in [("DHS",DHS),("SHS",SHS),("SES",SES),("HSQ",HSQ),("SRS",SRS),("MUS",MUS)]:
        w = float(w_rule.get(key, 0.0))
        if np.isfinite(val) and np.isfinite(w) and w > eps:
            num += w * float(val)
            den += w

    MCS = (num / den) if den > 0 else np.nan

    return {
        "DHS": float(DHS),
        "SHS": float(SHS),
        "SES": float(SES),
        "HSQ": float(HSQ),
        "SRS": float(SRS),
        "MUS": float(MUS) if np.isfinite(MUS) else np.nan,
        "MCS": _clip01(MCS),
    }

"""**Brute Force(Baseline)**
---
"""

import pandas as pd

# Load CSV
df = pd.read_csv("/content/Updated_combination_3_MINIST_20_20_V1.csv")

# Randomly sample 5000 rows
df_sampled = df.sample(n=2000, random_state=42)

# Optional: reset index
df_sampled = df_sampled.reset_index(drop=False)

df_sampled.to_csv("/content/sampled_combination_2_MINIST_20_FULL_EXTENDED.csv", index=False)

# =====================================================
# CELL 2 ‚Äî BRUTE FORCE: TOP-10 COMBINATIONS FOR 50 SERVICE REQUESTS
# Evaluates ALL 5,000 sampled combos for EACH request
# Saves: ~500 rows (50 * 10) + DFstatistics (per-request time + total)
# =====================================================
import pandas as pd
import numpy as np
import time
from google.colab import drive

drive.mount('/content/drive')

# =====================================================
# CONFIG
# =====================================================
NUM_SERVICE_REQUESTS = 50   # ‚úÖ you said you have 50 requests
TOP_K = 10
RANDOM_SEED = 42

# =====================================================
# INPUT FILES
# =====================================================
COMBOS_PATH       = "/content/sampled_combination_2_MINIST_20_FULL_EXTENDED.csv"  # ‚úÖ 5,000 samples
PROFILES_PATH     = "/content/MNIST_Client_Profiles_For_Composability_20_20.csv"
SERVICE_REQ_PATH  = "/content/Service_Request.csv"
# =====================================================
# OUTPUT FILES
# =====================================================
OUT_PATH = "/content/TOP10_COMBOS_FOR_50_SERVICE_REQUESTS_2.csv"
STATS_OUT_PATH = "/content/DFstatistics_ServiceRequests_Time_2.csv"

# =====================================================
# LOAD DATA
# =====================================================
df_combos = pd.read_csv(COMBOS_PATH)
df_profiles = pd.read_csv(PROFILES_PATH)
Service_Request_df = pd.read_csv(SERVICE_REQ_PATH)

client_cols = get_client_cols(df_combos)

# =====================================================
# BUILD Combination + Num_Clients ONCE
# =====================================================
df = df_combos.copy()
df["Combination"] = df.apply(lambda r: make_combination_string(r, client_cols), axis=1)
df["Num_Clients"] = df[client_cols].notna().sum(axis=1).astype(int)

print("‚úÖ Total sampled combos loaded =", len(df))

# =====================================================
# SES threshold: mean(Service_Request_df['Latency(ms)'])
# =====================================================
SES_T_MEAN = Service_Request_df["Latency(ms)"].astype(float).mean()
print("‚úÖ SES_T_MEAN =", SES_T_MEAN)

# =====================================================
# SELECT EXACTLY 50 SERVICE REQUESTS (NO SAMPLING)
# If the file has >50, we take the first 50.
# If it has exactly 50, we take all.
# =====================================================
df_selected_requests = Service_Request_df.head(NUM_SERVICE_REQUESTS).reset_index(drop=True)
print("‚úÖ Service requests used =", len(df_selected_requests))

# =====================================================
# PRE-PARSE CLIENT IDS ONCE FOR EACH OF THE 5,000 COMBOS
# (So we do NOT re-convert inside every request loop)
# =====================================================
combo_client_ids_list = []
valid_combo_mask = []

for _, row in df.iterrows():
    client_ids_raw = [row[c] for c in client_cols if pd.notna(row[c])]
    client_ids = []
    for cid in client_ids_raw:
        try:
            client_ids.append(int(cid))
        except Exception:
            pass
    if len(client_ids) == 0:
        valid_combo_mask.append(False)
        combo_client_ids_list.append([])
    else:
        valid_combo_mask.append(True)
        combo_client_ids_list.append(client_ids)

# Keep only combos that have valid client IDs
df_valid = df.loc[valid_combo_mask].reset_index(drop=True)
combo_client_ids_valid = [c for m, c in zip(valid_combo_mask, combo_client_ids_list) if m]

print("‚úÖ Valid combos to evaluate per request =", len(df_valid))

# =====================================================
# BRUTE FORCE: for each request -> score ALL valid combos -> top10 by MCS
# =====================================================
all_top_records = []
stats_records = []

total_start = time.time()

for _, sr_row in df_selected_requests.iterrows():
    req_id = int(sr_row["Request_ID"]) if "Request_ID" in sr_row else -1
    scenario = str(sr_row["Scenario"]) if "Scenario" in sr_row else f"Request{req_id}"

    print(f"\n==============================")
    print(f"üîé Processing Service Request: Request_ID={req_id}, Scenario={scenario}")
    print(f"==============================")

    req_start = time.time()

    records = []
    num_eval = 0

    # Evaluate ALL valid combos (‚âà 5,000, unless some invalid rows were removed)
    for i, row in df_valid.iterrows():
        client_ids = combo_client_ids_valid[i]

        out_rules = compute_rules_and_mcs_for_client_ids(
            client_ids,
            df_profiles,
            sr_row,
            SES_T_MEAN,
            SES_ALPHA_DEFAULT
        )
        if out_rules is None:
            continue

        num_eval += 1

        records.append({
            "Request_ID": req_id,
            "Scenario": scenario,
            "Combination": row["Combination"],
            "Num_Clients": int(len(client_ids)),
            **out_rules
        })

    df_scores = pd.DataFrame(records)

    req_end = time.time()
    req_time = req_end - req_start

    stats_records.append({
        "Request_ID": req_id,
        "Scenario": scenario,
        "Num_Combinations_Evaluated": int(num_eval),
        "Time_Seconds": float(req_time)
    })

    if df_scores.empty:
        print("‚ö†Ô∏è No valid results for this request.")
        continue

    # ‚úÖ Top-10 by MCS (highest first)
    df_top = df_scores.sort_values("MCS", ascending=False).head(TOP_K).reset_index(drop=True)
    df_top["Rank"] = np.arange(1, len(df_top) + 1)

    all_top_records.append(df_top)

total_end = time.time()
total_time = total_end - total_start

# =====================================================
# CREATE DFstatistics (per request + total)
# =====================================================
DFstatistics = pd.DataFrame(stats_records)

DFstatistics = pd.concat([
    DFstatistics,
    pd.DataFrame([{
        "Request_ID": "TOTAL",
        "Scenario": f"{len(df_selected_requests)}_Requests",
        "Num_Combinations_Evaluated": int(DFstatistics["Num_Combinations_Evaluated"].sum()) if not DFstatistics.empty else 0,
        "Time_Seconds": float(total_time)
    }])
], ignore_index=True)

print("\n‚úÖ DFstatistics (time per request + total):")
display(DFstatistics)

DFstatistics.to_csv(STATS_OUT_PATH, index=False)
print(f"‚úÖ Saved DFstatistics: {STATS_OUT_PATH}")

# =====================================================
# SAVE FINAL TOP-K OUTPUT (~ 50 * 10 = 500 rows)
# =====================================================
if len(all_top_records) == 0:
    print("‚ùå No outputs produced. Check input paths and data.")
else:
    df_final = pd.concat(all_top_records, ignore_index=True)

    ordered_cols = [
        "Request_ID", "Scenario", "Rank",
        "Combination", "Num_Clients",
        "MCS", "DHS", "SHS", "SES", "HSQ", "SRS", "MUS"
    ]
    # keep only columns that exist (safety)
    ordered_cols = [c for c in ordered_cols if c in df_final.columns]
    df_final = df_final[ordered_cols]

    df_final.to_csv(OUT_PATH, index=False)
    print(f"\n‚úÖ Saved Top-{TOP_K} for {len(df_selected_requests)} requests: {OUT_PATH}")
    print("‚úÖ Output rows =", len(df_final))  # should be ~500 (unless some requests had <10 valid results)
    display(df_final.head(20))

"""**Random Selection**
---
"""

# =====================================================
# RANDOM SELECTION (RF) ‚Äî FIXED 50 QUERIES (SEQUENTIAL), GROWING RANDOM SERVICE SAMPLES
# For each sample size S:
#   - For each of the 50 service requests IN SEQUENCE (as in Service_Request.csv):
#       - randomly sample S candidate combinations from df_combos (5,000 rows)
#       - compute MCS for each sampled combo
#       - pick TOP-10 by MCS
#   - record per-request time + totals
#
# Outputs:
# 1) dfstatistics_final.csv                  -> ONE ROW PER SAMPLE SIZE (totals + time)
# 2) dfstatistics_per_request_all_runs.csv   -> per-request stats for ALL sample sizes
# 3) RF_TOP10_ALL_RUNS.csv                   -> Top-10 for EACH request for EACH sample size
# =====================================================

import pandas as pd
import numpy as np
import random
import time
from google.colab import drive

drive.mount('/content/drive')

# =====================================================
# CONFIG
# =====================================================
NQ = 50
SAMPLE_COUNTS = [100,200,300,400,500,600,700,800,900,1000]   # ‚úÖ as you requested
TOP_K = 10
RANDOM_SEED = 42
OPTIMIZATION_TYPE = "RF"

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# =====================================================
# INPUT FILES
# =====================================================
COMBOS_PATH       = "/content/Updated_combination_3_MINIST_20_20_V1.csv"  # ‚úÖ 5,000 samples
PROFILES_PATH     = "/content/MNIST_Client_Profiles_For_Composability_20_20.csv"
SERVICE_REQ_PATH  = "/content/Service_Request.csv"

# =====================================================
# OUTPUT FILES
# =====================================================
STATS_FINAL_OUT_PATH      = "/content/dfstatistics_final_RF_3.csv"
STATS_PERREQ_OUT_PATH     = "/content/dfstatistics_per_request_all_runsRF_3.csv"
TOP10_ALLRUNS_OUT_PATH    = "/content/RF_TOP10_ALL_RUNS_3.csv"

# =====================================================
# LOAD DATA
# =====================================================
df_combos = pd.read_csv(COMBOS_PATH)
df_profiles = pd.read_csv(PROFILES_PATH)
Service_Request_df = pd.read_csv(SERVICE_REQ_PATH)

client_cols = get_client_cols(df_combos)

# =====================================================
# BUILD Combination + Num_Clients ONCE
# =====================================================
df = df_combos.copy()
df["Combination"] = df.apply(lambda r: make_combination_string(r, client_cols), axis=1)
df["Num_Clients"] = df[client_cols].notna().sum(axis=1).astype(int)

print("‚úÖ Total combos available =", len(df))

# =====================================================
# SES threshold: mean(Service_Request_df['Latency(ms)'])
# =====================================================
SES_T_MEAN = Service_Request_df["Latency(ms)"].astype(float).mean()
print("‚úÖ SES_T_MEAN =", SES_T_MEAN)

# =====================================================
# FIXED QUERIES (CONSTANT NQ) ‚Äî SEQUENTIAL (NO RANDOM)
# Takes the first NQ rows exactly in the CSV order.
# =====================================================
assert NQ <= len(Service_Request_df), f"NQ={NQ} exceeds available service requests ({len(Service_Request_df)})"

df_selected_requests = Service_Request_df.iloc[:NQ].reset_index(drop=True)

print(f"‚úÖ Sequential service requests selected in CSV order: 1..{NQ}")

# =====================================================
# PRE-PARSE CLIENT IDS ONCE FOR ALL combos (important speedup)
# =====================================================
parsed_client_ids = []
valid_mask = []

for _, row in df.iterrows():
    client_ids_raw = [row[c] for c in client_cols if pd.notna(row[c])]
    ids = []
    for cid in client_ids_raw:
        try:
            ids.append(int(cid))
        except Exception:
            pass

    if len(ids) == 0:
        valid_mask.append(False)
        parsed_client_ids.append([])
    else:
        valid_mask.append(True)
        parsed_client_ids.append(ids)

df_valid = df.loc[valid_mask].reset_index(drop=True)
client_ids_valid = [x for m, x in zip(valid_mask, parsed_client_ids) if m]

print("‚úÖ Valid combos to sample from =", len(df_valid))

# =====================================================
# MAIN LOOP
# =====================================================
final_rows = []          # one row per sample size
all_perreq_stats = []    # per-request per sample size
all_top10_records = []   # top10 per request per sample size

for S in SAMPLE_COUNTS:

    print(f"\n=====================================================")
    print(f"üöÄ RF RUN: FIXED_QUERIES(NQ)={NQ} | RANDOM_SAMPLES_PER_QUERY={S}")
    print(f"=====================================================")

    run_start = time.time()

    total_attempted = 0
    total_valid = 0

    # For each request (SEQUENTIAL order)
    for rq_idx, sr_row in df_selected_requests.iterrows():
        req_id = int(sr_row["Request_ID"]) if "Request_ID" in sr_row else rq_idx
        scenario = str(sr_row["Scenario"]) if "Scenario" in sr_row else f"Request{req_id}"

        req_start = time.time()

        # ‚úÖ Randomly sample S combos from df_valid (5,000 pool)
        n_pick = min(S, len(df_valid))
        sampled_idx = np.random.choice(len(df_valid), size=n_pick, replace=False)

        attempted = int(n_pick)
        valid = 0
        scored = []

        for j in sampled_idx:
            client_ids = client_ids_valid[j]

            out_rules = compute_rules_and_mcs_for_client_ids(
                client_ids,
                df_profiles,
                sr_row,
                SES_T_MEAN,
                SES_ALPHA_DEFAULT
            )
            if out_rules is None:
                continue

            valid += 1
            scored.append({
                "Optimization_Type": OPTIMIZATION_TYPE,
                "Sampled_Services_Per_Query": int(S),
                "Request_ID": int(req_id),
                "Scenario": scenario,
                "Combination": df_valid.loc[j, "Combination"],
                "Num_Clients": int(len(client_ids)),
                **out_rules
            })

        req_time = time.time() - req_start

        total_attempted += attempted
        total_valid += valid

        # per-request stats
        all_perreq_stats.append({
            "Optimization_Type": OPTIMIZATION_TYPE,
            "Fixed_Num_Service_Requests_NQ": int(NQ),
            "Sampled_Services_Per_Query": int(S),
            "Request_ID": int(req_id),
            "Scenario": scenario,
            "Attempted_Samples": int(attempted),
            "Valid_Evaluations": int(valid),
            "Time_Seconds": float(req_time)
        })

        # top-10 for this request at this sample size
        df_scores = pd.DataFrame(scored)
        if not df_scores.empty:
            df_top = df_scores.sort_values("MCS", ascending=False).head(TOP_K).reset_index(drop=True)
            df_top["Rank"] = np.arange(1, len(df_top) + 1)
            all_top10_records.append(df_top)

    run_time = time.time() - run_start

    final_rows.append({
        "Optimization_Type": OPTIMIZATION_TYPE,
        "Fixed_Num_Service_Requests_NQ": int(NQ),
        "Sampled_Services_Per_Query": int(S),
        "Ratio_NCES_over_NQ": float(S),
        "Total_Attempted_Samples": int(total_attempted),
        "Total_Valid_Evaluations": int(total_valid),
        "Total_Time_Seconds": float(run_time)
    })

# =====================================================
# SAVE OUTPUTS
# =====================================================
dfstatistics_final = pd.DataFrame(final_rows)
dfstatistics_final.to_csv(STATS_FINAL_OUT_PATH, index=False)
print(f"\n‚úÖ Saved: {STATS_FINAL_OUT_PATH}")
display(dfstatistics_final)

dfstatistics_per_request_all_runs = pd.DataFrame(all_perreq_stats)
dfstatistics_per_request_all_runs.to_csv(STATS_PERREQ_OUT_PATH, index=False)
print(f"‚úÖ Saved: {STATS_PERREQ_OUT_PATH}")
display(dfstatistics_per_request_all_runs.head(10))

if len(all_top10_records) > 0:
    df_top10_all = pd.concat(all_top10_records, ignore_index=True)

    ordered_cols = [
        "Optimization_Type", "Sampled_Services_Per_Query",
        "Request_ID", "Scenario", "Rank",
        "Combination", "Num_Clients",
        "MCS", "DHS", "SHS", "SES", "HSQ", "SRS", "MUS"
    ]
    ordered_cols = [c for c in ordered_cols if c in df_top10_all.columns]
    df_top10_all = df_top10_all[ordered_cols]
    # Optional: keep file neatly ordered
    df_top10_all = df_top10_all.sort_values(
        by=["Request_ID", "Sampled_Services_Per_Query", "Rank"],
        ascending=[True, True, True]
    ).reset_index(drop=True)
    df_top10_all.to_csv(TOP10_ALLRUNS_OUT_PATH, index=False)
    print(f"‚úÖ Saved: {TOP10_ALLRUNS_OUT_PATH}")
    print("‚úÖ Top10 rows =", len(df_top10_all))  # ~ NQ * TOP_K * len(SAMPLE_COUNTS)
    display(df_top10_all.head(20))
else:
    print("‚ùå No top-10 outputs produced (no valid results). Check input data / rules.")

import pandas as pd
import matplotlib.pyplot as plt

# =========================
# INPUT
# =========================
RF_PATH = "/content/RF_TOP10_ALL_RUNS_3.csv"
MANUAL_THRESHOLD = 0.89   # <<< SET THIS MANUALLY

# =========================
# LOAD
# =========================
rf = pd.read_csv(RF_PATH)

# =========================
# APPLY MANUAL THRESHOLD
# =========================
rf["Above_Threshold"] = rf["MCS"] >= MANUAL_THRESHOLD

# =========================
# ACCURACY PER QUERY SIZE
# =========================
accuracy_df = (
    rf.groupby("Sampled_Services_Per_Query")["Above_Threshold"]
      .mean()
      .reset_index()
      .rename(columns={"Above_Threshold": "Accuracy"})
      .sort_values("Sampled_Services_Per_Query")
)

print(accuracy_df)

# =========================
# BAR CHART
# =========================
plt.figure(figsize=(7, 4))
plt.bar(
    accuracy_df["Sampled_Services_Per_Query"].astype(str),
    accuracy_df["Accuracy"]
)
plt.xlabel("Sampled Services Per Query")
plt.ylabel("Accuracy (MCS ‚â• Threshold)")
plt.ylim(0, 1)
plt.title(f"Accuracy vs Query Size (Threshold = {MANUAL_THRESHOLD})")
plt.tight_layout()
plt.show()

"""**Genic Algorithm**
---
"""



# =====================================================
# GENETIC ALGORITHM (GA) ‚Äî FIXED 50 QUERIES (SEQUENTIAL), GROWING POPULATION SIZE
# For each sample size S (100,200,...):
#   - For each of the first 50 service requests IN SEQUENCE:
#       - run GA where POP_SIZE = S (i.e., S candidates explored per request)
#       - compute MCS via compute_rules_and_mcs_for_client_ids(...)
#       - pick TOP-10 by MCS from final population (deduped)
#
# Outputs:
# 1) dfstatistics_final_ga_samples.csv         -> ONE ROW PER SAMPLE SIZE (totals + time)
# 2) dfstatistics_ga_per_request_all_runs.csv  -> per-request stats for ALL sample sizes
# 3) GA_TOP10_ALL_RUNS.csv                     -> Top-10 for EACH request for EACH sample size
# =====================================================

import numpy as np
import pandas as pd
import random
import time
from typing import List, Tuple, Dict
from google.colab import drive

drive.mount('/content/drive')

# =====================================================
# CONFIG
# =====================================================
NQ = 50
SAMPLE_COUNTS = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]   # your "count" schedule
TOP_K = 10
RANDOM_SEED = 42
OPTIMIZATION_TYPE = "GA"

# ---- GA hyperparams (keep your logic)
GENERATIONS     = 1
TOURNAMENT_K    = 2
CROSSOVER_RATE  = 0.40
MUTATION_RATE   = 0.40
ELITISM         = 2
NUM_EVALS_CAP   = None  # set int if you want cap evaluations per request

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# =====================================================
# PATHS
# =====================================================

#"/content/Updated_combination_3_MINIST_20_20_V1.csv"
COMBOS_PATH       = "/content/Updated_combination_3_MINIST_20_20_V1.csv"   # 5,000 samples
PROFILES_PATH     = "/content/MNIST_Client_Profiles_For_Composability_20_20.csv"
SERVICE_REQ_PATH  = "/content/Service_Request.csv"

STATS_FINAL_OUT_PATH       = "/content/dfstatistics_final_ga_samples_3.csv"
STATS_PERREQ_OUT_PATH      = "/content/dfstatistics_ga_per_request_all_runs_3.csv"
TOP10_ALLRUNS_OUT_PATH     = "/content/GA_TOP10_ALL_RUNS_3.csv"

# =====================================================
# LOAD FILES
# =====================================================
df_combos   = pd.read_csv(COMBOS_PATH)
df_profiles = pd.read_csv(PROFILES_PATH)
Service_Request_df = pd.read_csv(SERVICE_REQ_PATH)

# use your helper if available; else fallback to Client_* prefix
try:
    client_cols = get_client_cols(df_combos)
except Exception:
    client_cols = [c for c in df_combos.columns if c.startswith("Client_")]

K = len(client_cols)
assert K > 0, "No Client_* columns found."

# =====================================================
# BUILD Combination + Num_Clients ONCE
# =====================================================
df = df_combos.copy()
df["Combination"] = df.apply(lambda r: make_combination_string(r, client_cols), axis=1)
df["Num_Clients"] = df[client_cols].notna().sum(axis=1).astype(int)

print("‚úÖ Total combos available =", len(df))

# =====================================================
# SES threshold (global): mean(Service_Request_df['Latency(ms)'])
# =====================================================
SES_T_MEAN = Service_Request_df["Latency(ms)"].astype(float).mean()
print("‚úÖ SES_T_MEAN =", SES_T_MEAN)

# =====================================================
# FIXED QUERIES ‚Äî SEQUENTIAL (NO RANDOM)
# =====================================================
assert NQ <= len(Service_Request_df), f"NQ={NQ} exceeds available service requests ({len(Service_Request_df)})"
df_selected_requests = Service_Request_df.iloc[:NQ].reset_index(drop=True)
print(f"‚úÖ Sequential service requests selected: first {NQ} rows")

# =====================================================
# BUILD: allowed client IDs + valid combinations (existing only)
# (Same idea as your GA code: GA must output ONLY existing combos)
# =====================================================
allowed_clients = sorted(pd.unique(df_combos[client_cols].values.ravel("K")))
allowed_clients = [int(x) for x in allowed_clients if pd.notna(x)]
allowed_set = set(allowed_clients)

def row_to_combo_tuple(row) -> Tuple[int, ...] | None:
    ids = []
    for c in client_cols:
        v = row[c]
        if pd.notna(v):
            try:
                ids.append(int(v))
            except Exception:
                return None
    ids = [x for x in ids if x in allowed_set]
    if len(ids) != K:
        return None
    return tuple(sorted(ids))

valid_combos = []
for _, r in df_combos.iterrows():
    t = row_to_combo_tuple(r)
    if t is None:
        continue
    valid_combos.append(t)

valid_combos = list(dict.fromkeys(valid_combos))  # unique keep order
valid_set = set(valid_combos)
assert len(valid_combos) > 0, "No valid combinations found."

print(f"‚úÖ Allowed unique client IDs: {len(allowed_clients)}")
print(f"‚úÖ Valid existing combinations: {len(valid_combos)}")
print("Example valid combo:", valid_combos[0])

# =====================================================
# REPAIR: snap any produced child to an existing combo
# =====================================================
def repair_to_existing(child: List[int]) -> Tuple[int, ...]:
    child = [int(x) for x in child if x in allowed_set]
    child = list(dict.fromkeys(child))  # remove dups

    if len(child) < K:
        remaining = [c for c in allowed_clients if c not in child]
        child += random.sample(remaining, K - len(child))

    child = child[:K]
    t = tuple(sorted(child))
    if t in valid_set:
        return t

    target = set(child)
    best_score = -1
    best = []
    for cand in valid_combos:
        score = len(target.intersection(cand))
        if score > best_score:
            best_score = score
            best = [cand]
        elif score == best_score:
            best.append(cand)

    return random.choice(best)

# =====================================================
# HELPER: combination string
# =====================================================
def make_combination_string_from_ids(client_ids: List[int]) -> str:
    return "_".join(str(int(x)) for x in client_ids)

# =====================================================
# GA operators (same as your style)
# =====================================================
def random_chromosome() -> Tuple[int, ...]:
    return random.choice(valid_combos)

def crossover(p1: Tuple[int, ...], p2: Tuple[int, ...]) -> Tuple[int, ...]:
    if random.random() > CROSSOVER_RATE:
        return p1

    p1 = list(p1)
    p2 = list(p2)

    a = random.randint(0, K - 2)
    b = random.randint(a + 1, K - 1)

    child = p1[a:b]
    used = set(child)

    for g in p2:
        if g not in used and len(child) < K:
            child.append(g)
            used.add(g)

    return repair_to_existing(child)

def mutate(combo: Tuple[int, ...]) -> Tuple[int, ...]:
    if random.random() > MUTATION_RATE:
        return combo

    # A) jump mutation
    if random.random() < 0.5:
        return random.choice(valid_combos)

    # B) single-gene replace + repair
    child = list(combo)
    idx = random.randint(0, K - 1)
    candidates = [c for c in allowed_clients if c not in child]
    if candidates:
        child[idx] = random.choice(candidates)

    return repair_to_existing(child)

# =====================================================
# RUN GA FOR ONE REQUEST (POP_SIZE = S)
# =====================================================
def run_ga_for_request(sr_row: pd.Series, request_seed: int, pop_size: int):
    random.seed(request_seed)
    np.random.seed(request_seed)

    _eval_cache: Dict[Tuple[int, ...], Dict] = {}
    eval_counter = 0

    def evaluate_combo(combo_tup: Tuple[int, ...]) -> Dict:
        nonlocal eval_counter

        if combo_tup in _eval_cache:
            return _eval_cache[combo_tup]

        if NUM_EVALS_CAP is not None and eval_counter >= int(NUM_EVALS_CAP):
            out = {
                "Combination": make_combination_string_from_ids(list(combo_tup)),
                "Num_Clients": len(combo_tup),
                "DHS": np.nan, "SHS": np.nan, "SES": np.nan,
                "HSQ": np.nan, "SRS": np.nan, "MUS": np.nan,
                "MCS": -1.0,
                "Fitness": 0.0
            }
            return out

        eval_counter += 1
        client_ids = list(combo_tup)

        out_rules = compute_rules_and_mcs_for_client_ids(
            client_ids,
            df_profiles,
            sr_row,
            SES_T_MEAN,
            SES_ALPHA_DEFAULT
        )

        if out_rules is None:
            out = {
                "Combination": make_combination_string_from_ids(client_ids),
                "Num_Clients": len(client_ids),
                "DHS": np.nan, "SHS": np.nan, "SES": np.nan,
                "HSQ": np.nan, "SRS": np.nan, "MUS": np.nan,
                "MCS": -1.0
            }
        else:
            out = {
                "Combination": make_combination_string_from_ids(client_ids),
                "Num_Clients": len(client_ids),
                **out_rules
            }

        mcs = out.get("MCS", -1.0)
        out["Fitness"] = float(mcs) if (mcs is not None and np.isfinite(mcs) and mcs > 0) else 0.0
        _eval_cache[combo_tup] = out
        return out

    def tournament_select(population: List[Tuple[int, ...]]) -> Tuple[int, ...]:
        contenders = random.sample(population, TOURNAMENT_K)
        return max(contenders, key=lambda c: evaluate_combo(c)["Fitness"])

    # population size = pop_size (your "count")
    pop_size = int(min(pop_size, len(valid_combos)))
    population = [random_chromosome() for _ in range(pop_size)]

    for _ in range(GENERATIONS):
        scored = [(evaluate_combo(c), c) for c in population]
        scored.sort(key=lambda x: x[0]["Fitness"], reverse=True)

        new_pop = [scored[i][1] for i in range(min(ELITISM, len(scored)))]

        while len(new_pop) < pop_size:
            p1 = tournament_select(population)
            p2 = tournament_select(population)
            child = crossover(p1, p2)
            child = mutate(child)
            new_pop.append(child)

        population = new_pop

    final = [evaluate_combo(c) for c in population]
    df_out = (
        pd.DataFrame(final)
        .sort_values("MCS", ascending=False)
        .drop_duplicates(subset=["Combination"])
        .reset_index(drop=True)
    )

    return df_out, eval_counter

# =====================================================
# MULTI-RUN FOR SAMPLE_COUNTS (same style as RF multi-run)
# =====================================================
final_rows = []            # one row per sample size
all_perreq_stats = []      # per request per sample size
all_top10_records = []     # top10 per request per sample size

for S in SAMPLE_COUNTS:

    print(f"\n=====================================================")
    print(f"üß¨ GA RUN: FIXED_QUERIES(NQ)={NQ} | POP_SIZE={S}")
    print(f"=====================================================")

    run_start = time.time()
    total_evals = 0

    # sequential requests
    for rq_idx, sr_row in df_selected_requests.iterrows():
        req_id = int(sr_row["Request_ID"]) if "Request_ID" in sr_row else (rq_idx + 1)
        scenario = str(sr_row["Scenario"]) if "Scenario" in sr_row else f"Request{req_id}"

        req_start = time.time()

        df_ga_out, eval_count = run_ga_for_request(
            sr_row=sr_row,
            request_seed=RANDOM_SEED + int(req_id) + int(S),
            pop_size=int(S)
        )

        req_time = time.time() - req_start
        total_evals += int(eval_count)

        all_perreq_stats.append({
            "Optimization_Type": OPTIMIZATION_TYPE,
            "Fixed_Num_Service_Requests_NQ": int(NQ),
            "POP_SIZE": int(S),
            "GENERATIONS": int(GENERATIONS),
            "Request_ID": int(req_id),
            "Scenario": scenario,
            "Evaluations": int(eval_count),
            "Time_Seconds": float(req_time),
            "Best_MCS": float(df_ga_out.iloc[0]["MCS"]) if (df_ga_out is not None and not df_ga_out.empty and np.isfinite(df_ga_out.iloc[0]["MCS"])) else np.nan,
            "Best_Combination": df_ga_out.iloc[0]["Combination"] if (df_ga_out is not None and not df_ga_out.empty) else None
        })

        # top-10 for this request at this pop size
        if df_ga_out is not None and not df_ga_out.empty:
            df_top = df_ga_out.head(TOP_K).copy()
            df_top["Optimization_Type"] = OPTIMIZATION_TYPE
            df_top["POP_SIZE"] = int(S)
            df_top["Request_ID"] = int(req_id)
            df_top["Scenario"] = scenario
            df_top["Rank"] = np.arange(1, len(df_top) + 1)

            df_top = df_top[[
                "Optimization_Type", "POP_SIZE",
                "Request_ID", "Scenario", "Rank",
                "Combination", "Num_Clients",
                "MCS", "DHS", "SHS", "SES", "HSQ", "SRS", "MUS"
            ]]
            all_top10_records.append(df_top)

    run_time = time.time() - run_start

    final_rows.append({
        "Optimization_Type": OPTIMIZATION_TYPE,
        "Fixed_Num_Service_Requests_NQ": int(NQ),
        "POP_SIZE": int(S),
        "GENERATIONS": int(GENERATIONS),
        "Total_Evaluations": int(total_evals),
        "Total_Time_Seconds": float(run_time)
    })

# =====================================================
# SAVE OUTPUTS
# =====================================================
dfstatistics_final = pd.DataFrame(final_rows)
dfstatistics_final.to_csv(STATS_FINAL_OUT_PATH, index=False)
print(f"\n‚úÖ Saved: {STATS_FINAL_OUT_PATH}")
display(dfstatistics_final)

dfstatistics_per_request_all_runs = pd.DataFrame(all_perreq_stats)
dfstatistics_per_request_all_runs.to_csv(STATS_PERREQ_OUT_PATH, index=False)
print(f"‚úÖ Saved: {STATS_PERREQ_OUT_PATH}")
display(dfstatistics_per_request_all_runs.head(10))

if len(all_top10_records) > 0:
    df_top10_all = pd.concat(all_top10_records, ignore_index=True)

    # neat ordering: request order -> pop size -> rank
    df_top10_all = df_top10_all.sort_values(
        by=["Request_ID", "POP_SIZE", "Rank"],
        ascending=[True, True, True]
    ).reset_index(drop=True)

    df_top10_all.to_csv(TOP10_ALLRUNS_OUT_PATH, index=False)
    print(f"‚úÖ Saved: {TOP10_ALLRUNS_OUT_PATH}")
    print("‚úÖ Top10 rows =", len(df_top10_all))  # ~ NQ * TOP_K * len(SAMPLE_COUNTS)
    display(df_top10_all.head(20))
else:
    print("‚ùå No top-10 outputs produced (no valid results). Check input data / rules.")

import pandas as pd

df = pd.read_csv("TOP10_COMBOS_FOR_50_SERVICE_REQUESTS.csv")

scenario_thresholds = (
    df.groupby("Scenario")["MCS"]
      .mean()
      .reset_index()
      .rename(columns={"MCS": "Mean_MCS_Threshold"})
)
print(scenario_thresholds)

import pandas as pd
import matplotlib.pyplot as plt

# =========================
# INPUT
# =========================
RF_PATH = "/content/GA_TOP10_ALL_RUNS_3.csv"
MANUAL_THRESHOLD = 0.89   # <<< SET THIS MANUALLY

# =========================
# LOAD
# =========================
rf = pd.read_csv(RF_PATH)

# =========================
# APPLY MANUAL THRESHOLD
# =========================
rf["Above_Threshold"] = rf["MCS"] >= MANUAL_THRESHOLD

# =========================
# ACCURACY PER QUERY SIZE
# =========================
accuracy_df = (
    rf.groupby("POP_SIZE")["Above_Threshold"]
      .mean()
      .reset_index()
      .rename(columns={"Above_Threshold": "Accuracy"})
      .sort_values("POP_SIZE")
)

print(accuracy_df)

# =========================
# BAR CHART
# =========================
plt.figure(figsize=(7, 4))
plt.bar(
    accuracy_df["POP_SIZE"].astype(str),
    accuracy_df["Accuracy"]
)
plt.xlabel("Sampled Services Per Query")
plt.ylabel("Accuracy (MCS ‚â• Threshold)")
plt.ylim(0, 1)
plt.title(f"Accuracy vs Query Size (Threshold = {MANUAL_THRESHOLD})")
plt.tight_layout()
plt.show()

"""**œµ Greedy**
---
"""

# =====================================================
# EPSILON-GREEDY (EG) ‚Äî FIXED 50 QUERIES (SEQUENTIAL), GROWING EVALUATION BUDGET
# For each budget S in SAMPLE_COUNTS:
#   - For each of the first 50 service requests IN SEQUENCE:
#       - run epsilon-greedy for BUDGET_EVALS = S steps (request-aware)
#       - pick TOP-10 unique combos discovered (by max MCS)
#   - record per-request time + totals
#
# Outputs:
# 1) dfstatistics_final_eg_samples.csv            -> ONE ROW PER SAMPLE SIZE (totals + time)
# 2) dfstatistics_eg_per_request_all_runs.csv     -> per-request stats for ALL sample sizes
# 3) EG_TOP10_ALL_RUNS.csv                        -> Top-10 for EACH request for EACH sample size
# =====================================================

import numpy as np
import pandas as pd
import random
import time
from typing import List, Tuple, Dict
from google.colab import drive

drive.mount('/content/drive')

# =====================================================
# CONFIG
# =====================================================
NQ = 50
SAMPLE_COUNTS = [100, 200, 300, 400, 500, 600,700,800,900,1000]   # ‚úÖ your "count" schedule (budget per request)
TOP_K = 10

EPSILON   = 0.40
EPS_DECAY = 1.0
EPS_MIN   = 0.05

RANDOM_SEED = 42
OPTIMIZATION_TYPE = "EG"

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# =====================================================
# INPUT FILES
# =====================================================
COMBOS_PATH       = "/content/Updated_combination_3_MINIST_20_20_V1.csv"  # or your 5k sampled combos file
PROFILES_PATH     = "/content/MNIST_Client_Profiles_For_Composability_20_20.csv"
SERVICE_REQ_PATH  = "/content/Service_Request.csv"

# =====================================================
# OUTPUT FILES
# =====================================================
OUT_PATH_RESULTS        = "/content/EG_TOP10_ALL_RUNS_3.csv"
OUT_PATH_STATS_REQUESTS = "/content/dfstatistics_eg_per_request_all_runs_3.csv"
OUT_PATH_STATS_FINAL    = "/content/dfstatistics_final_eg_samples_3.csv"

# =====================================================
# LOAD DATA
# =====================================================
df_combos = pd.read_csv(COMBOS_PATH)
df_profiles = pd.read_csv(PROFILES_PATH)
Service_Request_df = pd.read_csv(SERVICE_REQ_PATH)

client_cols = get_client_cols(df_combos)

# Build Combination + Num_Clients ONCE
df = df_combos.copy()
df["Combination"] = df.apply(lambda r: make_combination_string(r, client_cols), axis=1)
df["Num_Clients"] = df[client_cols].notna().sum(axis=1).astype(int)

# SES threshold = mean(Service_Request_df['Latency(ms)'])
SES_T_MEAN = Service_Request_df["Latency(ms)"].astype(float).mean()
print("‚úÖ SES_T_MEAN (mean of Service_Request_df['Latency(ms)']) =", SES_T_MEAN)

# =====================================================
# FIXED QUERIES ‚Äî SEQUENTIAL (NO RANDOM)
# =====================================================
assert NQ <= len(Service_Request_df), f"NQ={NQ} exceeds available service requests ({len(Service_Request_df)})"
df_selected_requests = Service_Request_df.iloc[:NQ].reset_index(drop=True)
print(f"‚úÖ Sequential service requests selected: first {NQ} rows (CSV order)")

# =====================================================
# Build valid_combos (existing only) for EG arms
# =====================================================
client_cols_raw = [c for c in df_combos.columns if c.startswith("Client_")]
K = len(client_cols_raw)
assert K > 0, "No Client_* columns found."

allowed_clients = sorted(pd.unique(df_combos[client_cols_raw].values.ravel("K")))
allowed_clients = [int(x) for x in allowed_clients if pd.notna(x)]
allowed_set = set(allowed_clients)

def row_to_combo_tuple(row) -> Tuple[int, ...] | None:
    ids = []
    for c in client_cols_raw:
        v = row[c]
        if pd.notna(v):
            try:
                ids.append(int(v))
            except Exception:
                return None
    ids = [x for x in ids if x in allowed_set]
    if len(ids) != K:
        return None
    return tuple(sorted(ids))

valid_combos = []
for _, r in df_combos.iterrows():
    t = row_to_combo_tuple(r)
    if t is None:
        continue
    valid_combos.append(t)

valid_combos = list(dict.fromkeys(valid_combos))
assert len(valid_combos) > 0, "No valid combinations found."
print(f"‚úÖ Valid existing combinations (arms): {len(valid_combos)}")
print("Example valid combo:", valid_combos[0])

def make_combination_string_from_ids(client_ids: List[int]) -> str:
    return "_".join(str(int(x)) for x in client_ids)

# =====================================================
# Run epsilon-greedy for ONE request (request-aware)
# BUDGET_EVALS is passed per run (S from SAMPLE_COUNTS)
# =====================================================
def run_epsilon_greedy_for_request(sr_row: pd.Series, request_seed: int, BUDGET_EVALS: int):
    random.seed(request_seed)
    np.random.seed(request_seed)

    _eval_cache: Dict[Tuple[int, ...], Dict] = {}

    def evaluate_combo(combo_tup: Tuple[int, ...]) -> Dict:
        if combo_tup in _eval_cache:
            return _eval_cache[combo_tup]

        client_ids = list(combo_tup)

        out_rules = compute_rules_and_mcs_for_client_ids(
            client_ids,
            df_profiles,
            sr_row,
            SES_T_MEAN,
            SES_ALPHA_DEFAULT
        )

        if out_rules is None:
            out = {
                "Combination": make_combination_string_from_ids(client_ids),
                "Num_Clients": len(client_ids),
                "DHS": np.nan, "SHS": np.nan, "SES": np.nan,
                "HSQ": np.nan, "SRS": np.nan, "MUS": np.nan,
                "MCS": -1.0
            }
        else:
            out = {
                "Combination": make_combination_string_from_ids(client_ids),
                "Num_Clients": len(client_ids),
                **out_rules
            }

        mcs = out.get("MCS", -1.0)
        out["Fitness"] = float(mcs) if (mcs is not None and np.isfinite(mcs) and mcs > 0) else 0.0

        _eval_cache[combo_tup] = out
        return out

    best_combo = None
    best_fit = -1.0

    eps = float(EPSILON)
    attempted = 0
    valid = 0
    history = []

    for t in range(1, int(BUDGET_EVALS) + 1):
        attempted += 1

        explore = (random.random() < eps)

        if explore or best_combo is None:
            combo = random.choice(valid_combos)
            action = "explore"
        else:
            combo = best_combo
            action = "exploit"

        res = evaluate_combo(combo)
        mcs_val = res.get("MCS", np.nan)

        if not np.isfinite(mcs_val) or mcs_val < 0:
            reward = 0.0
        else:
            valid += 1
            reward = float(res["Fitness"])

        if reward > best_fit:
            best_fit = reward
            best_combo = combo

        history.append({
            "step": t,
            "action": action,
            "epsilon": eps,
            "Combination": res.get("Combination", make_combination_string_from_ids(list(combo))),
            "Num_Clients": int(res.get("Num_Clients", len(combo))),
            "DHS": res.get("DHS", np.nan),
            "SHS": res.get("SHS", np.nan),
            "SES": res.get("SES", np.nan),
            "HSQ": res.get("HSQ", np.nan),
            "SRS": res.get("SRS", np.nan),
            "MUS": res.get("MUS", np.nan),
            "MCS": float(mcs_val) if np.isfinite(mcs_val) else np.nan,
            "Fitness": float(reward),
        })

        eps = max(EPS_MIN, eps * EPS_DECAY)

    df_hist = pd.DataFrame(history)

    # Top unique combos discovered (by max MCS)
    df_top = (
        df_hist.groupby("Combination", as_index=False)["MCS"].max()
        .sort_values("MCS", ascending=False)
        .head(TOP_K)
        .reset_index(drop=True)
    )

    # Attach best row details for each combo
    df_best_rows = (
        df_hist.sort_values("MCS", ascending=False)
        .drop_duplicates(subset=["Combination"])
    )

    df_top = df_top.merge(
        df_best_rows[["Combination","Num_Clients","DHS","SHS","SES","HSQ","SRS","MUS","MCS"]],
        on=["Combination","MCS"],
        how="left"
    )

    return df_top, attempted, valid, best_fit

# =====================================================
# MAIN LOOP: run for each budget in SAMPLE_COUNTS (like RF)
# =====================================================
all_results = []
all_request_stats = []
final_stats = []

for S in SAMPLE_COUNTS:
    print("\n======================================")
    print(f"üöÄ EPSILON-GREEDY RUN: NQ={NQ} | BUDGET_EVALS={S}")
    print("======================================")

    run_start = time.time()
    stats_records = []
    top_records = []

    # sequential requests
    for rq_idx, sr_row in df_selected_requests.iterrows():
        req_id = int(sr_row["Request_ID"]) if "Request_ID" in sr_row else (rq_idx + 1)
        scenario = str(sr_row["Scenario"]) if "Scenario" in sr_row else f"Request{req_id}"

        req_start = time.time()

        df_top, attempted, valid, best_fit = run_epsilon_greedy_for_request(
            sr_row=sr_row,
            request_seed=RANDOM_SEED + int(req_id) + int(S),  # request-aware + budget-aware
            BUDGET_EVALS=int(S)
        )

        req_time = time.time() - req_start

        stats_records.append({
            "Optimization_Type": "EG",
            "Fixed_Num_Service_Requests_NQ": int(NQ),
            "BUDGET_EVALS": int(S),
            "EPSILON": float(EPSILON),
            "EPS_DECAY": float(EPS_DECAY),
            "EPS_MIN": float(EPS_MIN),

            "Request_ID": int(req_id),
            "Scenario": scenario,

            "Attempted_Steps": int(attempted),
            "Valid_Evaluations": int(valid),
            "Time_Seconds": float(req_time),
            "Best_MCS": float(best_fit) if np.isfinite(best_fit) else np.nan
        })

        if not df_top.empty:
            df_top = df_top.copy()
            df_top["Optimization_Type"] = "EG"
            df_top["BUDGET_EVALS"] = int(S)
            df_top["Request_ID"] = int(req_id)
            df_top["Scenario"] = scenario
            df_top["Rank"] = np.arange(1, len(df_top) + 1)

            df_top = df_top[[
                "Optimization_Type", "BUDGET_EVALS",
                "Request_ID", "Scenario", "Rank",
                "Combination", "Num_Clients",
                "MCS", "DHS", "SHS", "SES", "HSQ", "SRS", "MUS"
            ]]

            top_records.append(df_top)

    total_time = time.time() - run_start
    dfstats_run = pd.DataFrame(stats_records)

    # final summary row for this S
    final_stats.append({
        "Optimization_Type": "EG",
        "Fixed_Num_Service_Requests_NQ": int(NQ),
        "BUDGET_EVALS": int(S),
        "Total_Time_Seconds": float(total_time),
        "Total_Attempted_Steps": int(dfstats_run["Attempted_Steps"].sum()) if not dfstats_run.empty else 0,
        "Total_Valid_Evaluations": int(dfstats_run["Valid_Evaluations"].sum()) if not dfstats_run.empty else 0
    })

    all_request_stats.append(dfstats_run)

    if len(top_records) > 0:
        all_results.append(pd.concat(top_records, ignore_index=True))

# =====================================================
# SAVE ALL OUTPUTS
# =====================================================
dfstatistics_requests_all = pd.concat(all_request_stats, ignore_index=True) if len(all_request_stats) else pd.DataFrame()
dfstatistics_final = pd.DataFrame(final_stats)

# sort neatly: request order -> budget -> rank
if not dfstatistics_requests_all.empty:
    dfstatistics_requests_all = dfstatistics_requests_all.sort_values(
        by=["Request_ID", "BUDGET_EVALS"],
        ascending=[True, True]
    ).reset_index(drop=True)

dfstatistics_requests_all.to_csv(OUT_PATH_STATS_REQUESTS, index=False)
dfstatistics_final.to_csv(OUT_PATH_STATS_FINAL, index=False)

print("\n‚úÖ Saved per-request EG statistics:", OUT_PATH_STATS_REQUESTS)
print("‚úÖ Saved final EG summary:", OUT_PATH_STATS_FINAL)

if len(all_results) > 0:
    df_results_all = pd.concat(all_results, ignore_index=True)

    df_results_all = df_results_all.sort_values(
        by=["Request_ID", "BUDGET_EVALS", "Rank"],
        ascending=[True, True, True]
    ).reset_index(drop=True)

    df_results_all.to_csv(OUT_PATH_RESULTS, index=False)
    print("‚úÖ Saved EG top-10 results:", OUT_PATH_RESULTS)
    display(df_results_all.head(20))
else:
    print("‚ùå No top-10 results produced. Check data / paths.")

import pandas as pd
import matplotlib.pyplot as plt

# =========================
# INPUT
# =========================
RF_PATH = "/content/EG_TOP10_ALL_RUNS_3.csv"
MANUAL_THRESHOLD = 0.885   # <<< SET THIS MANUALLY

# =========================
# LOAD
# =========================
rf = pd.read_csv(RF_PATH)

# =========================
# APPLY MANUAL THRESHOLD
# =========================
rf["Above_Threshold"] = rf["MCS"] >= MANUAL_THRESHOLD

# =========================
# ACCURACY PER QUERY SIZE
# =========================
accuracy_df = (
    rf.groupby("BUDGET_EVALS")["Above_Threshold"]
      .mean()
      .reset_index()
      .rename(columns={"Above_Threshold": "Accuracy"})
      .sort_values("BUDGET_EVALS")
)

print(accuracy_df)

# =========================
# BAR CHART
# =========================
plt.figure(figsize=(7, 4))
plt.bar(
    accuracy_df["BUDGET_EVALS"].astype(str),
    accuracy_df["Accuracy"]
)
plt.xlabel("Sampled Services Per Query")
plt.ylabel("Accuracy (MCS ‚â• Threshold)")
plt.ylim(0, 1)
plt.title(f"Accuracy vs Query Size (Threshold = {MANUAL_THRESHOLD})")
plt.tight_layout()
plt.show()

"""**Greedy**
---
"""

# =====================================================
# GREEDY (PURE MEANINGFUL EVALUATION) ‚Äî FIXED 50 QUERIES (SEQUENTIAL), GROWING SAMPLE COUNTS
# For each sample size S in SAMPLE_COUNTS:
#   - For each of the first 50 service requests IN SEQUENCE:
#       - evaluate EXACTLY S unique random valid combinations (no exploit/repeats)
#       - compute MCS using: compute_rules_and_mcs_for_client_ids(...)
#       - pick TOP-10 by MCS
#   - record per-request time + totals
#
# Outputs:
# 1) dfstatistics_final_greedy_samples.csv         -> ONE ROW PER SAMPLE SIZE (totals + time)
# 2) dfstatistics_greedy_per_request_all_runs.csv  -> per-request stats for ALL sample sizes
# 3) GREEDY_TOP10_ALL_RUNS.csv                     -> Top-10 for EACH request for EACH sample size
#
# REQUIRES: you already ran UPDATED CELL 1 defining:
#   compute_rules_and_mcs_for_client_ids(client_ids, df_profiles, sr_row, SES_T_MEAN, SES_ALPHA_DEFAULT)
#   make_combination_string(...)
#   get_client_cols(...)
#   SES_ALPHA_DEFAULT
# =====================================================

import numpy as np
import pandas as pd
import random
import time
from typing import List, Tuple, Dict
from google.colab import drive

drive.mount('/content/drive')

# =====================================================
# CONFIG
# =====================================================
NQ = 50
SAMPLE_COUNTS = [100, 200, 300, 400, 500, 600,700,800,900,1000]   # ‚úÖ your schedule
TOP_K = 10

RANDOM_SEED = 42
OPTIMIZATION_TYPE = "Greedy"

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# =====================================================
# INPUT FILES
# =====================================================
COMBOS_PATH       = "/content/Updated_combination_3_MINIST_20_20_V1.csv"  # or your 5k sampled combos file
PROFILES_PATH     = "/content/MNIST_Client_Profiles_For_Composability_20_20.csv"
SERVICE_REQ_PATH  = "/content/Service_Request.csv"

# =====================================================
# OUTPUT FILES
# =====================================================
OUT_PATH_RESULTS        = "/content/GREEDY_TOP10_ALL_RUNS_3.csv"
OUT_PATH_STATS_REQUESTS = "/content/dfstatistics_greedy_per_request_all_runs_3.csv"
OUT_PATH_STATS_FINAL    = "/content/dfstatistics_final_greedy_samples_3.csv"

# =====================================================
# LOAD FILES
# =====================================================
df_combos   = pd.read_csv(COMBOS_PATH)
df_profiles = pd.read_csv(PROFILES_PATH)
Service_Request_df = pd.read_csv(SERVICE_REQ_PATH)

# client columns (use your helper if available)
try:
    client_cols = get_client_cols(df_combos)
except Exception:
    client_cols = [c for c in df_combos.columns if c.startswith("Client_")]

K = len(client_cols)
assert K > 0, "No Client_* columns found."

# =====================================================
# SES threshold: mean(Service_Request_df['Latency(ms)'])
# =====================================================
SES_T_MEAN = Service_Request_df["Latency(ms)"].astype(float).mean()
print("‚úÖ SES_T_MEAN =", SES_T_MEAN)

# =====================================================
# FIXED QUERIES ‚Äî SEQUENTIAL (NO RANDOM)
# =====================================================
assert NQ <= len(Service_Request_df), f"NQ={NQ} exceeds available service requests ({len(Service_Request_df)})"
df_selected_requests = Service_Request_df.iloc[:NQ].reset_index(drop=True)
print(f"‚úÖ Sequential service requests selected: first {NQ} rows (CSV order)")

# =====================================================
# BUILD VALID COMBINATIONS (existing only) from df_combos
# Each combo is a tuple(sorted client ids))
# =====================================================
def row_to_combo_tuple(row) -> Tuple[int, ...] | None:
    ids = []
    for c in client_cols:
        v = row[c]
        if pd.notna(v):
            try:
                ids.append(int(v))
            except Exception:
                return None
    if len(ids) != K:
        return None
    return tuple(sorted(ids))

valid_combos = []
for _, r in df_combos.iterrows():
    t = row_to_combo_tuple(r)
    if t is not None:
        valid_combos.append(t)

# unique combos
valid_combos = list(dict.fromkeys(valid_combos))
assert len(valid_combos) > 0, "No valid combinations found."
print(f"‚úÖ Valid existing combinations available for sampling: {len(valid_combos)}")
print("Example combo:", valid_combos[0])

# =====================================================
# HELPER: format combo as string
# =====================================================
def make_combination_string_from_ids(client_ids: List[int]) -> str:
    return "_".join(str(int(x)) for x in client_ids)

# =====================================================
# PURE GREEDY EVALUATION FOR ONE REQUEST:
# - evaluate exactly S unique combos (no repeats)
# - return top-10
# =====================================================
def evaluate_S_unique_combos_for_request(sr_row: pd.Series, request_seed: int, S: int):
    random.seed(request_seed)
    np.random.seed(request_seed)

    # sample unique combos
    n_pick = min(int(S), len(valid_combos))
    sampled = random.sample(valid_combos, n_pick)

    attempted = n_pick
    valid = 0
    records = []

    for combo_tup in sampled:
        client_ids = list(combo_tup)

        out_rules = compute_rules_and_mcs_for_client_ids(
            client_ids,
            df_profiles,
            sr_row,
            SES_T_MEAN,
            SES_ALPHA_DEFAULT
        )
        if out_rules is None:
            continue

        valid += 1
        records.append({
            "Combination": make_combination_string_from_ids(client_ids),
            "Num_Clients": int(len(client_ids)),
            **out_rules
        })

    df_scores = pd.DataFrame(records)
    if df_scores.empty:
        return pd.DataFrame(), attempted, valid

    df_top = df_scores.sort_values("MCS", ascending=False).head(TOP_K).reset_index(drop=True)
    df_top["Rank"] = np.arange(1, len(df_top) + 1)
    return df_top, attempted, valid

# =====================================================
# MAIN LOOP: SAMPLE_COUNTS (like RF/Epsilon-Greedy)
# =====================================================
all_results = []
all_request_stats = []
final_stats = []

for S in SAMPLE_COUNTS:
    print("\n======================================")
    print(f"üü¶ GREEDY RUN (PURE): NQ={NQ} | UNIQUE_SAMPLES_PER_QUERY={S}")
    print("======================================")

    run_start = time.time()
    stats_records = []
    top_records = []

    # sequential requests
    for rq_idx, sr_row in df_selected_requests.iterrows():
        req_id = int(sr_row["Request_ID"]) if "Request_ID" in sr_row else (rq_idx + 1)
        scenario = str(sr_row["Scenario"]) if "Scenario" in sr_row else f"Request{req_id}"

        req_start = time.time()

        df_top, attempted, valid = evaluate_S_unique_combos_for_request(
            sr_row=sr_row,
            request_seed=RANDOM_SEED + int(req_id) + int(S),
            S=int(S)
        )

        req_time = time.time() - req_start

        stats_records.append({
            "Optimization_Type": OPTIMIZATION_TYPE,
            "Fixed_Num_Service_Requests_NQ": int(NQ),
            "Sampled_Services_Per_Query": int(S),

            "Request_ID": int(req_id),
            "Scenario": scenario,

            "Attempted_Samples": int(attempted),
            "Valid_Evaluations": int(valid),
            "Time_Seconds": float(req_time)
        })

        if df_top.empty:
            continue

        df_top = df_top.copy()
        df_top["Optimization_Type"] = OPTIMIZATION_TYPE
        df_top["Sampled_Services_Per_Query"] = int(S)
        df_top["Request_ID"] = int(req_id)
        df_top["Scenario"] = scenario

        df_top = df_top[[
            "Optimization_Type", "Sampled_Services_Per_Query",
            "Request_ID", "Scenario", "Rank",
            "Combination", "Num_Clients",
            "MCS", "DHS", "SHS", "SES", "HSQ", "SRS", "MUS"
        ]]

        top_records.append(df_top)

    total_time = time.time() - run_start
    dfstats_run = pd.DataFrame(stats_records)

    all_request_stats.append(dfstats_run)

    final_stats.append({
        "Optimization_Type": OPTIMIZATION_TYPE,
        "Fixed_Num_Service_Requests_NQ": int(NQ),
        "Sampled_Services_Per_Query": int(S),
        "Total_Time_Seconds": float(total_time),
        "Total_Attempted_Samples": int(dfstats_run["Attempted_Samples"].sum()) if not dfstats_run.empty else 0,
        "Total_Valid_Evaluations": int(dfstats_run["Valid_Evaluations"].sum()) if not dfstats_run.empty else 0
    })

    if len(top_records) > 0:
        all_results.append(pd.concat(top_records, ignore_index=True))

# =====================================================
# SAVE ALL OUTPUTS
# =====================================================
dfstatistics_requests_all = pd.concat(all_request_stats, ignore_index=True) if len(all_request_stats) else pd.DataFrame()
dfstatistics_final = pd.DataFrame(final_stats)

# neat ordering
if not dfstatistics_requests_all.empty:
    dfstatistics_requests_all = dfstatistics_requests_all.sort_values(
        by=["Request_ID", "Sampled_Services_Per_Query"],
        ascending=[True, True]
    ).reset_index(drop=True)

dfstatistics_requests_all.to_csv(OUT_PATH_STATS_REQUESTS, index=False)
dfstatistics_final.to_csv(OUT_PATH_STATS_FINAL, index=False)

print("\n‚úÖ Saved per-request Greedy statistics:", OUT_PATH_STATS_REQUESTS)
print("‚úÖ Saved final Greedy summary:", OUT_PATH_STATS_FINAL)

if len(all_results) > 0:
    df_results_all = pd.concat(all_results, ignore_index=True)

    df_results_all = df_results_all.sort_values(
        by=["Request_ID", "Sampled_Services_Per_Query", "Rank"],
        ascending=[True, True, True]
    ).reset_index(drop=True)

    df_results_all.to_csv(OUT_PATH_RESULTS, index=False)
    print("‚úÖ Saved Greedy top-10 results:", OUT_PATH_RESULTS)
    print("‚úÖ Total top-10 rows =", len(df_results_all))  # ~ NQ * TOP_K * len(SAMPLE_COUNTS)
    display(df_results_all.head(20))
else:
    print("‚ùå No top-10 results produced. Check data / paths.")

display(dfstatistics_final)

import pandas as pd
import matplotlib.pyplot as plt
# =========================
# INPUT
# =========================
RF_PATH = "/content/GREEDY_TOP10_ALL_RUNS_3.csv"
MANUAL_THRESHOLD = 0.89   # <<< SET THIS MANUALLY

# =========================
# LOAD
# =========================
rf = pd.read_csv(RF_PATH)

# =========================
# APPLY MANUAL THRESHOLD
# =========================
rf["Above_Threshold"] = rf["MCS"] >= MANUAL_THRESHOLD

# =========================
# ACCURACY PER QUERY SIZE
# =========================
accuracy_df = (
    rf.groupby("Sampled_Services_Per_Query")["Above_Threshold"]
      .mean()
      .reset_index()
      .rename(columns={"Above_Threshold": "Accuracy"})
      .sort_values("Sampled_Services_Per_Query")
)

print(accuracy_df)

# =========================
# BAR CHART
# =========================
plt.figure(figsize=(7, 4))
plt.bar(
    accuracy_df["Sampled_Services_Per_Query"].astype(str),
    accuracy_df["Accuracy"]
)
plt.xlabel("Sampled Services Per Query")
plt.ylabel("Accuracy (MCS ‚â• Threshold)")
plt.ylim(0, 1)
plt.title(f"Accuracy vs Query Size (Threshold = {MANUAL_THRESHOLD})")
plt.tight_layout()
plt.show()

"""**MAB**
---
"""

# =====================================================
# MAB (UCB1) ‚Äî FIXED 50 QUERIES (SEQUENTIAL), GROWING BUDGET (SAMPLE_COUNTS)
# For each budget S:
#   - For each of the first 50 service requests IN SEQUENCE:
#       - run UCB1 for BUDGET_EVALS = S steps (request-aware)
#       - return TOP-1 combo by best observed MCS (unique best)
#   - record per-request time + totals
#
# Outputs:
# 1) dfstatistics_final_ucb_samples.csv            -> ONE ROW PER SAMPLE SIZE (totals + time)
# 2) dfstatistics_ucb_per_request_all_runs.csv     -> per-request stats for ALL sample sizes
# 3) UCB_TOP1_ALL_RUNS.csv                         -> Top-1 for EACH request for EACH sample size
#
# REQUIRES: UPDATED CELL 1 already ran:
#   compute_rules_and_mcs_for_client_ids(client_ids, df_profiles, sr_row, SES_T_MEAN, SES_ALPHA_DEFAULT)
#   SES_ALPHA_DEFAULT
# =====================================================

import numpy as np
import pandas as pd
import random
import time
from typing import Tuple, Dict, List
from google.colab import drive

drive.mount('/content/drive')

# =====================================================
# CONFIG
# =====================================================
NQ = 50
SAMPLE_COUNTS = [100, 200, 300, 400, 500, 600,700,800,900,1000]   # ‚úÖ your schedule
TOP_K = 1                                        # ‚úÖ top-1 only

UCB_C       = 0.70
RANDOM_SEED = 42
OPTIMIZATION_TYPE = "MAB_UCB1"

random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

# =====================================================
# INPUT FILES
# =====================================================
# =====================================================
COMBOS_PATH       = "/content/Updated_combination_3_MINIST_20_20_V1.csv"  # or your 5k sampled combos file
#/content/Updated_combination_3_MINIST_20_20_V1.csv
PROFILES_PATH     = "/content/MNIST_Client_Profiles_For_Composability_20_20.csv"
SERVICE_REQ_PATH  = "/content/Service_Request.csv"

# =====================================================
# OUTPUT FILES
# =====================================================
OUT_PATH_RESULTS        = "/content/UCB_TOP1_ALL_RUNS_3.csv"
OUT_PATH_STATS_REQUESTS = "/content/dfstatistics_ucb_per_request_all_runs_3.csv"
OUT_PATH_STATS_FINAL    = "/content/dfstatistics_final_ucb_samples_3.csv"

# =====================================================
# LOAD FILES
# =====================================================
df_combos   = pd.read_csv(COMBOS_PATH)
df_profiles = pd.read_csv(PROFILES_PATH)
Service_Request_df = pd.read_csv(SERVICE_REQ_PATH)

client_cols = [c for c in df_combos.columns if c.startswith("Client_")]
K = len(client_cols)
assert K > 0, "No Client_* columns found."

# =====================================================
# SES threshold: mean(Service_Request_df['Latency(ms)'])
# =====================================================
SES_T_MEAN = Service_Request_df["Latency(ms)"].astype(float).mean()
print("‚úÖ SES_T_MEAN =", SES_T_MEAN)

# =====================================================
# FIXED QUERIES ‚Äî SEQUENTIAL (NO RANDOM)
# =====================================================
assert NQ <= len(Service_Request_df), f"NQ={NQ} exceeds available service requests ({len(Service_Request_df)})"
df_selected_requests = Service_Request_df.iloc[:NQ].reset_index(drop=True)
print(f"‚úÖ Sequential service requests selected: first {NQ} rows (CSV order)")

# =====================================================
# BUILD VALID COMBINATIONS (ARMS)
# =====================================================
def row_to_combo_tuple(row) -> Tuple[int, ...] | None:
    ids = []
    for c in client_cols:
        v = row[c]
        if pd.notna(v):
            try:
                ids.append(int(v))
            except Exception:
                return None
    if len(ids) != K:
        return None
    return tuple(sorted(ids))

valid_combos: List[Tuple[int, ...]] = []
for _, r in df_combos.iterrows():
    t = row_to_combo_tuple(r)
    if t is not None:
        valid_combos.append(t)

valid_combos = list(dict.fromkeys(valid_combos))
assert len(valid_combos) > 0, "No valid combinations found."

print(f"‚úÖ Total arms (valid combinations): {len(valid_combos)}")
print("Example arm:", valid_combos[0])

# =====================================================
# HELPER: combo string
# =====================================================
def make_combination_string_from_ids(client_ids: List[int]) -> str:
    return "_".join(str(int(x)) for x in client_ids)

# =====================================================
# RUN UCB1 FOR ONE REQUEST (budget-aware, request-aware)
# Efficient: maintains stats only for pulled arms.
# =====================================================
def run_ucb1_for_request(sr_row: pd.Series, request_seed: int, BUDGET_EVALS: int):
    random.seed(request_seed)
    np.random.seed(request_seed)

    _eval_cache: Dict[Tuple[int, ...], Dict] = {}

    def evaluate_arm(arm: Tuple[int, ...]) -> Dict:
        if arm in _eval_cache:
            return _eval_cache[arm]

        client_ids = list(arm)

        out_rules = compute_rules_and_mcs_for_client_ids(
            client_ids,
            df_profiles,
            sr_row,
            SES_T_MEAN,
            SES_ALPHA_DEFAULT
        )

        if out_rules is None:
            out = {
                "Combination": make_combination_string_from_ids(client_ids),
                "Num_Clients": len(client_ids),
                "DHS": np.nan, "SHS": np.nan, "SES": np.nan,
                "HSQ": np.nan, "SRS": np.nan, "MUS": np.nan,
                "MCS": -1.0
            }
        else:
            out = {
                "Combination": make_combination_string_from_ids(client_ids),
                "Num_Clients": len(client_ids),
                **out_rules
            }

        mcs = out.get("MCS", -1.0)
        out["Reward"] = float(mcs) if (mcs is not None and np.isfinite(mcs) and mcs > 0) else 0.0
        _eval_cache[arm] = out
        return out

    # pulled-arm stats only
    counts: Dict[Tuple[int, ...], int] = {}
    means:  Dict[Tuple[int, ...], float] = {}

    unpulled = valid_combos[:]   # big list, but we only pop <= budget
    random.shuffle(unpulled)

    history = []
    attempted = 0
    valid = 0
    step = 0

    best_arm = None
    best_reward = -1.0
    best_res = None

    BUDGET_EVALS = int(BUDGET_EVALS)

    # ---- Phase 1: pull NEW arms (uninformative to re-pull) until we run out of budget
    # (This is the meaningful part under deterministic rewards)
    while step < BUDGET_EVALS and len(unpulled) > 0:
        arm = unpulled.pop()
        step += 1
        attempted += 1

        res = evaluate_arm(arm)
        r = float(res["Reward"])

        counts[arm] = 1
        means[arm] = r

        if r > 0:
            valid += 1

        if r > best_reward:
            best_reward = r
            best_arm = arm
            best_res = res

        history.append({
            "step": step,
            "action": "new_arm",
            "Combination": res["Combination"],
            "Num_Clients": int(res.get("Num_Clients", len(arm))),
            "DHS": res.get("DHS", np.nan),
            "SHS": res.get("SHS", np.nan),
            "SES": res.get("SES", np.nan),
            "HSQ": res.get("HSQ", np.nan),
            "SRS": res.get("SRS", np.nan),
            "MUS": res.get("MUS", np.nan),
            "MCS": float(res.get("MCS", -1.0)),
            "reward": float(r),
        })

    # ---- Phase 2 (optional): classic UCB repeat pulls if budget remains
    # (Usually doesn‚Äôt help under deterministic rewards, but kept for correctness)
    while step < BUDGET_EVALS and len(counts) > 0:
        step += 1
        attempted += 1

        max_ucb = -np.inf
        best_arms = []

        for arm, mu in means.items():
            n = counts[arm]
            bonus = float(UCB_C) * np.sqrt(2.0 * np.log(step) / n)
            ucb = mu + bonus

            if ucb > max_ucb:
                max_ucb = ucb
                best_arms = [arm]
            elif ucb == max_ucb:
                best_arms.append(arm)

        arm = random.choice(best_arms)

        res = evaluate_arm(arm)
        r = float(res["Reward"])

        counts[arm] += 1
        n = counts[arm]
        means[arm] = means[arm] + (r - means[arm]) / n

        if r > 0:
            valid += 1

        if r > best_reward:
            best_reward = r
            best_arm = arm
            best_res = res

        history.append({
            "step": step,
            "action": "ucb_repull",
            "Combination": res["Combination"],
            "Num_Clients": int(res.get("Num_Clients", len(arm))),
            "DHS": res.get("DHS", np.nan),
            "SHS": res.get("SHS", np.nan),
            "SES": res.get("SES", np.nan),
            "HSQ": res.get("HSQ", np.nan),
            "SRS": res.get("SRS", np.nan),
            "MUS": res.get("MUS", np.nan),
            "MCS": float(res.get("MCS", -1.0)),
            "reward": float(r),
        })

    df_hist = pd.DataFrame(history)

    # TOP-1 = best observed (already tracked)
    if best_res is None:
        df_top1 = pd.DataFrame()
    else:
        df_top1 = pd.DataFrame([{
            "Combination": best_res["Combination"],
            "Num_Clients": int(best_res.get("Num_Clients", len(best_arm) if best_arm else 0)),
            "MCS": float(best_res.get("MCS", -1.0)),
            "DHS": best_res.get("DHS", np.nan),
            "SHS": best_res.get("SHS", np.nan),
            "SES": best_res.get("SES", np.nan),
            "HSQ": best_res.get("HSQ", np.nan),
            "SRS": best_res.get("SRS", np.nan),
            "MUS": best_res.get("MUS", np.nan),
        }])

    return df_hist, df_top1, attempted, valid, best_reward, best_arm
# =====================================================
# MAIN LOOP: SAMPLE_COUNTS
# =====================================================
all_results = []
all_request_stats = []
final_stats = []
for S in SAMPLE_COUNTS:
    print("\n======================================")
    print(f"üü¶ UCB RUN (PURE MEANINGFUL): NQ={NQ} | BUDGET_EVALS={S}")
    print("======================================")
    run_start = time.time()
    stats_records = []
    top_records = []
    # sequential requests
    for rq_idx, sr_row in df_selected_requests.iterrows():
        req_id = int(sr_row["Request_ID"]) if "Request_ID" in sr_row else (rq_idx + 1)
        scenario = str(sr_row["Scenario"]) if "Scenario" in sr_row else f"Request{req_id}"

        req_start = time.time()

        df_hist, df_top1, attempted, valid, best_reward, best_arm = run_ucb1_for_request(
            sr_row=sr_row,
            request_seed=RANDOM_SEED + int(req_id) + int(S),
            BUDGET_EVALS=int(S)
        )

        req_time = time.time() - req_start

        stats_records.append({
            "Optimization_Type": OPTIMIZATION_TYPE,
            "Fixed_Num_Service_Requests_NQ": int(NQ),
            "Sampled_Services_Per_Query": int(S),

            "Request_ID": int(req_id),
            "Scenario": scenario,

            "BUDGET_EVALS": int(S),
            "UCB_C": float(UCB_C),

            "Attempted_Steps": int(attempted),
            "Valid_Evaluations": int(valid),
            "Time_Seconds": float(req_time),

            "Best_MCS": float(best_reward) if np.isfinite(best_reward) else np.nan,
            "Best_Combination": make_combination_string_from_ids(list(best_arm)) if best_arm else None
        })

        if df_top1.empty:
            continue

        df_top1 = df_top1.copy()
        df_top1["Optimization_Type"] = OPTIMIZATION_TYPE
        df_top1["Sampled_Services_Per_Query"] = int(S)
        df_top1["Request_ID"] = int(req_id)
        df_top1["Scenario"] = scenario
        df_top1["Rank"] = 1

        df_top1 = df_top1[[
            "Optimization_Type", "Sampled_Services_Per_Query",
            "Request_ID", "Scenario", "Rank",
            "Combination", "Num_Clients",
            "MCS", "DHS", "SHS", "SES", "HSQ", "SRS", "MUS"
        ]]

        top_records.append(df_top1)

    total_time = time.time() - run_start
    dfstats_run = pd.DataFrame(stats_records)
    all_request_stats.append(dfstats_run)
    final_stats.append({
        "Optimization_Type": OPTIMIZATION_TYPE,
        "Fixed_Num_Service_Requests_NQ": int(NQ),
        "Sampled_Services_Per_Query": int(S),
        "Total_Time_Seconds": float(total_time),
        "Total_Attempted_Steps": int(dfstats_run["Attempted_Steps"].sum()) if not dfstats_run.empty else 0,
        "Total_Valid_Evaluations": int(dfstats_run["Valid_Evaluations"].sum()) if not dfstats_run.empty else 0
    })
    if len(top_records) > 0:
        all_results.append(pd.concat(top_records, ignore_index=True))
# =====================================================
# SAVE ALL OUTPUTS
# =====================================================
dfstatistics_requests_all = pd.concat(all_request_stats, ignore_index=True) if len(all_request_stats) else pd.DataFrame()
dfstatistics_final = pd.DataFrame(final_stats)

# neat ordering
if not dfstatistics_requests_all.empty:
    dfstatistics_requests_all = dfstatistics_requests_all.sort_values(
        by=["Request_ID", "Sampled_Services_Per_Query"],
        ascending=[True, True]
    ).reset_index(drop=True)

dfstatistics_requests_all.to_csv(OUT_PATH_STATS_REQUESTS, index=False)
dfstatistics_final.to_csv(OUT_PATH_STATS_FINAL, index=False)
print("\n‚úÖ Saved per-request UCB statistics:", OUT_PATH_STATS_REQUESTS)
print("‚úÖ Saved final UCB summary:", OUT_PATH_STATS_FINAL)

if len(all_results) > 0:
    df_results_all = pd.concat(all_results, ignore_index=True)

    df_results_all = df_results_all.sort_values(
        by=["Request_ID", "Sampled_Services_Per_Query", "Rank"],
        ascending=[True, True, True]
    ).reset_index(drop=True)

    df_results_all.to_csv(OUT_PATH_RESULTS, index=False)
    print("‚úÖ Saved UCB top-1 results:", OUT_PATH_RESULTS)
    print("‚úÖ Total top-1 rows =", len(df_results_all))  # ~ NQ * 1 * len(SAMPLE_COUNTS)
    display(df_results_all.head(20))
else:
    print("‚ùå No top-1 results produced. Check inputs / rules.")
display(dfstatistics_final)

import pandas as pd
import matplotlib.pyplot as plt
RF_PATH = "/content/UCB_TOP1_ALL_RUNS_3.csv"
MANUAL_THRESHOLD = 0.895   # <<< SET THIS MANUALLY
rf = pd.read_csv(RF_PATH)
# =========================
# APPLY MANUAL THRESHOLD
# =========================
rf["Above_Threshold"] = rf["MCS"] >= MANUAL_THRESHOLD
# =========================
# ACCURACY PER QUERY SIZE
# =========================
accuracy_df = (
    rf.groupby("Sampled_Services_Per_Query")["Above_Threshold"]
      .mean()
      .reset_index()
      .rename(columns={"Above_Threshold": "Accuracy"})
      .sort_values("Sampled_Services_Per_Query")
)
print(accuracy_df)
plt.figure(figsize=(7, 4))
plt.bar(
    accuracy_df["Sampled_Services_Per_Query"].astype(str),
    accuracy_df["Accuracy"]
)
plt.xlabel("Sampled Services Per Query")
plt.ylabel("Accuracy (MCS ‚â• Threshold)")
plt.ylim(0, 1)
plt.title(f"Accuracy vs Query Size (Threshold = {MANUAL_THRESHOLD})")
plt.tight_layout()
plt.show()

/content/EG_TOP10_RESULTS_ALL_REQUEST_COUNTS.csv,
/content/GA_TOP10_LASTRUN.csv,
/content/GREEDY_TOP10_RESULTS_ALL_REQUEST_COUNTS.csv,
/content/UCB_TOP10_RESULTS_ALL_REQUEST_COUNTS.csv,
/content/TOP10_COMBOS_FOR_20_SERVICE_REQUESTS.csv

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# =========================
# INPUT FILES (from your image)
# =========================
FILES = {
    "EG":     "/content/EG_TOP10_RESULTS_ALL_REQUEST_COUNTS.csv",
    "GA":     "/content/GA_TOP10_LASTRUN.csv",
    "GREEDY": "/content/GREEDY_TOP10_RESULTS_ALL_REQUEST_COUNTS.csv",
    "UCB":    "/content/UCB_TOP10_RESULTS_ALL_REQUEST_COUNTS.csv",
}

# =========================
# LOAD + STANDARDIZE
# =========================
dfs = []
for algo, path in FILES.items():
    df = pd.read_csv(path)

    # keep only what we need (but tolerate extra cols)
    needed = ["Request_ID", "Request_Count", "MCS"]
    missing = [c for c in needed if c not in df.columns]
    if missing:
        raise ValueError(f"{algo} file is missing columns: {missing}. Found: {list(df.columns)}")

    df = df[needed].copy()
    df["Algorithm"] = algo
    dfs.append(df)

all_df = pd.concat(dfs, ignore_index=True)

# Ensure numeric
all_df["Request_Count"] = pd.to_numeric(all_df["Request_Count"], errors="coerce")
all_df["MCS"] = pd.to_numeric(all_df["MCS"], errors="coerce")
all_df = all_df.dropna(subset=["Request_Count", "MCS"])

# =========================
# CREATE Low / Medium / High GROUPS from Request_Count
# (robust: uses sorted unique counts and splits into 3 buckets)
# =========================
unique_counts = np.sort(all_df["Request_Count"].unique())

if len(unique_counts) < 3:
    raise ValueError(f"Need at least 3 distinct Request_Count values to form Low/Medium/High. Found: {unique_counts}")

# Split counts into 3 groups by index (not by value), so it works for 20/40/60... or 1000/2000...
idx_splits = np.array_split(np.arange(len(unique_counts)), 3)

low_counts    = set(unique_counts[idx_splits[0]])
medium_counts = set(unique_counts[idx_splits[1]])
high_counts   = set(unique_counts[idx_splits[2]])

def count_to_category(x):
    if x in low_counts: return "Low"
    if x in medium_counts: return "Medium"
    return "High"

all_df["Category"] = all_df["Request_Count"].apply(count_to_category)

# Fix category order
cat_order = ["Low", "Medium", "High"]
all_df["Category"] = pd.Categorical(all_df["Category"], categories=cat_order, ordered=True)

# =========================
# PLOT: 2x2 VIOLIN (real Top10 MCS per Request_ID)
# =========================
fig, axes = plt.subplots(2, 2, figsize=(14, 10), sharey=True, sharex=True)
axes = axes.flatten()

plot_order = ["EG", "GA", "GREEDY", "UCB"]

for ax, algo in zip(axes, plot_order):
    sub = all_df[all_df["Algorithm"] == algo]

    sns.violinplot(
        data=sub,
        x="Category",
        y="MCS",
        inner="box",       # shows median/IQR inside violin
        cut=0,
        ax=ax
    )
    ax.set_title(algo, fontsize=14)
    ax.set_xlabel("Category", fontsize=12)
    ax.set_ylabel("MCS [0, 1]", fontsize=12)
    ax.set_ylim(0, 1)
    ax.grid(True, linestyle="--", alpha=0.6)

fig.suptitle("Violin Plots of Top-10 MCS per Request_ID grouped by Request_Count Category", fontsize=16, y=0.98)
plt.tight_layout()
plt.show()

# Optional: sanity check (how many points per algo/category)
print(all_df.groupby(["Algorithm", "Category"])["MCS"].count())

import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ============================================================
# CONFIG
# ============================================================
REQUEST_COUNTS = [20, 40, 60, 80, 100, 120, 140, 160, 180]   # cumulative buckets: 1..N
TOP_K = 10                                                   # top-10 per Request_ID

FILES = {
    "EG":     "/content/EG_TOP10_RESULTS_ALL_REQUEST_COUNTS.csv",
    "GA":     "/content/GA_TOP10_LASTRUN.csv",
    "GREEDY": "/content/GREEDY_TOP10_RESULTS_ALL_REQUEST_COUNTS.csv",
    "UCB":    "/content/UCB_TOP10_RESULTS_ALL_REQUEST_COUNTS.csv",
}

# ============================================================
# HELPERS
# ============================================================
def to_request_int(x):
    """
    Robustly convert Request_ID to an integer.
    Works for:
      - 1, 2, 3 ...
      - "Request_12", "R12", "12" etc.
    """
    if pd.isna(x):
        return np.nan
    if isinstance(x, (int, np.integer)):
        return int(x)
    if isinstance(x, float) and x.is_integer():
        return int(x)
    s = str(x)
    m = re.search(r"\d+", s)
    return int(m.group()) if m else np.nan
def load_standardize(algo, path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"[{algo}] File not found: {path}")

    df = pd.read_csv(path)

    needed = ["Request_ID", "MCS"]
    missing = [c for c in needed if c not in df.columns]
    if missing:
        raise ValueError(f"[{algo}] Missing columns {missing}. Found: {list(df.columns)}")

    out = df[["Request_ID", "MCS"]].copy()

    # If Rank exists, keep only top-K (rank 1..TOP_K)
    if "Rank" in df.columns:
        out["Rank"] = pd.to_numeric(df["Rank"], errors="coerce")
        out = out[out["Rank"].between(1, TOP_K)]
        out = out.drop(columns=["Rank"])

    # Convert request id -> integer index
    out["Request_ID_int"] = out["Request_ID"].apply(to_request_int)
    out["MCS"] = pd.to_numeric(out["MCS"], errors="coerce")

    out = out.dropna(subset=["Request_ID_int", "MCS"]).copy()
    out["Request_ID_int"] = out["Request_ID_int"].astype(int)

    out["Algorithm"] = algo
    return out rapist girl
def build_cumulative_buckets(df_algo):
    """
    For each N in REQUEST_COUNTS, select Request_ID_int <= N (i.e., 1..N)
    and aggregate all MCS values in that range.
    Returns long-format df with columns: Algorithm, Bucket, MCS
    """
    rows = []
    for N in REQUEST_COUNTS:
        sub = df_algo[df_algo["Request_ID_int"] <= N]
        # Store all MCS values (not mean)
        for v in sub["MCS"].values:
            rows.append({"Bucket": str(N), "MCS": v})
    out = pd.DataFrame(rows)
    return out
# ============================================================
# LOAD ALL
# ============================================================
dfs = []
for algo, path in FILES.items():
    dfs.append(load_standardize(algo, path))

all_df = pd.concat(dfs, ignore_index=True) democracy
# ============================================================
# BUILD BUCKETED DATA (cumulative 1..N)
# ============================================================
bucketed = []
for algo in FILES.keys():
    df_algo = all_df[all_df["Algorithm"] == algo].copy()
    b = build_cumulative_buckets(df_algo)
    b["Algorithm"] = algo
    bucketed.append(b)
bucketed_df = pd.concat(bucketed, ignore_index=True)
# Ensure bucket order 20,40,...,180
bucket_order = [str(x) for x in REQUEST_COUNTS]
bucketed_df["Bucket"] = pd.Categorical(bucketed_df["Bucket"], categories=bucket_order, ordered=True)
sns.set_style("whitegrid")
fig, axes = plt.subplots(2, 2, figsize=(14, 10), sharey=True)
axes = axes.flatten()

plot_order = ["EG", "GA", "GREEDY", "UCB"]
colors = ["#A93226", "#17A589", "#1F618D", "#626567"]

for ax, algo, color in zip(axes, plot_order, colors):
    sub = bucketed_df[bucketed_df["Algorithm"] == algo]

    sns.violinplot(
        data=sub,
        x="Bucket",
        y="MCS",
        color=color,
        inner="box",
        cut=0,
        linewidth=1,
        ax=ax
    )

    ax.set_title(algo, fontsize=14)
    ax.set_xlabel("Cumulative Service Requests (1..N)", fontsize=12)
    ax.set_ylabel("MCS [0, 1]", fontsize=12)
    ax.set_ylim(0, 1)
    ax.grid(True, linestyle="--", alpha=0.6)

fig.suptitle(
    f"Violin Plots of Aggregated Top-{TOP_K} MCS Values across Cumulative Request Ranges",
    fontsize=16,
    y=0.95
)
plt.tight_layout()
plt.show()

# ============================================================
# SANITY CHECKS
# ============================================================
print("\nSanity check: rows per algo per bucket (should grow with N):")
print(bucketed_df.groupby(["Algorithm", "Bucket"])["MCS"].count())

print("\nSanity check: Request_ID_int range per algorithm:")
print(all_df.groupby("Algorithm")["Request_ID_int"].agg(["min", "max", "nunique"]))

"""**Visulization 2**
---
"""

import pandas as pd

# =========================
# INPUT FILES
# =========================
files = [
    "dfstatistics_final.csv",          # has Num_Service_Requests (or similar)
    "dfstatistics_final_eg.csv",
    "dfstatistics_final_ga.csv",
    "dfstatistics_final_greedy.csv",
    "dfstatistics_final_ucb.csv",
]

# =========================
# OUTPUT FILE
# =========================
OUT_MERGED = "/content/dfstatistics_final_ALL.csv"

# =========================
# Helper: detect request-count column
# =========================
def detect_request_col(df):
    # preferred standard
    if "Request_Count" in df.columns:
        return "Request_Count"

    # your known variants
    for c in ["Num_Service_Requests", "numServiceRequest", "NumServiceRequest", "num_service_requests"]:
        if c in df.columns:
            return c

    # fallback: anything containing both "service" and "request" and "num/count"
    for c in df.columns:
        cl = c.lower()
        if ("request" in cl or "service" in cl) and ("num" in cl or "count" in cl):
            return c

    raise ValueError(f"‚ùå Could not find request-count column in: {list(df.columns)}")

# =========================
# LOAD + STANDARDIZE + MERGE
# =========================
dfs = []

for f in files:
    df = pd.read_csv(f)

    # find request column and rename -> Request_Count
    req_col = detect_request_col(df)
    if req_col != "Request_Count":
        df = df.rename(columns={req_col: "Request_Count"})

    # ensure required columns exist
    if "Optimization_Type" not in df.columns:
        # fallback: infer from filename if missing
        df["Optimization_Type"] = f.replace("dfstatistics_final_", "").replace(".csv", "").upper()

    if "Total_Time_Seconds" not in df.columns:
        raise ValueError(f"‚ùå Missing Total_Time_Seconds in {f}. Columns={list(df.columns)}")

    # keep only requested columns
    df = df[["Optimization_Type", "Request_Count", "Total_Time_Seconds"]].copy()

    # clean types
    df["Request_Count"] = pd.to_numeric(df["Request_Count"], errors="coerce")
    df["Total_Time_Seconds"] = pd.to_numeric(df["Total_Time_Seconds"], errors="coerce")

    # drop bad rows
    df = df.dropna(subset=["Request_Count", "Total_Time_Seconds"])

    dfs.append(df)

df_merged = pd.concat(dfs, ignore_index=True)

# optional: sort nicely
df_merged = df_merged.sort_values(["Optimization_Type", "Request_Count"]).reset_index(drop=True)

# save
df_merged.to_csv(OUT_MERGED, index=False)

print("‚úÖ Saved merged CSV:", OUT_MERGED)
display(df_merged.head(30))

df_merged['Optimization_Type'].value_counts()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load merged CSV
path = "dfstatistics_final_ALL.csv"
df = pd.read_csv(path)

# Map Request_Count values to scalability index 1..9
request_counts = [20, 40, 60, 80, 100, 120, 140, 160, 180]
x_map = {rc: i + 1 for i, rc in enumerate(request_counts)}

df["Request_Count"] = pd.to_numeric(df["Request_Count"], errors="coerce")
df["Total_Time_Seconds"] = pd.to_numeric(df["Total_Time_Seconds"], errors="coerce")
df = df.dropna(subset=["Request_Count", "Total_Time_Seconds", "Optimization_Type"]).copy()

df["Scalability_Index"] = df["Request_Count"].map(x_map)
df = df.dropna(subset=["Scalability_Index"]).copy()
df["Scalability_Index"] = df["Scalability_Index"].astype(int)

# Convert seconds to nanoseconds
df["Total_Time_Nanoseconds"] = df["Total_Time_Seconds"] * 1e9

# Aggregate in case there are duplicates (take mean)
df_plot = (
    df.groupby(["Optimization_Type", "Scalability_Index"], as_index=False)
      .agg({"Total_Time_Nanoseconds": "mean"})
      .sort_values(["Optimization_Type", "Scalability_Index"])
)

# Marker styles (different shapes)
markers = ["o", "s", "^", "D", "v", "P", "X", "*", ">", "<", "h", "H", "p", "8"]
opt_types = sorted(df_plot["Optimization_Type"].unique().tolist())
marker_map = {opt: markers[i % len(markers)] for i, opt in enumerate(opt_types)}

# Plot
plt.figure(figsize=(10, 5))

for opt in opt_types:
    g = df_plot[df_plot["Optimization_Type"] == opt].sort_values("Scalability_Index")
    plt.plot(
        g["Scalability_Index"],
        g["Total_Time_Nanoseconds"],
        marker=marker_map[opt],
        linewidth=2,
        label=opt
    )

plt.xticks(range(1, 10), [str(i) for i in range(1, 10)])
plt.xlabel("Scalability Index (20..180 requests mapped to 1..9)")
plt.ylabel("Total Time (nanoseconds)")
plt.title("Scalability Plot by Optimization Technique")
plt.grid(True, linestyle="--", alpha=0.6)
plt.legend(title="Optimization Technique", ncol=2)
plt.tight_layout()
plt.show()

"""**Visulization 3**
---
"""

import pandas as pd

# =========================
# INPUT FILES
# =========================
files = [
    "dfstatistics_final.csv",          # has Num_Service_Requests (or similar)
    "dfstatistics_final_eg.csv",
    "dfstatistics_final_ga.csv",
    "dfstatistics_final_greedy.csv",
    "dfstatistics_final_ucb.csv",
]

# =========================
# OUTPUT FILE
# =========================
OUT_MERGED = "/content/dfstatistics_final_ALL.csv"

# =========================
# Helper: detect request-count column
# =========================
def detect_request_col(df):
    # preferred standard
    if "Request_Count" in df.columns:
        return "Request_Count"

    # your known variants
    for c in ["Num_Service_Requests", "numServiceRequest", "NumServiceRequest", "num_service_requests"]:
        if c in df.columns:
            return c

    # fallback: anything containing both "service" and "request" and "num/count"
    for c in df.columns:
        cl = c.lower()
        if ("request" in cl or "service" in cl) and ("num" in cl or "count" in cl):
            return c

    raise ValueError(f"‚ùå Could not find request-count column in: {list(df.columns)}")

# =========================
# LOAD + STANDARDIZE + MERGE
# =========================
dfs = []

for f in files:
    df = pd.read_csv(f)

    # find request column and rename -> Request_Count
    req_col = detect_request_col(df)
    if req_col != "Request_Count":
        df = df.rename(columns={req_col: "Request_Count"})

    # ensure required columns exist
    if "Optimization_Type" not in df.columns:
        # fallback: infer from filename if missing
        df["Optimization_Type"] = f.replace("dfstatistics_final_", "").replace(".csv", "").upper()

    if "Total_Time_Seconds" not in df.columns:
        raise ValueError(f"‚ùå Missing Total_Time_Seconds in {f}. Columns={list(df.columns)}")

    # keep only requested columns
    df = df[["Optimization_Type", "Request_Count", "Total_Time_Seconds"]].copy()

    # clean types
    df["Request_Count"] = pd.to_numeric(df["Request_Count"], errors="coerce")
    df["Total_Time_Seconds"] = pd.to_numeric(df["Total_Time_Seconds"], errors="coerce")

    # drop bad rows
    df = df.dropna(subset=["Request_Count", "Total_Time_Seconds"])

    dfs.append(df)

df_merged = pd.concat(dfs, ignore_index=True)

# optional: sort nicely
df_merged = df_merged.sort_values(["Optimization_Type", "Request_Count"]).reset_index(drop=True)

# save
df_merged.to_csv(OUT_MERGED, index=False)

print("‚úÖ Saved merged CSV:", OUT_MERGED)
display(df_merged.head(30))

df_merged['Optimization_Type'].value_counts()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load merged CSV
path = "dfstatistics_final_ALL.csv"
df = pd.read_csv(path)

# Map Request_Count values to scalability index 1..9
request_counts = [20, 40, 60, 80, 100, 120, 140, 160, 180]
x_map = {rc: i + 1 for i, rc in enumerate(request_counts)}

df["Request_Count"] = pd.to_numeric(df["Request_Count"], errors="coerce")
df["Total_Time_Seconds"] = pd.to_numeric(df["Total_Time_Seconds"], errors="coerce")
df = df.dropna(subset=["Request_Count", "Total_Time_Seconds", "Optimization_Type"]).copy()

df["Scalability_Index"] = df["Request_Count"].map(x_map)
df = df.dropna(subset=["Scalability_Index"]).copy()
df["Scalability_Index"] = df["Scalability_Index"].astype(int)

# Convert seconds to nanoseconds
df["Total_Time_Nanoseconds"] = df["Total_Time_Seconds"] * 1e9

# Aggregate in case there are duplicates (take mean)
df_plot = (
    df.groupby(["Optimization_Type", "Scalability_Index"], as_index=False)
      .agg({"Total_Time_Nanoseconds": "mean"})
      .sort_values(["Optimization_Type", "Scalability_Index"])
)

# Marker styles (different shapes)
markers = ["o", "s", "^", "D", "v", "P", "X", "*", ">", "<", "h", "H", "p", "8"]
opt_types = sorted(df_plot["Optimization_Type"].unique().tolist())
marker_map = {opt: markers[i % len(markers)] for i, opt in enumerate(opt_types)}

# Plot
plt.figure(figsize=(10, 5))

for opt in opt_types:
    g = df_plot[df_plot["Optimization_Type"] == opt].sort_values("Scalability_Index")
    plt.plot(
        g["Scalability_Index"],
        g["Total_Time_Nanoseconds"],
        marker=marker_map[opt],
        linewidth=2,
        label=opt
    )

plt.xticks(range(1, 10), [str(i) for i in range(1, 10)])
plt.xlabel("Scalability Index (20..180 requests mapped to 1..9)")
plt.ylabel("Total Time (nanoseconds)")
plt.title("Scalability Plot by Optimization Technique")
plt.grid(True, linestyle="--", alpha=0.6)
plt.legend(title="Optimization Technique", ncol=2)
plt.tight_layout()
plt.show()



"""**Visulization--5**
---
"""

import pandas as pd

# =========================
# INPUT FILES
# =========================
files = [
    "dfstatistics_final.csv",          # has Num_Service_Requests (or similar)
    "dfstatistics_final_eg.csv",
    "dfstatistics_final_ga.csv",
    "dfstatistics_final_greedy.csv",
    "dfstatistics_final_ucb.csv",
]

# =========================
# OUTPUT FILE
# =========================
OUT_MERGED = "/content/dfstatistics_final_ALL.csv"

# =========================
# Helper: detect request-count column
# =========================
def detect_request_col(df):
    # preferred standard
    if "Request_Count" in df.columns:
        return "Request_Count"

    # your known variants
    for c in ["Num_Service_Requests", "numServiceRequest", "NumServiceRequest", "num_service_requests"]:
        if c in df.columns:
            return c

    # fallback: anything containing both "service" and "request" and "num/count"
    for c in df.columns:
        cl = c.lower()
        if ("request" in cl or "service" in cl) and ("num" in cl or "count" in cl):
            return c

    raise ValueError(f"‚ùå Could not find request-count column in: {list(df.columns)}")

# =========================
# LOAD + STANDARDIZE + MERGE
# =========================
dfs = []

for f in files:
    df = pd.read_csv(f)

    # find request column and rename -> Request_Count
    req_col = detect_request_col(df)
    if req_col != "Request_Count":
        df = df.rename(columns={req_col: "Request_Count"})

    # ensure required columns exist
    if "Optimization_Type" not in df.columns:
        # fallback: infer from filename if missing
        df["Optimization_Type"] = f.replace("dfstatistics_final_", "").replace(".csv", "").upper()

    if "Total_Time_Seconds" not in df.columns:
        raise ValueError(f"‚ùå Missing Total_Time_Seconds in {f}. Columns={list(df.columns)}")

    # keep only requested columns
    df = df[["Optimization_Type", "Request_Count", "Total_Time_Seconds"]].copy()

    # clean types
    df["Request_Count"] = pd.to_numeric(df["Request_Count"], errors="coerce")
    df["Total_Time_Seconds"] = pd.to_numeric(df["Total_Time_Seconds"], errors="coerce")

    # drop bad rows
    df = df.dropna(subset=["Request_Count", "Total_Time_Seconds"])

    dfs.append(df)

df_merged = pd.concat(dfs, ignore_index=True)

# optional: sort nicely
df_merged = df_merged.sort_values(["Optimization_Type", "Request_Count"]).reset_index(drop=True)

# save
df_merged.to_csv(OUT_MERGED, index=False)

print("‚úÖ Saved merged CSV:", OUT_MERGED)
display(df_merged.head(30))

df_merged['Optimization_Type'].value_counts()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load merged CSV
path = "dfstatistics_final_ALL.csv"
df = pd.read_csv(path)

# Map Request_Count values to scalability index 1..9
REQUEST_COUNTS = [60,10, 20, 50, 70, 100,130,150,160,180]   # ‚úÖ runs
x_map = {rc: i + 1 for i, rc in enumerate(request_counts)}

df["Request_Count"] = pd.to_numeric(df["Request_Count"], errors="coerce")
df["Total_Time_Seconds"] = pd.to_numeric(df["Total_Time_Seconds"], errors="coerce")
df = df.dropna(subset=["Request_Count", "Total_Time_Seconds", "Optimization_Type"]).copy()

df["Scalability_Index"] = df["Request_Count"].map(x_map)
df = df.dropna(subset=["Scalability_Index"]).copy()
df["Scalability_Index"] = df["Scalability_Index"].astype(int)

# Convert seconds to nanoseconds
df["Total_Time_Nanoseconds"] = df["Total_Time_Seconds"] * 1e9

# Aggregate in case there are duplicates (take mean)
df_plot = (
    df.groupby(["Optimization_Type", "Scalability_Index"], as_index=False)
      .agg({"Total_Time_Nanoseconds": "mean"})
      .sort_values(["Optimization_Type", "Scalability_Index"])
)

# Marker styles (different shapes)
markers = ["o", "s", "^", "D", "v", "P", "X", "*", ">", "<", "h", "H", "p", "8"]
opt_types = sorted(df_plot["Optimization_Type"].unique().tolist())
marker_map = {opt: markers[i % len(markers)] for i, opt in enumerate(opt_types)}

# Plot
plt.figure(figsize=(10, 5))

for opt in opt_types:
    g = df_plot[df_plot["Optimization_Type"] == opt].sort_values("Scalability_Index")
    plt.plot(
        g["Scalability_Index"],
        g["Total_Time_Nanoseconds"],
        marker=marker_map[opt],
        linewidth=2,
        label=opt
    )

plt.xticks(range(1, 10), [str(i) for i in range(1, 10)])
plt.xlabel("Scalability Index (20..180 requests mapped to 1..9)")
plt.ylabel("Total Time (nanoseconds)")
plt.title("Scalability Plot by Optimization Technique")
plt.grid(True, linestyle="--", alpha=0.6)
plt.legend(title="Optimization Technique", ncol=2)
plt.tight_layout()
plt.show()

