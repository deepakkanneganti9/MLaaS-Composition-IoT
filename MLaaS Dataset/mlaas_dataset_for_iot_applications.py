# -*- coding: utf-8 -*-
"""MLaaS Dataset for IoT Applications.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aA7IthWWmvQBbvryzt69T2E23CFFX4_I

**MNIST Dataset**
---
"""

import numpy as np
import pandas as pd
import time, os, json
import tensorflow as tf
from tensorflow.keras import layers, models

# ======================================================
# CONNECT GOOGLE DRIVE
# ======================================================
from google.colab import drive
drive.mount('/content/drive')

# ======================================================
# CONFIG
# ======================================================
NUM_CLIENTS      = 100      # you can reduce for faster runs
IID_FRACTION    = 0.4        # half IID, half non-IID
LOCAL_EPOCHS    = 5
GLOBAL_ROUNDS   = 5          # rounds for quality & reliability
BATCH_SIZE      = 64
AVAILABILITY_PROB = 0.6      # probability a client participates in a round

# ======================================================
# SAVE ALL WEIGHTS IN GOOGLE DRIVE
# ======================================================
WEIGHTS_DIR = "/content/drive/MyDrive/mlaas_weights100"  # <--- UPDATED LOCATION
os.makedirs(WEIGHTS_DIR, exist_ok=True)

FEATURE_COUNT = 28 * 28

# ======================================================
# LOAD MNIST
# ======================================================
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32") / 255.0
y_train = y_train.astype("int32")
y_test  = y_test.astype("int32")

all_images = np.concatenate([x_train, x_test], axis=0)
all_labels = np.concatenate([y_train, y_test], axis=0)


def create_clients_iid_noniid(images, labels, num_clients, iid_indices, non_iid_indices):
    clients = {}
    scenario = {}

    class_indices = {cls: np.where(labels == cls)[0] for cls in range(10)}

    for i in range(num_clients):
        samples_per_client = np.random.randint(800, 4000)
        client_samples = []

        if i in iid_indices:
            per_class = samples_per_client // 10
            for cls in range(10):
                chosen = np.random.choice(class_indices[cls],
                                          per_class,
                                          replace=True)
                client_samples.extend(chosen)
            scen = "IID"

        elif i in non_iid_indices:
            alpha = np.random.uniform(0.3, 3.0)
            class_weights = np.random.dirichlet(np.ones(10) * alpha)

            for cls in range(10):
                n_cls = int(class_weights[cls] * samples_per_client)
                if n_cls > 0:
                    chosen = np.random.choice(class_indices[cls],
                                              n_cls,
                                              replace=True)
                    client_samples.extend(chosen)
            scen = "NonIID"

        np.random.shuffle(client_samples)
        cid = i + 1
        clients[cid] = (images[client_samples], labels[client_samples])
        scenario[cid] = scen

    return clients, scenario


num_iid = int(NUM_CLIENTS * IID_FRACTION)
iid_indices     = list(range(0, num_iid))
non_iid_indices = list(range(num_iid, NUM_CLIENTS))

clients, scenario = create_clients_iid_noniid(
    all_images, all_labels,
    NUM_CLIENTS, iid_indices, non_iid_indices
)

print(f"âœ… Created {len(clients)} clients "
      f"({sum(1 for s in scenario.values() if s=='IID')} IID, "
      f"{sum(1 for s in scenario.values() if s=='NonIID')} NonIID)")


# ======================================================
# MODEL
# ======================================================
def build_model():
    model = models.Sequential([
        layers.Conv2D(16, (3, 3), activation="relu", input_shape=(28, 28, 1)),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(32, activation="relu"),
        layers.Dense(10, activation="softmax"),
    ])
    model.compile(
        optimizer="adam",
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model


# ======================================================
# LOCAL TRAINING + LOCAL QoS
# ======================================================
client_records = []
base_accuracy = {}
local_results_for_global = {}

for cid, (x_c, y_c) in clients.items():
    x_c = x_c[..., None]

    model = build_model()

    t0 = time.time()
    model.fit(x_c, y_c,
              epochs=LOCAL_EPOCHS,
              batch_size=BATCH_SIZE,
              verbose=0)
    train_time = time.time() - t0

    C_p = 1.0 / train_time
    latency_ms = train_time * 1000.0

    _, acc = model.evaluate(x_test[..., None], y_test, verbose=0)
    acc_pct = float(acc * 100.0)
    base_accuracy[cid] = acc_pct

    weights = model.get_weights()
    flat = np.concatenate([w.flatten() for w in weights])
    size_MB = flat.nbytes / (1024 * 1024)
    bw_t0 = time.time()
    _ = flat.copy()
    bw_time = time.time() - bw_t0
    BW_MBps = float(size_MB / bw_time)

    # ======================================================
    # UPDATED: SAVE CLIENT WEIGHTS IN GOOGLE DRIVE
    # ======================================================
    weight_path = os.path.join(WEIGHTS_DIR, f"client_{cid}_local.npz")
    np.savez_compressed(weight_path, *weights)

    unique, counts = np.unique(y_c, return_counts=True)
    label_map = dict(zip(unique, counts))

    row = {
        "Client_ID": cid,
        "Scenario": scenario[cid],
        "DataVolume(Samples)": int(len(y_c)),
        "FeatureCount": FEATURE_COUNT,
        "Local_Accuracy(%)": round(acc_pct, 2),
        "Train_Time(s)": train_time,
        "Latency(ms)": latency_ms,
        "C_p": C_p,
        "BW": BW_MBps,
        "Weights_Path": weight_path,
    }

    for l in range(10):
        row[f"Label{l}"] = int(label_map.get(l, 0))

    client_records.append(row)

    local_results_for_global[cid] = {
        "samples": len(y_c),
        "data": (x_c, y_c),
    }

print(f"âœ… Local training finished for {len(client_records)} clients")


# ======================================================
# FEDERATED ROUNDS
# ======================================================
def fedavg_weights(active_clients, local_results, global_model_weights):
    total_samples = sum(local_results[c]["samples"] for c in active_clients)
    if total_samples == 0:
        return global_model_weights

    agg = [np.zeros_like(w) for w in global_model_weights]

    for c in active_clients:
        x_c, y_c = local_results[c]["data"]
        model = build_model()
        model.set_weights(global_model_weights)
        model.fit(x_c, y_c, epochs=1, batch_size=BATCH_SIZE, verbose=0)
        w_c = model.get_weights()
        weight_factor = local_results[c]["samples"] / total_samples
        for i, w in enumerate(w_c):
            agg[i] += w * weight_factor

    return agg


quality_history = {cid: [] for cid in clients.keys()}
availability_history = {cid: [] for cid in clients.keys()}

global_model = build_model()
global_weights = global_model.get_weights()

for rnd in range(1, GLOBAL_ROUNDS + 1):
    active_clients = []

    for cid in clients.keys():
        is_active = (np.random.rand() < AVAILABILITY_PROB)
        availability_history[cid].append(1 if is_active else 0)
        if is_active:
            active_clients.append(cid)

    if active_clients:
        global_weights = fedavg_weights(
            active_clients, local_results_for_global, global_weights
        )
        global_model.set_weights(global_weights)

    for cid, (x_c, y_c) in clients.items():
        if availability_history[cid][-1] == 1:
            loss_c, acc_c = global_model.evaluate(x_c[..., None], y_c, verbose=0)
            acc_value = float(acc_c * 100.0)
        else:
            acc_value = base_accuracy[cid]

        quality_history[cid].append(acc_value)

    print(f"Round {rnd}: active clients = {len(active_clients)}")


reliability_score = {}
for cid in clients.keys():
    activations = sum(availability_history[cid])
    reliability_score[cid] = activations / GLOBAL_ROUNDS

print("âœ… Quality_Factor (per-round accuracies) and Reliability_Score computed.")


# ======================================================
# FINAL CLIENT DATAFRAME
# ======================================================
df_clients = pd.DataFrame(client_records)
df_clients["Quality_Factor"] = df_clients["Client_ID"].apply(
    lambda cid: json.dumps(quality_history[int(cid)])
)
df_clients["Mean_Quality_Factor(%)"] = df_clients["Client_ID"].apply(
    lambda cid: float(np.mean(quality_history[int(cid)]))
)
df_clients["Reliability_Score"] = df_clients["Client_ID"].apply(
    lambda cid: reliability_score[int(cid)]
)
print(df_clients.head())

df_clients

df_clients.to_csv("Client_Profiles_For_Composability_100.csv")

df_clients['Local_Accuracy(%)'].describe()

df_clients.columns

"""**FMNIST Dataset**
---
"""

import numpy as np
import pandas as pd
import time, os, json
import tensorflow as tf
from tensorflow.keras import layers, models

# ======================================================
# CONNECT GOOGLE DRIVE
# ======================================================
from google.colab import drive
drive.mount('/content/drive')

# ======================================================
# CONFIG
# ======================================================
NUM_CLIENTS       = 100      # you can reduce for faster runs
IID_FRACTION      = 0.4      # fraction of IID clients
LOCAL_EPOCHS      = 5
GLOBAL_ROUNDS     = 5        # rounds for quality & reliability
BATCH_SIZE        = 64
AVAILABILITY_PROB = 0.6      # probability a client participates in a round

# ======================================================
# SAVE ALL WEIGHTS IN GOOGLE DRIVE
# ======================================================
WEIGHTS_DIR = "/content/drive/MyDrive/mlaas_weights100_fmnist"  # UPDATED LOCATION FOR FMNIST
os.makedirs(WEIGHTS_DIR, exist_ok=True)

FEATURE_COUNT = 28 * 28

# ======================================================
# LOAD FASHION-MNIST
# ======================================================
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32") / 255.0
y_train = y_train.astype("int32")
y_test  = y_test.astype("int32")

all_images = np.concatenate([x_train, x_test], axis=0)
all_labels = np.concatenate([y_train, y_test], axis=0)


def create_clients_iid_noniid(images, labels, num_clients, iid_indices, non_iid_indices):
    clients = {}
    scenario = {}

    class_indices = {cls: np.where(labels == cls)[0] for cls in range(10)}

    for i in range(num_clients):
        samples_per_client = np.random.randint(800, 4000)
        client_samples = []

        if i in iid_indices:
            per_class = samples_per_client // 10
            for cls in range(10):
                chosen = np.random.choice(class_indices[cls],
                                          per_class,
                                          replace=True)
                client_samples.extend(chosen)
            scen = "IID"

        elif i in non_iid_indices:
            alpha = np.random.uniform(0.3, 3.0)
            class_weights = np.random.dirichlet(np.ones(10) * alpha)

            for cls in range(10):
                n_cls = int(class_weights[cls] * samples_per_client)
                if n_cls > 0:
                    chosen = np.random.choice(class_indices[cls],
                                              n_cls,
                                              replace=True)
                    client_samples.extend(chosen)
            scen = "NonIID"

        np.random.shuffle(client_samples)
        cid = i + 1
        clients[cid] = (images[client_samples], labels[client_samples])
        scenario[cid] = scen

    return clients, scenario


num_iid = int(NUM_CLIENTS * IID_FRACTION)
iid_indices     = list(range(0, num_iid))
non_iid_indices = list(range(num_iid, NUM_CLIENTS))

clients, scenario = create_clients_iid_noniid(
    all_images, all_labels,
    NUM_CLIENTS, iid_indices, non_iid_indices
)

print(f"âœ… Created {len(clients)} clients "
      f"({sum(1 for s in scenario.values() if s=='IID')} IID, "
      f"{sum(1 for s in scenario.values() if s=='NonIID')} NonIID)")


# ======================================================
# MODEL
# ======================================================
def build_model():
    model = models.Sequential([
        layers.Conv2D(16, (3, 3), activation="relu", input_shape=(28, 28, 1)),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(32, activation="relu"),
        layers.Dense(10, activation="softmax"),
    ])
    model.compile(
        optimizer="adam",
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model


# ======================================================
# LOCAL TRAINING + LOCAL QoS
# ======================================================
client_records = []
base_accuracy = {}
local_results_for_global = {}

for cid, (x_c, y_c) in clients.items():
    x_c = x_c[..., None]

    model = build_model()

    t0 = time.time()
    model.fit(x_c, y_c,
              epochs=LOCAL_EPOCHS,
              batch_size=BATCH_SIZE,
              verbose=0)
    train_time = time.time() - t0

    C_p = 1.0 / train_time
    latency_ms = train_time * 1000.0

    _, acc = model.evaluate(x_test[..., None], y_test, verbose=0)
    acc_pct = float(acc * 100.0)
    base_accuracy[cid] = acc_pct

    weights = model.get_weights()
    flat = np.concatenate([w.flatten() for w in weights])
    size_MB = flat.nbytes / (1024 * 1024)
    bw_t0 = time.time()
    _ = flat.copy()
    bw_time = time.time() - bw_t0
    BW_MBps = float(size_MB / bw_time)

    # ======================================================
    # UPDATED: SAVE CLIENT WEIGHTS IN GOOGLE DRIVE
    # ======================================================
    weight_path = os.path.join(WEIGHTS_DIR, f"client_{cid}_local.npz")
    np.savez_compressed(weight_path, *weights)

    unique, counts = np.unique(y_c, return_counts=True)
    label_map = dict(zip(unique, counts))

    row = {
        "Client_ID": cid,
        "Scenario": scenario[cid],
        "DataVolume(Samples)": int(len(y_c)),
        "FeatureCount": FEATURE_COUNT,
        "Local_Accuracy(%)": round(acc_pct, 2),
        "Train_Time(s)": train_time,
        "Latency(ms)": latency_ms,
        "C_p": C_p,
        "BW": BW_MBps,
        "Weights_Path": weight_path,
    }

    for l in range(10):
        row[f"Label{l}"] = int(label_map.get(l, 0))

    client_records.append(row)

    local_results_for_global[cid] = {
        "samples": len(y_c),
        "data": (x_c, y_c),
    }

print(f"âœ… Local training finished for {len(client_records)} clients")


# ======================================================
# FEDERATED ROUNDS
# ======================================================
def fedavg_weights(active_clients, local_results, global_model_weights):
    total_samples = sum(local_results[c]["samples"] for c in active_clients)
    if total_samples == 0:
        return global_model_weights

    agg = [np.zeros_like(w) for w in global_model_weights]

    for c in active_clients:
        x_c, y_c = local_results[c]["data"]
        model = build_model()
        model.set_weights(global_model_weights)
        model.fit(x_c, y_c, epochs=1, batch_size=BATCH_SIZE, verbose=0)
        w_c = model.get_weights()
        weight_factor = local_results[c]["samples"] / total_samples
        for i, w in enumerate(w_c):
            agg[i] += w * weight_factor

    return agg


quality_history = {cid: [] for cid in clients.keys()}
availability_history = {cid: [] for cid in clients.keys()}

global_model = build_model()
global_weights = global_model.get_weights()

for rnd in range(1, GLOBAL_ROUNDS + 1):
    active_clients = []

    for cid in clients.keys():
        is_active = (np.random.rand() < AVAILABILITY_PROB)
        availability_history[cid].append(1 if is_active else 0)
        if is_active:
            active_clients.append(cid)

    if active_clients:
        global_weights = fedavg_weights(
            active_clients, local_results_for_global, global_weights
        )
        global_model.set_weights(global_weights)

    for cid, (x_c, y_c) in clients.items():
        if availability_history[cid][-1] == 1:
            loss_c, acc_c = global_model.evaluate(x_c[..., None], y_c, verbose=0)
            acc_value = float(acc_c * 100.0)
        else:
            acc_value = base_accuracy[cid]

        quality_history[cid].append(acc_value)

    print(f"Round {rnd}: active clients = {len(active_clients)}")


reliability_score = {}
for cid in clients.keys():
    activations = sum(availability_history[cid])
    reliability_score[cid] = activations / GLOBAL_ROUNDS

print("âœ… Quality_Factor (per-round accuracies) and Reliability_Score computed.")


# ======================================================
# FINAL CLIENT DATAFRAME
# ======================================================
df_clients = pd.DataFrame(client_records)
df_clients["Quality_Factor"] = df_clients["Client_ID"].apply(
    lambda cid: json.dumps(quality_history[int(cid)])
)
df_clients["Mean_Quality_Factor(%)"] = df_clients["Client_ID"].apply(
    lambda cid: float(np.mean(quality_history[int(cid)]))
)
df_clients["Reliability_Score"] = df_clients["Client_ID"].apply(
    lambda cid: reliability_score[int(cid)]
)
print(df_clients.head())

df_clients.to_csv("FMNIST_Client_Profiles_For_Composability_1000.csv")

"""**CIFAR-10**
---

**Training**
"""

import numpy as np
import pandas as pd
import time, os, json
import tensorflow as tf
from tensorflow.keras import layers, models

# ======================================================
# CONNECT GOOGLE DRIVE  ðŸš¨ (ADDED AS PER YOUR FORMAT)
# ======================================================
from google.colab import drive
drive.mount('/content/drive')

# ======================================================
# CONFIG
# ======================================================
NUM_CLIENTS       = 100      # you can reduce for faster runs
IID_FRACTION      = 0.4      # fraction of IID clients
LOCAL_EPOCHS      = 5
GLOBAL_ROUNDS     = 5        # rounds for quality & reliability
BATCH_SIZE        = 64
AVAILABILITY_PROB = 0.6      # probability a client participates in a round
# ======================================================
# SAVE ALL WEIGHTS IN GOOGLE DRIVE  (CONSISTENT LOCATION)
# ======================================================
WEIGHTS_DIR = "/content/drive/MyDrive/mlaas_weights_cifar10"
os.makedirs(WEIGHTS_DIR, exist_ok=True)

FEATURE_COUNT = 32 * 32 * 3   # CIFAR-10 feature dimension

# ======================================================
# LOAD CIFAR-10 DATASET
# ======================================================
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
y_train = y_train.flatten()
y_test  = y_test.flatten()

x_train = x_train.astype("float32") / 255.0
x_test  = x_test.astype("float32") / 255.0

all_images = np.concatenate([x_train, x_test], axis=0)
all_labels = np.concatenate([y_train, y_test], axis=0)

print("ðŸ“¦ CIFAR-10 Loaded â†’ Total Samples:", len(all_images))


# ======================================================
# CLIENT CREATION (IID + Non-IID)
# ======================================================
def create_clients_iid_noniid(images, labels, num_clients, iid_indices, non_iid_indices):
    clients = {}
    scenario = {}
    class_indices = {cls: np.where(labels == cls)[0] for cls in range(10)}

    for i in range(num_clients):
        cid = i + 1
        samples = np.random.randint(800, 4000)
        client_samples = []

        if i in iid_indices:   # IID distribution
            per_cls = samples // 10
            for cls in range(10):
                chosen = np.random.choice(class_indices[cls], per_cls, replace=True)
                client_samples.extend(chosen)
            scenario[cid] = "IID"

        else:  # NON-IID using Dirichlet allocation
            alpha = np.random.uniform(0.3, 3.0)
            dist = np.random.dirichlet(np.ones(10) * alpha)
            for cls in range(10):
                n_cls = int(dist[cls] * samples)
                if n_cls > 0:
                    chosen = np.random.choice(class_indices[cls], n_cls, replace=True)
                    client_samples.extend(chosen)
            scenario[cid] = "NonIID"

        np.random.shuffle(client_samples)
        clients[cid] = (images[client_samples], labels[client_samples])

    return clients, scenario


num_iid = int(NUM_CLIENTS * IID_FRACTION)
iid_indices     = list(range(num_iid))
non_iid_indices = list(range(num_iid, NUM_CLIENTS))

clients, scenario = create_clients_iid_noniid(all_images, all_labels, NUM_CLIENTS, iid_indices, non_iid_indices)

print(f"âœ… Created {len(clients)} Clients "
      f"({sum(1 for s in scenario.values() if s=='IID')} IID, "
      f"{sum(1 for s in scenario.values() if s=='NonIID')} NonIID)")


# ======================================================
# CIFAR-10 MODEL
# ======================================================
def build_model():
    model = models.Sequential([
        layers.Conv2D(32, (3,3), padding="same", activation="relu", input_shape=(32,32,3)),
        layers.Conv2D(32, (3,3), padding="same", activation="relu"),
        layers.MaxPooling2D((2,2)),

        layers.Conv2D(64, (3,3), padding="same", activation="relu"),
        layers.Conv2D(64, (3,3), padding="same", activation="relu"),
        layers.MaxPooling2D((2,2)),

        layers.Flatten(),
        layers.Dense(128, activation="relu"),
        layers.Dense(10, activation="softmax"),
    ])
    model.compile(
        optimizer="adam",
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model


# ======================================================
# LOCAL TRAINING + QoS EXTRACTION
# ======================================================
client_records = []
base_accuracy = {}
local_results_for_global = {}

for cid, (x_c, y_c) in clients.items():
    model = build_model()

    t0 = time.time()
    model.fit(x_c, y_c, epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE, verbose=0)
    train_time = time.time() - t0

    C_p = 1.0 / train_time
    latency_ms = train_time * 1000

    _, acc = model.evaluate(x_test, y_test, verbose=0)
    acc_pct = float(acc * 100)
    base_accuracy[cid] = acc_pct

    weights = model.get_weights()
    flat = np.concatenate([w.flatten() for w in weights])
    size_MB = flat.nbytes / (1024 * 1024)

    bw_t0 = time.time()
    _ = flat.copy()
    bw_s = time.time() - bw_t0
    BW_MBps = size_MB / bw_s

    path = os.path.join(WEIGHTS_DIR, f"client_{cid}_local.npz")
    np.savez_compressed(path, *weights)

    unique, counts = np.unique(y_c, return_counts=True)
    lbl_map = dict(zip(unique, counts))

    row = {
        "Client_ID": cid,
        "Scenario": scenario[cid],
        "DataVolume(Samples)": len(y_c),
        "FeatureCount": FEATURE_COUNT,
        "Local_Accuracy(%)": round(acc_pct, 2),
        "Train_Time(s)": train_time,
        "Latency(ms)": latency_ms,
        "C_p": C_p,
        "BW": BW_MBps,
        "Weights_Path": path
    }
    for l in range(10):
        row[f"Label{l}"] = lbl_map.get(l, 0)

    client_records.append(row)
    local_results_for_global[cid] = {"samples": len(y_c), "data": (x_c, y_c)}

print("ðŸš€ Local Training Finished")


# ======================================================
# FEDERATED GLOBAL AGGREGATION + QUALITY TRACKING
# ======================================================
def fedavg_weights(active, local, global_w):
    total = sum(local[c]["samples"] for c in active)
    if total == 0: return global_w

    agg = [np.zeros_like(w) for w in global_w]
    for c in active:
        x, y = local[c]["data"]
        m = build_model()
        m.set_weights(global_w)
        m.fit(x, y, epochs=1, batch_size=BATCH_SIZE, verbose=0)
        w = m.get_weights()
        factor = local[c]["samples"] / total
        for i in range(len(w)):
            agg[i] += w[i] * factor
    return agg


quality_history      = {cid: [] for cid in clients}
availability_history = {cid: [] for cid in clients}

global_model = build_model()
global_w = global_model.get_weights()

for rnd in range(1, GLOBAL_ROUNDS + 1):
    active = []
    for cid in clients:
        flag = np.random.rand() < AVAILABILITY_PROB
        availability_history[cid].append(flag)
        if flag: active.append(cid)

    if active:
        global_w = fedavg_weights(active, local_results_for_global, global_w)
        global_model.set_weights(global_w)

    for cid, (x_c, y_c) in clients.items():
        if availability_history[cid][-1]:
            _, acc = global_model.evaluate(x_c, y_c, verbose=0)
            val = acc * 100
        else:
            val = base_accuracy[cid]
        quality_history[cid].append(val)

    print(f"ðŸŒ Round {rnd}: Active Clients = {len(active)}")


reliability = {
    cid: sum(availability_history[cid]) / GLOBAL_ROUNDS
    for cid in clients
}


# ======================================================
# FINAL CLIENT DATAFRAME
# ======================================================
df_clients = pd.DataFrame(client_records)
df_clients["Quality_Factor"] = df_clients["Client_ID"].apply(lambda c: json.dumps(quality_history[c]))
df_clients["Mean_Quality_Factor(%)"] = df_clients["Client_ID"].apply(lambda c: float(np.mean(quality_history[c])))
df_clients["Reliability_Score"] = df_clients["Client_ID"].apply(lambda c: reliability[c])

print(df_clients.head())

"""**WITHOUT training**
---
"""

import numpy as np
import pandas as pd
import time, os, json
import tensorflow as tf
from tensorflow.keras import layers, models

# ======================================================
# CONNECT GOOGLE DRIVE
# ======================================================
from google.colab import drive
drive.mount('/content/drive')

# ======================================================
# CONFIG
# ======================================================
NUM_CLIENTS       = 100
LOCAL_EPOCHS      = 0       # ðŸ”¥ NO TRAINING
GLOBAL_ROUNDS     = 5
BATCH_SIZE        = 64
AVAILABILITY_PROB = 0.6

WEIGHTS_DIR = "/content/drive/MyDrive/mlaas_weights_cifar10"
FEATURE_COUNT = 32 * 32 * 3

# ======================================================
# LOAD CIFAR-10
# ======================================================
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
y_test = y_test.flatten()
x_test = x_test.astype("float32") / 255.0

# ======================================================
# DEFINE CIFAR-10 MODEL
# ======================================================
def build_model():
    model = models.Sequential([
        layers.Conv2D(32, (3,3), padding="same", activation="relu", input_shape=(32,32,3)),
        layers.Conv2D(32, (3,3), padding="same", activation="relu"),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(64, (3,3), padding="same", activation="relu"),
        layers.Conv2D(64, (3,3), padding="same", activation="relu"),
        layers.MaxPooling2D((2,2)),
        layers.Flatten(),
        layers.Dense(128, activation="relu"),
        layers.Dense(10, activation="softmax"),
    ])
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    return model

# ======================================================
# LOAD CLIENT WEIGHTS + LOCAL TESTING
# ======================================================
client_records = {}
local_results_for_global = {}
base_accuracy = {}

for cid in range(1, NUM_CLIENTS + 1):
    path = os.path.join(WEIGHTS_DIR, f"client_{cid}_local.npz")
    data = np.load(path, allow_pickle=True)
    weights = [data[f'arr_{i}'] for i in range(len(data.files))]

    model = build_model()
    model.set_weights(weights)

    t0 = time.time()
    loss, acc = model.evaluate(x_test, y_test, verbose=0)
    latency = (time.time() - t0) * 1000

    flat = np.concatenate([w.flatten() for w in weights])
    size_MB = flat.nbytes / (1024 * 1024)

    bw_t0 = time.time()
    _ = flat.copy()
    bw_val = size_MB / (time.time() - bw_t0)

    local_results_for_global[cid] = {
        "weights": weights,
        "samples": model.count_params()
    }
    base_accuracy[cid] = acc * 100

    client_records[cid] = {
        "Client_ID": cid,
        "Local_Accuracy(%)": round(acc * 100, 2),
        "Latency(ms)": latency,
        "C_p": 1 / (latency / 1000),
        "BW": bw_val,
        "FeatureCount": FEATURE_COUNT,
        "Weights_Path": path
    }

print("ðŸ“Œ All client weights loaded successfully & tested locally")

# ======================================================
# FEDERATED AGGREGATION USING SAVED WEIGHTS
# ======================================================
def fedavg(global_w, active_clients):
    total = len(active_clients)
    if total == 0:
        return global_w

    agg = [np.zeros_like(w) for w in global_w]

    for cid in active_clients:
        client_w = local_results_for_global[cid]["weights"]
        for i in range(len(client_w)):
            agg[i] += client_w[i] / total

    return agg

global_model = build_model()
global_w = global_model.get_weights()

quality_history = {cid: [] for cid in range(1, NUM_CLIENTS + 1)}
availability_history = {cid: [] for cid in range(1, NUM_CLIENTS + 1)}

for rnd in range(1, GLOBAL_ROUNDS + 1):
    active = []
    for cid in range(1, NUM_CLIENTS + 1):
        avail = np.random.rand() < AVAILABILITY_PROB
        availability_history[cid].append(avail)
        if avail:
            active.append(cid)

    if active:
        global_w = fedavg(global_w, active)
        global_model.set_weights(global_w)

    for cid in range(1, NUM_CLIENTS + 1):
        if availability_history[cid][-1]:
            _, acc = global_model.evaluate(x_test, y_test, verbose=0)
            quality_history[cid].append(acc * 100)
        else:
            quality_history[cid].append(base_accuracy[cid])

    print(f"ðŸŒ Round {rnd}: Active Clients = {len(active)}")

# ======================================================
# RELIABILITY + FINAL DF
# ======================================================
reliability = {cid: sum(availability_history[cid]) / GLOBAL_ROUNDS for cid in range(1, NUM_CLIENTS + 1)}

df_clients = pd.DataFrame.from_dict(client_records, orient='index')
df_clients["Quality_Factor"] = df_clients["Client_ID"].apply(lambda c: json.dumps(quality_history[c]))
df_clients["Mean_Quality_Factor(%)"] = df_clients["Client_ID"].apply(lambda c: float(np.mean(quality_history[c])))
df_clients["Reliability_Score"] = df_clients["Client_ID"].apply(lambda c: reliability[c])

print("ðŸŽ¯ Final DF Ready")
print(df_clients.head())

df_clients.to_csv("CIFAR10_Client_Profiles_For_Composability_1000.csv")

df_clients

"""**HAR Dataset**
---
"""

import numpy as np
import pandas as pd
import time, os, json
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# ======================================================
# CONNECT GOOGLE DRIVE
# ======================================================
from google.colab import drive
drive.mount('/content/drive')

# ======================================================
# CONFIG
# ======================================================
NUM_CLIENTS       = 100       # adjust if needed
IID_FRACTION      = 0.4
LOCAL_EPOCHS      = 10
GLOBAL_ROUNDS     = 5
BATCH_SIZE        = 32
AVAILABILITY_PROB = 0.6

WEIGHTS_DIR = "/content/drive/MyDrive/mlaas_weights_har"
os.makedirs(WEIGHTS_DIR, exist_ok=True)

file_path = '/content/drive/My Drive/Early Drift Detection/pamap2_final.csv'
# ======================================================
# LOAD YOUR HAR CSV DATASET
# ======================================================
df = pd.read_csv(file_path)   # <-- change if needed
df = df.sample(frac=1).reset_index(drop=True)

label_encoder = LabelEncoder()
df["activity"] = label_encoder.fit_transform(df["activity"])

X = df.drop("activity", axis=1).values.astype("float32")
y = df["activity"].values.astype("int32")

FEATURE_COUNT = X.shape[1]
NUM_CLASSES   = len(np.unique(y))

# GLOBAL TRAIN/TEST
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

# ======================================================
# CREATE IID + NON-IID CLIENTS
# ======================================================
def create_clients(X, y, num_clients, iid_idx, non_iid_idx):
    clients, scenario = {}, {}
    cls_idx = {cls: np.where(y == cls)[0] for cls in np.unique(y)}

    for i in range(num_clients):
        cid = i + 1
        samples = np.random.randint(300, 2000)
        chosen = []

        if i in iid_idx:
            per = samples // NUM_CLASSES
            for cls in cls_idx:
                sel = np.random.choice(cls_idx[cls], per, replace=True)
                chosen.extend(sel)
            scenario[cid] = "IID"
        else:
            alpha = np.random.uniform(0.3, 3.0)
            dist = np.random.dirichlet(np.ones(NUM_CLASSES) * alpha)
            for idx, cls in enumerate(cls_idx):
                n = int(dist[idx] * samples)
                if n > 0:
                    sel = np.random.choice(cls_idx[cls], n, replace=True)
                    chosen.extend(sel)
            scenario[cid] = "NonIID"

        np.random.shuffle(chosen)
        clients[cid] = (X[chosen], y[chosen])
    return clients, scenario

num_iid = int(NUM_CLIENTS * IID_FRACTION)
iid_idx = list(range(num_iid))
non_iid_idx = list(range(num_iid, NUM_CLIENTS))

clients, scenario = create_clients(X_train, y_train, NUM_CLIENTS, iid_idx, non_iid_idx)
print("Clients created:", len(clients))

# ======================================================
# BUILD HAR MLP MODEL
# ======================================================
def build_model():
    model = models.Sequential([
        layers.Input(shape=(FEATURE_COUNT,)),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.2),
        layers.Dense(NUM_CLASSES, activation='softmax')
    ])
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    return model

# ======================================================
# LOCAL TRAINING + QOS EXTRACTION
# ======================================================
client_records = []
base_accuracy = {}
local_store = {}

for cid, (x_c, y_c) in clients.items():
    model = build_model()

    t0 = time.time()
    model.fit(x_c, y_c, epochs=LOCAL_EPOCHS, batch_size=BATCH_SIZE, verbose=0)
    train_time = time.time() - t0

    C_p = 1.0 / train_time
    latency_ms = train_time * 1000
    _, acc = model.evaluate(X_test, y_test, verbose=0)
    acc_pct = float(acc * 100)
    base_accuracy[cid] = acc_pct

    weights = model.get_weights()
    flat = np.concatenate([w.flatten() for w in weights])
    size_MB = flat.nbytes / (1024 * 1024)
    bw_t0 = time.time()
    _ = flat.copy()
    BW = size_MB / (time.time() - bw_t0)

    path = f"{WEIGHTS_DIR}/client_{cid}.npz"
    np.savez_compressed(path, *weights)

    unique, counts = np.unique(y_c, return_counts=True)
    lbl = dict(zip(unique, counts))

    row = {
        "Client_ID": cid,
        "Scenario": scenario[cid],
        "DataVolume(Samples)": len(y_c),
        "FeatureCount": FEATURE_COUNT,
        "Local_Accuracy(%)": round(acc_pct, 2),
        "Train_Time(s)": train_time,
        "Latency(ms)": latency_ms,
        "C_p": C_p,
        "BW": BW,
        "Weights_Path": path
    }
    for l in range(NUM_CLASSES):
        row[f"Label{l}"] = int(lbl.get(l, 0))

    client_records.append(row)
    local_store[cid] = {"samples": len(y_c), "data": (x_c, y_c)}

print("Local training complete.")

# ======================================================
# FEDERATED AGGREGATION (FEDAVG)
# ======================================================
def fedavg(active, local_store, global_w):
    total = sum(local_store[c]["samples"] for c in active)
    agg = [np.zeros_like(w) for w in global_w]
    for cid in active:
        model = build_model()
        model.set_weights(global_w)
        x, y = local_store[cid]["data"]
        model.fit(x, y, epochs=1, batch_size=BATCH_SIZE, verbose=0)
        w = model.get_weights()
        factor = local_store[cid]["samples"] / total
        for i in range(len(w)):
            agg[i] += w[i] * factor
    return agg

quality_history = {cid: [] for cid in clients}
availability = {cid: [] for cid in clients}

global_model = build_model()
global_w = global_model.get_weights()

for rnd in range(GLOBAL_ROUNDS):
    active = [cid for cid in clients if np.random.rand() < AVAILABILITY_PROB]

    if active:
        global_w = fedavg(active, local_store, global_w)
        global_model.set_weights(global_w)

    for cid in clients:
        if cid in active:
            _, acc = global_model.evaluate(*local_store[cid]["data"], verbose=0)
            val = acc * 100
        else:
            val = base_accuracy[cid]
        availability[cid].append(int(cid in active))
        quality_history[cid].append(val)

    print(f"Round {rnd+1}: {len(active)} clients active")

print("Federated rounds completed.")

# ======================================================
# FINAL CLIENT DATAFRAME (SAME AS MNIST)
# ======================================================
df_clients = pd.DataFrame(client_records)
df_clients["Quality_Factor"] = df_clients["Client_ID"].apply(lambda c: json.dumps(quality_history[c]))
df_clients["Mean_Quality_Factor(%)"] = df_clients["Client_ID"].apply(lambda c: float(np.mean(quality_history[c])))
df_clients["Reliability_Score"] = df_clients["Client_ID"].apply(lambda c: sum(availability[c]) / GLOBAL_ROUNDS)
print(df_clients.head())

df_clients.to_csv("HAR_Client_Profiles_For_Composability.csv")